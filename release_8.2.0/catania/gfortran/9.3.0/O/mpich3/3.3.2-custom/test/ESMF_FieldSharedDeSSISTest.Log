build time -- 2021-10-15 11:36:28
20211015 115039.974 INFO             PET0 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20211015 115039.975 INFO             PET0 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20211015 115039.975 INFO             PET0 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20211015 115039.975 INFO             PET0 !!! FOR PRODUCTION RUNS, USE:                      !!!
20211015 115039.975 INFO             PET0 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20211015 115039.975 INFO             PET0 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20211015 115039.975 INFO             PET0 Running with ESMF Version   : ESMF_8_2_0_beta_snapshot_22-10-g6974ca47a6
20211015 115039.975 INFO             PET0 ESMF library build date/time: "Oct 15 2021" "11:35:17"
20211015 115039.975 INFO             PET0 ESMF library build location : /Volumes/esmf/rocky/esmf-testing/gfortran_9.3.0_mpich3_O_release_8.2.0
20211015 115039.975 INFO             PET0 ESMF_COMM                   : mpich3
20211015 115039.975 INFO             PET0 ESMF_MOAB                   : enabled
20211015 115039.975 INFO             PET0 ESMF_LAPACK                 : enabled
20211015 115039.975 INFO             PET0 ESMF_NETCDF                 : enabled
20211015 115039.975 INFO             PET0 ESMF_PNETCDF                : disabled
20211015 115039.975 INFO             PET0 ESMF_PIO                    : enabled
20211015 115039.975 INFO             PET0 ESMF_YAMLCPP                : enabled
20211015 115039.976 INFO             PET0 --- VMK::logSystem() start -------------------------------
20211015 115039.976 INFO             PET0 esmfComm=mpich3
20211015 115039.976 INFO             PET0 isPthreadsEnabled=0
20211015 115039.976 INFO             PET0 isOpenMPEnabled=1
20211015 115039.976 INFO             PET0 isOpenACCEnabled=0
20211015 115039.976 INFO             PET0 isSsiSharedMemoryEnabled=1
20211015 115039.976 INFO             PET0 ssiCount=1 peCount=6
20211015 115039.976 INFO             PET0 PE=0 SSI=0 SSIPE=0
20211015 115039.976 INFO             PET0 PE=1 SSI=0 SSIPE=1
20211015 115039.976 INFO             PET0 PE=2 SSI=0 SSIPE=2
20211015 115039.976 INFO             PET0 PE=3 SSI=0 SSIPE=3
20211015 115039.976 INFO             PET0 PE=4 SSI=0 SSIPE=4
20211015 115039.976 INFO             PET0 PE=5 SSI=0 SSIPE=5
20211015 115039.976 INFO             PET0 --- VMK::logSystem() MPI Control Variables ---------------
20211015 115039.976 INFO             PET0 index=   0                          MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the short message algorithm will be used if the send buffer size is < this value (in bytes). (See also: MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE)
20211015 115039.976 INFO             PET0 index=   1                           MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the long message algorithm will be used if the send buffer size is >= this value (in bytes) (See also: MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE)
20211015 115039.976 INFO             PET0 index=   2                         MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM : Variable to select allgather algorithmauto               - Internal algorithm selectionbrucks             - Force brucks algorithmnb                 - Force nonblocking algorithmrecursive_doubling - Force recursive doubling algorithmring               - Force ring algorithm
20211015 115039.976 INFO             PET0 index=   3                         MPIR_CVAR_ALLGATHER_INTER_ALGORITHM : Variable to select allgather algorithmauto                      - Internal algorithm selectionlocal_gather_remote_bcast - Force local-gather-remote-bcast algorithmnb                        - Force nonblocking algorithm
20211015 115039.976 INFO             PET0 index=   4                       MPIR_CVAR_ALLGATHER_DEVICE_COLLECTIVE : If set to true, MPI_Allgather will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level allgather function will not be called.
20211015 115039.976 INFO             PET0 index=   5                      MPIR_CVAR_ALLGATHERV_PIPELINE_MSG_SIZE : The smallest message size that will be used for the pipelined, large-message, ring algorithm in the MPI_Allgatherv implementation.
20211015 115039.976 INFO             PET0 index=   6                        MPIR_CVAR_ALLGATHERV_INTRA_ALGORITHM : Variable to select allgatherv algorithmauto               - Internal algorithm selectionbrucks             - Force brucks algorithmnb                 - Force nonblocking algorithmrecursive_doubling - Force recursive doubling algorithmring               - Force ring algorithm
20211015 115039.976 INFO             PET0 index=   7                        MPIR_CVAR_ALLGATHERV_INTER_ALGORITHM : Variable to select allgatherv algorithmauto                      - Internal algorithm selectionnb                        - Force nonblocking algorithmremote_gather_local_bcast - Force remote-gather-local-bcast algorithm
20211015 115039.976 INFO             PET0 index=   8                      MPIR_CVAR_ALLGATHERV_DEVICE_COLLECTIVE : If set to true, MPI_Allgatherv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level allgatherv function will not be called.
20211015 115039.976 INFO             PET0 index=   9                          MPIR_CVAR_ALLREDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20211015 115039.976 INFO             PET0 index=  10                            MPIR_CVAR_ENABLE_SMP_COLLECTIVES : Enable SMP aware collective communication.
20211015 115039.976 INFO             PET0 index=  11                              MPIR_CVAR_ENABLE_SMP_ALLREDUCE : Enable SMP aware allreduce.
20211015 115039.976 INFO             PET0 index=  12                        MPIR_CVAR_MAX_SMP_ALLREDUCE_MSG_SIZE : Maximum message size for which SMP-aware allreduce is used.  A value of '0' uses SMP-aware allreduce for all message sizes.
20211015 115039.976 INFO             PET0 index=  13                         MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM : Variable to select allreduce algorithmauto                     - Internal algorithm selectionnb                       - Force nonblocking algorithmrecursive_doubling       - Force recursive doubling algorithmreduce_scatter_allgather - Force reduce scatter allgather algorithm
20211015 115039.976 INFO             PET0 index=  14                         MPIR_CVAR_ALLREDUCE_INTER_ALGORITHM : Variable to select allreduce algorithmauto                  - Internal algorithm selectionnb                    - Force nonblocking algorithmreduce_exchange_bcast - Force reduce-exchange-bcast algorithm
20211015 115039.976 INFO             PET0 index=  15                       MPIR_CVAR_ALLREDUCE_DEVICE_COLLECTIVE : If set to true, MPI_Allreduce will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level allreduce function will not be called.
20211015 115039.976 INFO             PET0 index=  16                           MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE : the short message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value (See also: MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE)
20211015 115039.976 INFO             PET0 index=  17                          MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE : the medium message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value and larger than MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE (See also: MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE)
20211015 115039.976 INFO             PET0 index=  18                                 MPIR_CVAR_ALLTOALL_THROTTLE : max no. of irecvs/isends posted at a time in some alltoall algorithms. Setting it to 0 causes all irecvs/isends to be posted at once
20211015 115039.976 INFO             PET0 index=  19                          MPIR_CVAR_ALLTOALL_INTRA_ALGORITHM : Variable to select alltoall algorithmauto                      - Internal algorithm selectionbrucks                    - Force brucks algorithmnb                        - Force nonblocking algorithmpairwise                  - Force pairwise algorithmpairwise_sendrecv_replace - Force pairwise sendrecv replace algorithmscattered                 - Force scattered algorithm
20211015 115039.976 INFO             PET0 index=  20                          MPIR_CVAR_ALLTOALL_INTER_ALGORITHM : Variable to select alltoall algorithmauto              - Internal algorithm selectionnb                - Force nonblocking algorithmpairwise_exchange - Force pairwise exchange algorithm
20211015 115039.976 INFO             PET0 index=  21                        MPIR_CVAR_ALLTOALL_DEVICE_COLLECTIVE : If set to true, MPI_Alltoall will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level alltoall function will not be called.
20211015 115039.976 INFO             PET0 index=  22                         MPIR_CVAR_ALLTOALLV_INTRA_ALGORITHM : Variable to select alltoallv algorithmauto                      - Internal algorithm selectionnb                        - Force nonblocking algorithmpairwise_sendrecv_replace - Force pairwise_sendrecv_replace algorithmscattered                 - Force scattered algorithm
20211015 115039.976 INFO             PET0 index=  23                         MPIR_CVAR_ALLTOALLV_INTER_ALGORITHM : Variable to select alltoallv algorithmauto              - Internal algorithm selectionpairwise_exchange - Force pairwise exchange algorithmnb                - Force nonblocking algorithm
20211015 115039.976 INFO             PET0 index=  24                       MPIR_CVAR_ALLTOALLV_DEVICE_COLLECTIVE : If set to true, MPI_Alltoallv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level alltoallv function will not be called.
20211015 115039.976 INFO             PET0 index=  25                         MPIR_CVAR_ALLTOALLW_INTRA_ALGORITHM : Variable to select alltoallw algorithmauto                      - Internal algorithm selectionnb                        - Force nonblocking algorithmpairwise_sendrecv_replace - Force pairwise sendrecv replace algorithmscattered                 - Force scattered algorithm
20211015 115039.976 INFO             PET0 index=  26                         MPIR_CVAR_ALLTOALLW_INTER_ALGORITHM : Variable to select alltoallw algorithmauto              - Internal algorithm selectionnb                - Force nonblocking algorithmpairwise_exchange - Force pairwise exchange algorithm
20211015 115039.976 INFO             PET0 index=  27                       MPIR_CVAR_ALLTOALLW_DEVICE_COLLECTIVE : If set to true, MPI_Alltoallw will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level alltoallw function will not be called.
20211015 115039.977 INFO             PET0 index=  28                                MPIR_CVAR_ENABLE_SMP_BARRIER : Enable SMP aware barrier.
20211015 115039.977 INFO             PET0 index=  29                           MPIR_CVAR_BARRIER_INTRA_ALGORITHM : Variable to select barrier algorithmauto               - Internal algorithm selectionnb                 - Force nonblocking algorithmrecursive_doubling - Force recursive doubling algorithm
20211015 115039.977 INFO             PET0 index=  30                           MPIR_CVAR_BARRIER_INTER_ALGORITHM : Variable to select barrier algorithmauto  - Internal algorithm selectionbcast - Force bcast algorithmnb    - Force nonblocking algorithm
20211015 115039.977 INFO             PET0 index=  31                         MPIR_CVAR_BARRIER_DEVICE_COLLECTIVE : If set to true, MPI_Barrier will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level barrier function will not be called.
20211015 115039.977 INFO             PET0 index=  32                                   MPIR_CVAR_BCAST_MIN_PROCS : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_SHORT_MSG_SIZE, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20211015 115039.977 INFO             PET0 index=  33                              MPIR_CVAR_BCAST_SHORT_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20211015 115039.977 INFO             PET0 index=  34                               MPIR_CVAR_BCAST_LONG_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_SHORT_MSG_SIZE)
20211015 115039.977 INFO             PET0 index=  35                                  MPIR_CVAR_ENABLE_SMP_BCAST : Enable SMP aware broadcast (See also: MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE)
20211015 115039.977 INFO             PET0 index=  36                            MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE : Maximum message size for which SMP-aware broadcast is used.  A value of '0' uses SMP-aware broadcast for all message sizes. (See also: MPIR_CVAR_ENABLE_SMP_BCAST)
20211015 115039.977 INFO             PET0 index=  37                             MPIR_CVAR_BCAST_INTRA_ALGORITHM : Variable to select bcast algorithmauto                                    - Internal algorithm selectionbinomial                                - Force Binomial Treenb                                      - Force nonblocking algorithmscatter_recursive_doubling_allgather    - Force Scatter Recursive-Doubling Allgatherscatter_ring_allgather                  - Force Scatter Ring
20211015 115039.977 INFO             PET0 index=  38                             MPIR_CVAR_BCAST_INTER_ALGORITHM : Variable to select bcast algorithmauto                    - Internal algorithm selectionnb                      - Force nonblocking algorithmremote_send_local_bcast - Force remote-send-local-bcast algorithm
20211015 115039.977 INFO             PET0 index=  39                           MPIR_CVAR_BCAST_DEVICE_COLLECTIVE : If set to true, MPI_Bcast will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually.  If set to false, the device-level bcast function will not be called.
20211015 115039.977 INFO             PET0 index=  40                            MPIR_CVAR_EXSCAN_INTRA_ALGORITHM : Variable to select allgather algorithmauto               - Internal algorithm selectionnb                 - Force nonblocking algorithmrecursive_doubling - Force recursive doubling algorithm
20211015 115039.977 INFO             PET0 index=  41                          MPIR_CVAR_EXSCAN_DEVICE_COLLECTIVE : If set to true, MPI_Exscan will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level exscan function will not be called.
20211015 115039.977 INFO             PET0 index=  42                       MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_VSMALL_MSG_SIZE)
20211015 115039.977 INFO             PET0 index=  43                            MPIR_CVAR_GATHER_INTRA_ALGORITHM : Variable to select gather algorithmauto     - Internal algorithm selectionbinomial - Force binomial algorithmnb       - Force nonblocking algorithm
20211015 115039.977 INFO             PET0 index=  44                            MPIR_CVAR_GATHER_INTER_ALGORITHM : Variable to select gather algorithmauto                     - Internal algorithm selectionlinear                   - Force linear algorithmlocal_gather_remote_send - Force local-gather-remote-send algorithmnb                       - Force nonblocking algorithm
20211015 115039.977 INFO             PET0 index=  45                          MPIR_CVAR_GATHER_DEVICE_COLLECTIVE : If set to true, MPI_Gather will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level gather function will not be called.
20211015 115039.977 INFO             PET0 index=  46                            MPIR_CVAR_GATHER_VSMALL_MSG_SIZE : use a temporary buffer for intracommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE)
20211015 115039.977 INFO             PET0 index=  47                           MPIR_CVAR_GATHERV_INTRA_ALGORITHM : Variable to select gatherv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithmnb     - Force nonblocking algorithm
20211015 115039.977 INFO             PET0 index=  48                           MPIR_CVAR_GATHERV_INTER_ALGORITHM : Variable to select gatherv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithmnb     - Force nonblocking algorithm
20211015 115039.977 INFO             PET0 index=  49                         MPIR_CVAR_GATHERV_DEVICE_COLLECTIVE : If set to true, MPI_Gatherv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level gatherv function will not be called.
20211015 115039.977 INFO             PET0 index=  50                     MPIR_CVAR_GATHERV_INTER_SSEND_MIN_PROCS : Use Ssend (synchronous send) for intercommunicator MPI_Gatherv if the "group B" size is >= this value.  Specifying "-1" always avoids using Ssend.  For backwards compatibility, specifying "0" uses the default value.
20211015 115039.977 INFO             PET0 index=  51                           MPIR_CVAR_IALLGATHER_RECEXCH_KVAL : k value for recursive exchange based iallgather
20211015 115039.977 INFO             PET0 index=  52                            MPIR_CVAR_IALLGATHER_BRUCKS_KVAL : k value for radix in brucks based iallgather
20211015 115039.977 INFO             PET0 index=  53                        MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM : Variable to select iallgather algorithmauto               - Internal algorithm selectionbrucks             - Force brucks algorithmrecursive_doubling - Force recursive doubling algorithmring               - Force ring algorithmrecexch_distance_doubling    - Force generic transport recursive exchange with neighbours doubling in distance in each phaserecexch_distance_halving  - Force generic transport recursive exchange with neighbours halving in distance in each phasegentran_brucks     - Force generic transport based brucks algorithm
20211015 115039.977 INFO             PET0 index=  54                        MPIR_CVAR_IALLGATHER_INTER_ALGORITHM : Variable to select iallgather algorithmauto                      - Internal algorithm selectionlocal_gather_remote_bcast - Force local-gather-remote-bcast algorithm
20211015 115039.977 INFO             PET0 index=  55                      MPIR_CVAR_IALLGATHER_DEVICE_COLLECTIVE : If set to true, MPI_Iallgather will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iallgather function will not be called.
20211015 115039.977 INFO             PET0 index=  56                          MPIR_CVAR_IALLGATHERV_RECEXCH_KVAL : k value for recursive exchange based iallgatherv
20211015 115039.977 INFO             PET0 index=  57                       MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM : Variable to select iallgatherv algorithmauto               - Internal algorithm selectionbrucks             - Force brucks algorithmrecursive_doubling - Force recursive doubling algorithmring               - Force ring algorithmrecexch_distance_doubling    - Force generic transport recursive exchange with neighbours doubling in distance in each phaserecexch_distance_halving     - Force generic transport recursive exchange with neighbours halving in distance in each phasegentran_ring              - Force generic transport ring algorithm
20211015 115039.977 INFO             PET0 index=  58                       MPIR_CVAR_IALLGATHERV_INTER_ALGORITHM : Variable to select iallgatherv algorithmauto                      - Internal algorithm selectionremote_gather_local_bcast - Force remote-gather-local-bcast algorithm
20211015 115039.977 INFO             PET0 index=  59                     MPIR_CVAR_IALLGATHERV_DEVICE_COLLECTIVE : If set to true, MPI_Iallgatherv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iallgatherv function will not be called.
20211015 115039.977 INFO             PET0 index=  60                              MPIR_CVAR_IALLREDUCE_TREE_KVAL : k value for tree based iallreduce (for tree_kary and tree_knomial)
20211015 115039.977 INFO             PET0 index=  61               MPIR_CVAR_IALLREDUCE_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based iallreduce (tree_kary and tree_knomial). Default value is 0, that is, no pipelining by default
20211015 115039.977 INFO             PET0 index=  62                  MPIR_CVAR_IALLREDUCE_TREE_BUFFER_PER_CHILD : If set to true, a rank in tree_kary and tree_knomial algorithms will allocate a dedicated buffer for every child it receives data from. This would mean more memory consumption but it would allow preposting of the receives and hence reduce the number of unexpected messages. If set to false, there is only one buffer that is used to receive the data from all the children. The receives are therefore serialized, that is, only one receive can be posted at a time.
20211015 115039.977 INFO             PET0 index=  63                           MPIR_CVAR_IALLREDUCE_RECEXCH_KVAL : k value for recursive exchange based iallreduce
20211015 115039.977 INFO             PET0 index=  64                        MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM : Variable to select iallreduce algorithmauto                     - Internal algorithm selectionnaive                    - Force naive algorithmrecursive_doubling       - Force recursive doubling algorithmreduce_scatter_allgather - Force reduce scatter allgather algorithmrecexch_single_buffer    - Force generic transport recursive exchange with single buffer for receivesrecexch_multiple_buffer  - Force generic transport recursive exchange with multiple buffers for receives
20211015 115039.977 INFO             PET0 index=  65                        MPIR_CVAR_IALLREDUCE_INTER_ALGORITHM : Variable to select iallreduce algorithmauto                      - Internal algorithm selectionremote_reduce_local_bcast - Force remote-reduce-local-bcast algorithm
20211015 115039.977 INFO             PET0 index=  66                      MPIR_CVAR_IALLREDUCE_DEVICE_COLLECTIVE : If set to true, MPI_Iallreduce will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iallreduce function will not be called.
20211015 115039.977 INFO             PET0 index=  67                         MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM : Variable to select ialltoall algorithmauto              - Internal algorithm selectionbrucks            - Force brucks algorithminplace           - Force inplace algorithmpairwise          - Force pairwise algorithmpermuted_sendrecv - Force permuted sendrecv algorithm
20211015 115039.977 INFO             PET0 index=  68                         MPIR_CVAR_IALLTOALL_INTER_ALGORITHM : Variable to select ialltoall algorithmauto              - Internal algorithm selectionpairwise_exchange - Force pairwise exchange algorithm
20211015 115039.977 INFO             PET0 index=  69                       MPIR_CVAR_IALLTOALL_DEVICE_COLLECTIVE : If set to true, MPI_Ialltoall will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ialltoall function will not be called.
20211015 115039.977 INFO             PET0 index=  70                        MPIR_CVAR_IALLTOALLV_INTRA_ALGORITHM : Variable to select ialltoallv algorithmauto              - Internal algorithm selectionblocked           - Force blocked algorithminplace           - Force inplace algorithmpairwise_exchange - Force pairwise exchange algorithm
20211015 115039.977 INFO             PET0 index=  71                        MPIR_CVAR_IALLTOALLV_INTER_ALGORITHM : Variable to select ialltoallv algorithmauto - Internal algorithm selection
20211015 115039.977 INFO             PET0 index=  72                      MPIR_CVAR_IALLTOALLV_DEVICE_COLLECTIVE : If set to true, MPI_Ialltoallv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ialltoallv function will not be called.
20211015 115039.977 INFO             PET0 index=  73                        MPIR_CVAR_IALLTOALLW_INTRA_ALGORITHM : Variable to select ialltoallw algorithmauto              - Internal algorithm selectionblocked           - Force blocked algorithminplace           - Force inplace algorithmpairwise_exchange - Force pairwise exchange algorithm
20211015 115039.977 INFO             PET0 index=  74                        MPIR_CVAR_IALLTOALLW_INTER_ALGORITHM : Variable to select ialltoallw algorithmauto - Internal algorithm selection
20211015 115039.977 INFO             PET0 index=  75                      MPIR_CVAR_IALLTOALLW_DEVICE_COLLECTIVE : If set to true, MPI_Ialltoallw will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ialltoallw function will not be called.
20211015 115039.977 INFO             PET0 index=  76                             MPIR_CVAR_IBARRIER_RECEXCH_KVAL : k value for recursive exchange based ibarrier
20211015 115039.977 INFO             PET0 index=  77                          MPIR_CVAR_IBARRIER_INTRA_ALGORITHM : Variable to select ibarrier algorithmauto               - Internal algorithm selectionrecursive_doubling - Force recursive doubling algorithmrecexch            - Force generic transport based recursive exchange algorithm
20211015 115039.977 INFO             PET0 index=  78                          MPIR_CVAR_IBARRIER_INTER_ALGORITHM : Variable to select ibarrier algorithmauto  - Internal algorithm selectionbcast - Force bcast algorithm
20211015 115039.977 INFO             PET0 index=  79                        MPIR_CVAR_IBARRIER_DEVICE_COLLECTIVE : If set to true, MPI_Ibarrier will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ibarrier function will not be called.
20211015 115039.977 INFO             PET0 index=  80                                  MPIR_CVAR_IBCAST_TREE_KVAL : k value for tree (kary, knomial, etc.) based ibcast
20211015 115039.977 INFO             PET0 index=  81                                  MPIR_CVAR_IBCAST_TREE_TYPE : Tree type for tree based ibcast kary      - kary tree type knomial_1 - knomial_1 tree type knomial_2 - knomial_2 tree type
20211015 115039.977 INFO             PET0 index=  82                   MPIR_CVAR_IBCAST_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based ibcast. Default value is 0, that is, no pipelining by default
20211015 115039.977 INFO             PET0 index=  83                            MPIR_CVAR_IBCAST_RING_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in ibcast ring algorithm. Default value is 0, that is, no pipelining by default
20211015 115039.977 INFO             PET0 index=  84                            MPIR_CVAR_IBCAST_INTRA_ALGORITHM : Variable to select ibcast algorithmauto                                 - Internal algorithm selectionbinomial                             - Force Binomial algorithmscatter_recursive_doubling_allgather - Force Scatter Recursive Doubling Allgather algorithmscatter_ring_allgather               - Force Scatter Ring Allgather algorithmtree                                 - Force Generic Transport Tree algorithmscatter_recexch_allgather            - Force Generic Transport Scatter followed by Recursive Exchange Allgather algorithmring                                 - Force Generic Transport Ring algorithm
20211015 115039.977 INFO             PET0 index=  85                               MPIR_CVAR_IBCAST_SCATTER_KVAL : k value for tree based scatter in scatter_recexch_allgather algorithm
20211015 115039.977 INFO             PET0 index=  86                     MPIR_CVAR_IBCAST_ALLGATHER_RECEXCH_KVAL : k value for recursive exchange based allgather in scatter_recexch_allgather algorithm
20211015 115039.977 INFO             PET0 index=  87                            MPIR_CVAR_IBCAST_INTER_ALGORITHM : Variable to select ibcast algorithmauto - Internal algorithm selectionflat - Force flat algorithm
20211015 115039.977 INFO             PET0 index=  88                          MPIR_CVAR_IBCAST_DEVICE_COLLECTIVE : If set to true, MPI_Ibcast will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually.  If set to false, the device-level ibcast function will not be called.
20211015 115039.977 INFO             PET0 index=  89                           MPIR_CVAR_IEXSCAN_INTRA_ALGORITHM : Variable to select iexscan algorithmauto               - Internal algorithm selectionrecursive_doubling - Force recursive doubling algorithm
20211015 115039.978 INFO             PET0 index=  90                         MPIR_CVAR_IEXSCAN_DEVICE_COLLECTIVE : If set to true, MPI_Iexscan will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iexscan function will not be called.
20211015 115039.978 INFO             PET0 index=  91                           MPIR_CVAR_IGATHER_INTRA_ALGORITHM : Variable to select igather algorithmauto     - Internal algorithm selectionbinomial - Force binomial algorithmtree     - Force genetric transport based tree algorithm
20211015 115039.978 INFO             PET0 index=  92                                 MPIR_CVAR_IGATHER_TREE_KVAL : k value for tree based igather
20211015 115039.978 INFO             PET0 index=  93                           MPIR_CVAR_IGATHER_INTER_ALGORITHM : Variable to select igather algorithmauto        - Internal algorithm selectionlong_inter  - Force long inter algorithmshort_inter - Force short inter algorithm
20211015 115039.978 INFO             PET0 index=  94                         MPIR_CVAR_IGATHER_DEVICE_COLLECTIVE : If set to true, MPI_Igather will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level igather function will not be called.
20211015 115039.978 INFO             PET0 index=  95                          MPIR_CVAR_IGATHERV_INTRA_ALGORITHM : Variable to select igatherv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET0 index=  96                          MPIR_CVAR_IGATHERV_INTER_ALGORITHM : Variable to select igatherv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET0 index=  97                        MPIR_CVAR_IGATHERV_DEVICE_COLLECTIVE : If set to true, MPI_Igatherv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level igatherv function will not be called.
20211015 115039.978 INFO             PET0 index=  98               MPIR_CVAR_INEIGHBOR_ALLGATHER_INTRA_ALGORITHM : Variable to select ineighbor_allgather algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET0 index=  99               MPIR_CVAR_INEIGHBOR_ALLGATHER_INTER_ALGORITHM : Variable to select ineighbor_allgather algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET0 index= 100             MPIR_CVAR_INEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE : If set to true, MPI_ineighbor_allgather will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ineighbor_allgather function will not be called.
20211015 115039.978 INFO             PET0 index= 101              MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTRA_ALGORITHM : Variable to select ineighbor_allgatherv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET0 index= 102              MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTER_ALGORITHM : Variable to select ineighbor_allgatherv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET0 index= 103            MPIR_CVAR_INEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE : If set to true, MPI_ineighbor_allgatherv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ineighbor_allgatherv function will not be called.
20211015 115039.978 INFO             PET0 index= 104                MPIR_CVAR_INEIGHBOR_ALLTOALL_INTRA_ALGORITHM : Variable to select ineighbor_alltoall algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET0 index= 105                MPIR_CVAR_INEIGHBOR_ALLTOALL_INTER_ALGORITHM : Variable to select ineighbor_alltoall algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET0 index= 106              MPIR_CVAR_INEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE : If set to true, MPI_ineighbor_alltoall will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ineighbor_alltoall function will not be called.
20211015 115039.978 INFO             PET0 index= 107               MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTRA_ALGORITHM : Variable to select ineighbor_alltoallv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET0 index= 108               MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTER_ALGORITHM : Variable to select ineighbor_alltoallv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET0 index= 109             MPIR_CVAR_INEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE : If set to true, MPI_ineighbor_alltoallv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ineighbor_alltoallv function will not be called.
20211015 115039.978 INFO             PET0 index= 110               MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTRA_ALGORITHM : Variable to select ineighbor_alltoallw algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET0 index= 111               MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTER_ALGORITHM : Variable to select ineighbor_alltoallw algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET0 index= 112             MPIR_CVAR_INEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE : If set to true, MPI_ineighbor_alltoallw will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ineighbor_alltoallw function will not be called.
20211015 115039.978 INFO             PET0 index= 113                                 MPIR_CVAR_IREDUCE_TREE_KVAL : k value for tree (kary, knomial, etc.) based ireduce
20211015 115039.978 INFO             PET0 index= 114                                 MPIR_CVAR_IREDUCE_TREE_TYPE : Tree type for tree based ireduce kary      - kary tree knomial_1 - knomial_1 tree knomial_2 - knomial_2 tree
20211015 115039.978 INFO             PET0 index= 115                  MPIR_CVAR_IREDUCE_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based ireduce. Default value is 0, that is, no pipelining by default
20211015 115039.978 INFO             PET0 index= 116                           MPIR_CVAR_IREDUCE_RING_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in ireduce ring algorithm. Default value is 0, that is, no pipelining by default
20211015 115039.978 INFO             PET0 index= 117                     MPIR_CVAR_IREDUCE_TREE_BUFFER_PER_CHILD : If set to true, a rank in tree algorithms will allocate a dedicated buffer for every child it receives data from. This would mean more memory consumption but it would allow preposting of the receives and hence reduce the number of unexpected messages. If set to false, there is only one buffer that is used to receive the data from all the children. The receives are therefore serialized, that is, only one receive can be posted at a time.
20211015 115039.978 INFO             PET0 index= 118                           MPIR_CVAR_IREDUCE_INTRA_ALGORITHM : Variable to select ireduce algorithmauto                  - Internal algorithm selectionbinomial              - Force binomial algorithmreduce_scatter_gather - Force reduce scatter gather algorithmtree                  - Force Generic Transport Treering                  - Force Generic Transport Ring
20211015 115039.978 INFO             PET0 index= 119                           MPIR_CVAR_IREDUCE_INTER_ALGORITHM : Variable to select ireduce algorithmauto                     - Internal algorithm selectionlocal_reduce_remote_send - Force local-reduce-remote-send algorithm
20211015 115039.978 INFO             PET0 index= 120                         MPIR_CVAR_IREDUCE_DEVICE_COLLECTIVE : If set to true, MPI_Ireduce will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ireduce function will not be called.
20211015 115039.978 INFO             PET0 index= 121                      MPIR_CVAR_IREDUCE_SCATTER_RECEXCH_KVAL : k value for recursive exchange based ireduce_scatter
20211015 115039.978 INFO             PET0 index= 122                   MPIR_CVAR_IREDUCE_SCATTER_INTRA_ALGORITHM : Variable to select ireduce_scatter algorithmauto               - Internal algorithm selectionnoncommutative     - Force noncommutative algorithmrecursive_doubling - Force recursive doubling algorithmpairwise           - Force pairwise algorithmrecursive_halving  - Force recursive halving algorithmrecexch  - Force generic transport recursive exchange algorithm
20211015 115039.978 INFO             PET0 index= 123                   MPIR_CVAR_IREDUCE_SCATTER_INTER_ALGORITHM : Variable to select ireduce_scatter algorithmauto                         - Internal algorithm selectionremote_reduce_local_scatterv - Force remote-reduce-local-scatterv algorithm
20211015 115039.978 INFO             PET0 index= 124                 MPIR_CVAR_IREDUCE_SCATTER_DEVICE_COLLECTIVE : If set to true, MPI_Ireduce_scatter will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ireduce_scatter function will not be called.
20211015 115039.978 INFO             PET0 index= 125                MPIR_CVAR_IREDUCE_SCATTER_BLOCK_RECEXCH_KVAL : k value for recursive exchange based ireduce_scatter_block
20211015 115039.978 INFO             PET0 index= 126             MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM : Variable to select ireduce_scatter_block algorithmauto               - Internal algorithm selectionnoncommutative     - Force noncommutative algorithmrecursive_doubling - Force recursive doubling algorithmpairwise           - Force pairwise algorithmrecursive_halving  - Force recursive halving algorithmrecexch  - Force generic transport recursive exchange algorithm
20211015 115039.978 INFO             PET0 index= 127             MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTER_ALGORITHM : Variable to select ireduce_scatter_block algorithmauto                         - Internal algorithm selectionremote_reduce_local_scatterv - Force remote-reduce-local-scatterv algorithm
20211015 115039.978 INFO             PET0 index= 128           MPIR_CVAR_IREDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE : If set to true, MPI_Ireduce_scatter_block will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ireduce_scatter_block function will not be called.
20211015 115039.978 INFO             PET0 index= 129                             MPIR_CVAR_ISCAN_INTRA_ALGORITHM : Variable to select allgather algorithmauto               - Internal algorithm selectionrecursive_doubling - Force recursive doubling algorithm
20211015 115039.978 INFO             PET0 index= 130                           MPIR_CVAR_ISCAN_DEVICE_COLLECTIVE : If set to true, MPI_Iscan will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iscan function will not be called.
20211015 115039.978 INFO             PET0 index= 131                          MPIR_CVAR_ISCATTER_INTRA_ALGORITHM : Variable to select iscatter algorithmauto     - Internal algorithm selectionbinomial - Force binomial algorithmtree     - Force genetric transport based tree algorithm
20211015 115039.978 INFO             PET0 index= 132                                MPIR_CVAR_ISCATTER_TREE_KVAL : k value for tree based iscatter
20211015 115039.978 INFO             PET0 index= 133                          MPIR_CVAR_ISCATTER_INTER_ALGORITHM : Variable to select iscatter algorithmauto                      - Internal algorithm selectionlinear                    - Force linear algorithmremote_send_local_scatter - Force remote-send-local-scatter algorithm
20211015 115039.978 INFO             PET0 index= 134                        MPIR_CVAR_ISCATTER_DEVICE_COLLECTIVE : If set to true, MPI_Iscatter will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iscatter function will not be called.
20211015 115039.978 INFO             PET0 index= 135                         MPIR_CVAR_ISCATTERV_INTRA_ALGORITHM : Variable to select iscatterv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET0 index= 136                         MPIR_CVAR_ISCATTERV_INTER_ALGORITHM : Variable to select iscatterv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET0 index= 137                       MPIR_CVAR_ISCATTERV_DEVICE_COLLECTIVE : If set to true, MPI_Iscatterv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iscatterv function will not be called.
20211015 115039.978 INFO             PET0 index= 138                MPIR_CVAR_NEIGHBOR_ALLGATHER_INTRA_ALGORITHM : Variable to select ineighbor_allgather algorithmauto - Internal algorithm selectionnb   - Force nonblocking algorithm
20211015 115039.978 INFO             PET0 index= 139                MPIR_CVAR_NEIGHBOR_ALLGATHER_INTER_ALGORITHM : Variable to select ineighbor_allgather algorithmauto - Internal algorithm selectionnb   - Force nonblocking algorithm
20211015 115039.978 INFO             PET0 index= 140              MPIR_CVAR_NEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE : If set to true, MPI_neighbor_allgather will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level neighbor_allgather function will not be called.
20211015 115039.978 INFO             PET0 index= 141               MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTRA_ALGORITHM : Variable to select neighbor_allgatherv algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET0 index= 142               MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTER_ALGORITHM : Variable to select neighbor_allgatherv algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET0 index= 143             MPIR_CVAR_NEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE : If set to true, MPI_neighbor_allgatherv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level neighbor_allgatherv function will not be called.
20211015 115039.978 INFO             PET0 index= 144                 MPIR_CVAR_NEIGHBOR_ALLTOALL_INTRA_ALGORITHM : Variable to select neighbor_alltoall algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET0 index= 145                 MPIR_CVAR_NEIGHBOR_ALLTOALL_INTER_ALGORITHM : Variable to select neighbor_alltoall algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET0 index= 146               MPIR_CVAR_NEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE : If set to true, MPI_neighbor_alltoall will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level neighbor_alltoall function will not be called.
20211015 115039.978 INFO             PET0 index= 147                MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTRA_ALGORITHM : Variable to select neighbor_alltoallv algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET0 index= 148                MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTER_ALGORITHM : Variable to select neighbor_alltoallv algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET0 index= 149              MPIR_CVAR_NEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE : If set to true, MPI_neighbor_alltoallv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level neighbor_alltoallv function will not be called.
20211015 115039.978 INFO             PET0 index= 150                MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTRA_ALGORITHM : Variable to select neighbor_alltoallw algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET0 index= 151                MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTER_ALGORITHM : Variable to select neighbor_alltoallw algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET0 index= 152              MPIR_CVAR_NEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE : If set to true, MPI_neighbor_alltoallw will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level neighbor_alltoallw function will not be called.
20211015 115039.978 INFO             PET0 index= 153                             MPIR_CVAR_REDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20211015 115039.978 INFO             PET0 index= 154                                 MPIR_CVAR_ENABLE_SMP_REDUCE : Enable SMP aware reduce.
20211015 115039.978 INFO             PET0 index= 155                           MPIR_CVAR_MAX_SMP_REDUCE_MSG_SIZE : Maximum message size for which SMP-aware reduce is used.  A value of '0' uses SMP-aware reduce for all message sizes.
20211015 115039.978 INFO             PET0 index= 156                            MPIR_CVAR_REDUCE_INTRA_ALGORITHM : Variable to select reduce algorithmauto                  - Internal algorithm selectionbinomial              - Force binomial algorithmnb                    - Force nonblocking algorithmreduce_scatter_gather - Force reduce scatter gather algorithm
20211015 115039.978 INFO             PET0 index= 157                            MPIR_CVAR_REDUCE_INTER_ALGORITHM : Variable to select reduce algorithmauto                     - Internal algorithm selectionlocal_reduce_remote_send - Force local-reduce-remote-send algorithmnb                       - Force nonblocking algorithm
20211015 115039.978 INFO             PET0 index= 158                          MPIR_CVAR_REDUCE_DEVICE_COLLECTIVE : If set to true, MPI_Reduce will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level reduce function will not be called.
20211015 115039.978 INFO             PET0 index= 159          MPIR_CVAR_REDUCE_SCATTER_COMMUTATIVE_LONG_MSG_SIZE : the long message algorithm will be used if the operation is commutative and the send buffer size is >= this value (in bytes)
20211015 115039.978 INFO             PET0 index= 160                    MPIR_CVAR_REDUCE_SCATTER_INTRA_ALGORITHM : Variable to select reduce_scatter algorithmauto               - Internal algorithm selectionnb                 - Force nonblocking algorithmnoncommutative     - Force noncommutative algorithmpairwise           - Force pairwise algorithmrecursive_doubling - Force recursive doubling algorithmrecursive_halving  - Force recursive halving algorithm
20211015 115039.978 INFO             PET0 index= 161                    MPIR_CVAR_REDUCE_SCATTER_INTER_ALGORITHM : Variable to select reduce_scatter algorithmauto                        - Internal algorithm selectionnb                          - Force nonblocking algorithmremote_reduce_local_scatter - Force remote-reduce-local-scatter algorithm
20211015 115039.978 INFO             PET0 index= 162                  MPIR_CVAR_REDUCE_SCATTER_DEVICE_COLLECTIVE : If set to true, MPI_Redscat will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level reduce_scatter function will not be called.
20211015 115039.978 INFO             PET0 index= 163              MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM : Variable to select reduce_scatter_block algorithmauto               - Internal algorithm selectionnoncommutative     - Force noncommutative algorithmrecursive_doubling - Force recursive doubling algorithmpairwise           - Force pairwise algorithmrecursive_halving  - Force recursive halving algorithmnb                 - Force nonblocking algorithm
20211015 115039.978 INFO             PET0 index= 164              MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTER_ALGORITHM : Variable to select reduce_scatter_block algorithmauto                        - Internal algorithm selectionnb                          - Force nonblocking algorithmremote_reduce_local_scatter - Force remote-reduce-local-scatter algorithm
20211015 115039.978 INFO             PET0 index= 165            MPIR_CVAR_REDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE : If set to true, MPI_Reduce_scatter_block will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level reduce_scatter_block function will not be called.
20211015 115039.978 INFO             PET0 index= 166                              MPIR_CVAR_SCAN_INTRA_ALGORITHM : Variable to select allgather algorithmauto               - Internal algorithm selectionnb                 - Force nonblocking algorithmrecursive_doubling - Force recursive doubling algorithm
20211015 115039.978 INFO             PET0 index= 167                            MPIR_CVAR_SCAN_DEVICE_COLLECTIVE : If set to true, MPI_Scan will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level scan function will not be called.
20211015 115039.978 INFO             PET0 index= 168                      MPIR_CVAR_SCATTER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Scatter if the send buffer size is < this value (in bytes)
20211015 115039.979 INFO             PET0 index= 169                           MPIR_CVAR_SCATTER_INTRA_ALGORITHM : Variable to select scatter algorithmauto     - Internal algorithm selectionbinomial - Force binomial algorithmnb       - Force nonblocking algorithm
20211015 115039.979 INFO             PET0 index= 170                           MPIR_CVAR_SCATTER_INTER_ALGORITHM : Variable to select scatter algorithmauto                      - Internal algorithm selectionlinear                    - Force linear algorithmnb                        - Force nonblocking algorithmremote_send_local_scatter - Force remote-send-local-scatter algorithm
20211015 115039.979 INFO             PET0 index= 171                         MPIR_CVAR_SCATTER_DEVICE_COLLECTIVE : If set to true, MPI_Scatter will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level scatter function will not be called.
20211015 115039.979 INFO             PET0 index= 172                          MPIR_CVAR_SCATTERV_INTRA_ALGORITHM : Variable to select scatterv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithmnb     - Force nonblocking algorithm
20211015 115039.979 INFO             PET0 index= 173                          MPIR_CVAR_SCATTERV_INTER_ALGORITHM : Variable to select scatterv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithmnb     - Force nonblocking algorithm
20211015 115039.979 INFO             PET0 index= 174                        MPIR_CVAR_SCATTERV_DEVICE_COLLECTIVE : If set to true, MPI_scatterv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level scatterv function will not be called.
20211015 115039.979 INFO             PET0 index= 175                                MPIR_CVAR_DEVICE_COLLECTIVES : If set to true, MPI collectives will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually.  If set to false, the device-level collective function will not be called.
20211015 115039.979 INFO             PET0 index= 176                                MPIR_CVAR_PROGRESS_MAX_COLLS : Maximum number of collective operations at a time that the progress engine should make progress on
20211015 115039.979 INFO             PET0 index= 177                              MPIR_CVAR_COMM_SPLIT_USE_QSORT : Use qsort(3) in the implementation of MPI_Comm_split instead of bubble sort.
20211015 115039.979 INFO             PET0 index= 178                                  MPIR_CVAR_CTXID_EAGER_SIZE : The MPIR_CVAR_CTXID_EAGER_SIZE environment variable allows you to specify how many words in the context ID mask will be set aside for the eager allocation protocol.  If the application is running out of context IDs, reducing this value may help.
20211015 115039.979 INFO             PET0 index= 179                                    MPIR_CVAR_PROCTABLE_SIZE : Size of the "MPIR" debugger interface proctable (process table).
20211015 115039.979 INFO             PET0 index= 180                                   MPIR_CVAR_PROCTABLE_PRINT : If true, dump the proctable entries at MPII_Wait_for_debugger-time.
20211015 115039.979 INFO             PET0 index= 181                                 MPIR_CVAR_PRINT_ERROR_STACK : If true, print an error stack trace at error handling time.
20211015 115039.979 INFO             PET0 index= 182                                  MPIR_CVAR_CHOP_ERROR_STACK : If >0, truncate error stack output lines this many characters wide.  If 0, do not truncate, and if <0 use a sensible default.
20211015 115039.979 INFO             PET0 index= 183                            MPIR_CVAR_SUPPRESS_ABORT_MESSAGE : Disable printing of abort error message.
20211015 115039.979 INFO             PET0 index= 184                                           MPIR_CVAR_MEMDUMP : If true, list any memory that was allocated by MPICH and that remains allocated when MPI_Finalize completes.
20211015 115039.979 INFO             PET0 index= 185                          MPIR_CVAR_MEM_CATEGORY_INFORMATION : If true, print a summary of memory allocation by category. The category definitions are found in mpl_trmem.h.
20211015 115039.979 INFO             PET0 index= 186                                    MPIR_CVAR_ASYNC_PROGRESS : If set to true, MPICH will initiate an additional thread to make asynchronous progress on all communication operations including point-to-point, collective, one-sided operations and I/O.  Setting this variable will automatically increase the thread-safety level to MPI_THREAD_MULTIPLE.  While this improves the progress semantics, it might cause a small amount of performance overhead for regular MPI operations.  The user is encouraged to leave one or more hardware threads vacant in order to prevent contention between the application threads and the progress thread(s).  The impact of oversubscription is highly system dependent but may be substantial in some cases, hence this recommendation.
20211015 115039.979 INFO             PET0 index= 187                              MPIR_CVAR_DEFAULT_THREAD_LEVEL : Sets the default thread level to use when using MPI_INIT. This variable is case-insensitive.
20211015 115039.979 INFO             PET0 index= 188                                        MPIR_CVAR_DEBUG_HOLD : If true, causes processes to wait in MPI_Init and MPI_Initthread for a debugger to be attached.  Once the debugger has attached, the variable 'hold' should be set to 0 in order to allow the process to continue (e.g., in gdb, "set hold=0").
20211015 115039.979 INFO             PET0 index= 189                                    MPIR_CVAR_ERROR_CHECKING : If true, perform checks for errors, typically to verify valid inputs to MPI routines.  Only effective when MPICH is configured with --enable-error-checking=runtime .
20211015 115039.979 INFO             PET0 index= 190                                  MPIR_CVAR_NETLOC_NODE_FILE : Subnet json file
20211015 115039.979 INFO             PET0 index= 191                                      MPIR_CVAR_DIMS_VERBOSE : If true, enable verbose output about the actions of the implementation of MPI_Dims_create.
20211015 115039.979 INFO             PET0 index= 192                              MPIR_CVAR_NAMESERV_FILE_PUBDIR : Sets the directory to use for MPI service publishing in the file nameserv implementation.  Allows the user to override where the publish and lookup information is placed for connect/accept based applications.
20211015 115039.979 INFO             PET0 index= 193                           MPIR_CVAR_ABORT_ON_LEAKED_HANDLES : If true, MPI will call MPI_Abort at MPI_Finalize if any MPI object handles have been leaked.  For example, if MPI_Comm_dup is called without calling a corresponding MPI_Comm_free.  For uninteresting reasons, enabling this option may prevent all known object leaks from being reported.  MPICH must have been configure with "--enable-g=handlealloc" or better in order for this functionality to work.
20211015 115039.979 INFO             PET0 index= 194                                           MPIR_CVAR_NOLOCAL : If true, force all processes to operate as though all processes are located on another node.  For example, this disables shared memory communication hierarchical collectives.
20211015 115039.979 INFO             PET0 index= 195                                  MPIR_CVAR_ODD_EVEN_CLIQUES : If true, odd procs on a node are seen as local to each other, and even procs on a node are seen as local to each other.  Used for debugging on a single machine. Deprecated in favor of MPIR_CVAR_NUM_CLIQUES.
20211015 115039.979 INFO             PET0 index= 196                                       MPIR_CVAR_NUM_CLIQUES : Specify the number of cliques that should be used to partition procs on a local node. Procs with the same clique number are seen as local to each other. Used for debugging on a single machine.
20211015 115039.979 INFO             PET0 index= 197                                  MPIR_CVAR_COLL_ALIAS_CHECK : Enable checking of aliasing in collective operations
20211015 115039.979 INFO             PET0 index= 198                                 MPIR_CVAR_REQUEST_POLL_FREQ : How frequent to poll during completion calls (wait/test) in terms of number of processed requests before polling.
20211015 115039.979 INFO             PET0 index= 199                                MPIR_CVAR_REQUEST_BATCH_SIZE : The number of requests to make completion as a batch in MPI_Waitall and MPI_Testall implementation. A large number is likely to cause more cache misses.
20211015 115039.979 INFO             PET0 index= 200                                MPIR_CVAR_POLLS_BEFORE_YIELD : When MPICH is in a busy waiting loop, it will periodically call a function to yield the processor.  This cvar sets the number of loops before the yield function is called.  A value of 0 disables yielding.
20211015 115039.979 INFO             PET0 index= 201                          MPIR_CVAR_NEMESIS_MXM_BULK_CONNECT : If true, force mxm to connect all processes at initialization time.
20211015 115039.979 INFO             PET0 index= 202                       MPIR_CVAR_NEMESIS_MXM_BULK_DISCONNECT : If true, force mxm to disconnect all processes at finalization time.
20211015 115039.979 INFO             PET0 index= 203                              MPIR_CVAR_NEMESIS_MXM_HUGEPAGE : If true, mxm tries detecting hugepage support.  On HPC-X 2.3 and earlier, this might cause problems on Ubuntu and other platforms even if the system provides hugepage support.
20211015 115039.979 INFO             PET0 index= 204                                  MPIR_CVAR_OFI_USE_PROVIDER : If non-null, choose an OFI provider by name. If using with the CH4 device and using a provider that supports an older version of the libfabric API then the default version of the installed library, specifying the OFI version via the appropriate CVARs is also recommended.
20211015 115039.979 INFO             PET0 index= 205                                MPIR_CVAR_OFI_DUMP_PROVIDERS : If true, dump provider information at init
20211015 115039.979 INFO             PET0 index= 206                            MPIR_CVAR_CH3_INTERFACE_HOSTNAME : If non-NULL, this cvar specifies the IP address that other processes should use when connecting to this process. This cvar is mutually exclusive with the MPIR_CVAR_CH3_NETWORK_IFACE cvar and it is an error to set them both.
20211015 115039.979 INFO             PET0 index= 207                                    MPIR_CVAR_CH3_PORT_RANGE : The MPIR_CVAR_CH3_PORT_RANGE environment variable allows you to specify the range of TCP ports to be used by the process manager and the MPICH library. The format of this variable is <low>:<high>.  To specify any available port, use 0:0.
20211015 115039.979 INFO             PET0 index= 208                         MPIR_CVAR_NEMESIS_TCP_NETWORK_IFACE : If non-NULL, this cvar specifies which pseudo-ethernet interface the tcp netmod should use (e.g., "eth1", "ib0"). Note, this is a Linux-specific cvar. This cvar is mutually exclusive with the MPIR_CVAR_CH3_INTERFACE_HOSTNAME cvar and it is an error to set them both.
20211015 115039.979 INFO             PET0 index= 209                   MPIR_CVAR_NEMESIS_TCP_HOST_LOOKUP_RETRIES : This cvar controls the number of times to retry the gethostbyname() function before giving up.
20211015 115039.979 INFO             PET0 index= 210                            MPIR_CVAR_NEMESIS_ENABLE_CKPOINT : If true, enables checkpointing support and returns an error if checkpointing library cannot be initialized.
20211015 115039.979 INFO             PET0 index= 211                          MPIR_CVAR_NEMESIS_SHM_EAGER_MAX_SZ : This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for shared memory. If this cvar is set to -1, then Nemesis will choose an appropriate value.
20211015 115039.979 INFO             PET0 index= 212                    MPIR_CVAR_NEMESIS_SHM_READY_EAGER_MAX_SZ : This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for ready-send messages.  If this cvar is set to -1, then ready messages will always be sent eagerly.  If this cvar is set to -2, then Nemesis will choose an appropriate value.
20211015 115039.979 INFO             PET0 index= 213                                         MPIR_CVAR_ENABLE_FT : Enable fault tolerance functions
20211015 115039.979 INFO             PET0 index= 214                         MPIR_CVAR_NEMESIS_LMT_DMA_THRESHOLD : Messages larger than this size will use the "dma" (knem) intranode LMT implementation, if it is enabled and available.
20211015 115039.979 INFO             PET0 index= 215                                    MPIR_CVAR_NEMESIS_NETMOD : If non-empty, this cvar specifies which network module should be used for communication. This variable is case-insensitive.
20211015 115039.979 INFO             PET0 index= 216                                  MPIR_CVAR_CH3_ENABLE_HCOLL : If true, enable HCOLL collectives.
20211015 115039.979 INFO             PET0 index= 217                          MPIR_CVAR_CH3_COMM_CONNECT_TIMEOUT : The default time out period in seconds for a connection attempt to the server communicator where the named port exists but no pending accept. User can change the value for a specified connection through its info argument.
20211015 115039.979 INFO             PET0 index= 218               MPIR_CVAR_CH3_RMA_OP_PIGGYBACK_LOCK_DATA_SIZE : Specify the threshold of data size of a RMA operation which can be piggybacked with a LOCK message. It is always a positive value and should not be smaller than MPIDI_RMA_IMMED_BYTES. If user sets it as a small value, for middle and large data size, we will lose performance because of always waiting for round-trip of LOCK synchronization; if user sets it as a large value, we need to consume more memory on target side to buffer this lock request when lock is not satisfied.
20211015 115039.979 INFO             PET0 index= 219                      MPIR_CVAR_CH3_RMA_ACTIVE_REQ_THRESHOLD : Threshold of number of active requests to trigger blocking waiting in operation routines. When the value is negative, we never blockingly wait in operation routines. When the value is zero, we always trigger blocking waiting in operation routines to wait until no. of active requests becomes zero. When the value is positive, we do blocking waiting in operation routines to wait until no. of active requests being reduced to this value.
20211015 115039.979 INFO             PET0 index= 220               MPIR_CVAR_CH3_RMA_POKE_PROGRESS_REQ_THRESHOLD : Threshold at which the RMA implementation attempts to complete requests while completing RMA operations and while using the lazy synchonization approach.  Change this value if programs fail because they run out of requests or other internal resources
20211015 115039.979 INFO             PET0 index= 221                MPIR_CVAR_CH3_RMA_SCALABLE_FENCE_PROCESS_NUM : Specify the threshold of switching the algorithm used in FENCE from the basic algorithm to the scalable algorithm. The value can be nagative, zero or positive. When the number of processes is larger than or equal to this value, FENCE will use a scalable algorithm which do not use O(P) data structure; when the number of processes is smaller than the value, FENCE will use a basic but fast algorithm which requires an O(P) data structure.
20211015 115039.979 INFO             PET0 index= 222            MPIR_CVAR_CH3_RMA_DELAY_ISSUING_FOR_PIGGYBACKING : Specify if delay issuing of RMA operations for piggybacking LOCK/UNLOCK/FLUSH is enabled. It can be either 0 or 1. When it is set to 1, the issuing of LOCK message is delayed until origin process see the first RMA operation and piggyback LOCK with that operation, and the origin process always keeps the current last operation until the ending synchronization call in order to piggyback UNLOCK/FLUSH with that operation. When it is set to 0, in WIN_LOCK/UNLOCK case, the LOCK message is sent out as early as possible, in WIN_LOCK_ALL/UNLOCK_ALL case, the origin process still tries to piggyback LOCK message with the first operation; for UNLOCK/FLUSH message, the origin process no longer keeps the current last operation but only piggyback UNLOCK/FLUSH if there is an operation avaliable in the ending synchronization call.
20211015 115039.979 INFO             PET0 index= 223                                MPIR_CVAR_CH3_RMA_SLOTS_SIZE : Number of RMA slots during window creation. Each slot contains a linked list of target elements. The distribution of ranks among slots follows a round-robin pattern. Requires a positive value.
20211015 115039.979 INFO             PET0 index= 224                    MPIR_CVAR_CH3_RMA_TARGET_LOCK_DATA_BYTES : Size (in bytes) of available lock data this window can provided. If current buffered lock data is more than this value, the process will drop the upcoming operation data. Requires a positive calue.
20211015 115039.979 INFO             PET0 index= 225                            MPIR_CVAR_CH3_EAGER_MAX_MSG_SIZE : This cvar controls the message size at which CH3 switches from eager to rendezvous mode.
20211015 115039.979 INFO             PET0 index= 226                          MPIR_CVAR_CH3_RMA_OP_WIN_POOL_SIZE : Size of the window-private RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediately.  Requires a positive value.
20211015 115039.979 INFO             PET0 index= 227                       MPIR_CVAR_CH3_RMA_OP_GLOBAL_POOL_SIZE : Size of the Global RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediatly.  Requires a positive value.
20211015 115039.979 INFO             PET0 index= 228                      MPIR_CVAR_CH3_RMA_TARGET_WIN_POOL_SIZE : Size of the window-private RMA target pool (in number of targets) that stores information about RMA targets that could not be issued immediately.  Requires a positive value.
20211015 115039.979 INFO             PET0 index= 229                   MPIR_CVAR_CH3_RMA_TARGET_GLOBAL_POOL_SIZE : Size of the Global RMA targets pool (in number of targets) that stores information about RMA targets that could not be issued immediatly.  Requires a positive value.
20211015 115039.979 INFO             PET0 index= 230           MPIR_CVAR_CH3_RMA_TARGET_LOCK_ENTRY_WIN_POOL_SIZE : Size of the window-private RMA lock entries pool (in number of lock entries) that stores information about RMA lock requests that could not be satisfied immediatly.  Requires a positive value.
20211015 115039.979 INFO             PET0 index= 231                     MPIR_CVAR_CH4_OFI_CAPABILITY_SETS_DEBUG : Prints out the configuration of each capability selected via the capability sets interface.
20211015 115039.979 INFO             PET0 index= 232                               MPIR_CVAR_CH4_OFI_ENABLE_DATA : Enable immediate data fields in OFI to transmit source rank outside of the match bits
20211015 115039.979 INFO             PET0 index= 233                           MPIR_CVAR_CH4_OFI_ENABLE_AV_TABLE : If true, the OFI addressing information will be stored with an FI_AV_TABLE. If false, an FI_AV_MAP will be used.
20211015 115039.979 INFO             PET0 index= 234                 MPIR_CVAR_CH4_OFI_ENABLE_SCALABLE_ENDPOINTS : If true, use OFI scalable endpoints.
20211015 115039.979 INFO             PET0 index= 235                    MPIR_CVAR_CH4_OFI_ENABLE_SHARED_CONTEXTS : If set to false (zero), MPICH does not use OFI shared contexts. If set to -1, it is determined by the OFI capability sets based on the provider. Otherwise, MPICH tries to use OFI shared contexts. If they are unavailable, it'll fall back to the mode without shared contexts.
20211015 115039.979 INFO             PET0 index= 236                        MPIR_CVAR_CH4_OFI_ENABLE_MR_SCALABLE : If true, MR_SCALABLE for OFI memory regions. If false, MR_BASIC for OFI memory regions.
20211015 115039.979 INFO             PET0 index= 237                             MPIR_CVAR_CH4_OFI_ENABLE_TAGGED : If true, use tagged message transmission functions in OFI.
20211015 115039.979 INFO             PET0 index= 238                                 MPIR_CVAR_CH4_OFI_ENABLE_AM : If true, enable OFI active message support.
20211015 115039.979 INFO             PET0 index= 239                                MPIR_CVAR_CH4_OFI_ENABLE_RMA : If true, enable OFI RMA support.
20211015 115039.979 INFO             PET0 index= 240                            MPIR_CVAR_CH4_OFI_ENABLE_ATOMICS : If true, enable OFI Atomics support.
20211015 115039.979 INFO             PET0 index= 241                       MPIR_CVAR_CH4_OFI_FETCH_ATOMIC_IOVECS : Specifies the maximum number of iovecs that can be used by the OFI provider for fetch_atomic operations. The default value is -1, indicating that no value is set.
20211015 115039.979 INFO             PET0 index= 242                 MPIR_CVAR_CH4_OFI_ENABLE_DATA_AUTO_PROGRESS : If true, enable MPI data auto progress.
20211015 115039.979 INFO             PET0 index= 243              MPIR_CVAR_CH4_OFI_ENABLE_CONTROL_AUTO_PROGRESS : If true, enable MPI control auto progress.
20211015 115039.979 INFO             PET0 index= 244                       MPIR_CVAR_CH4_OFI_ENABLE_PT2PT_NOPACK : If true, enable iovec for pt2pt.
20211015 115039.979 INFO             PET0 index= 245                           MPIR_CVAR_CH4_OFI_CONTEXT_ID_BITS : Specifies the number of bits that will be used for matching the context ID. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20211015 115039.979 INFO             PET0 index= 246                                 MPIR_CVAR_CH4_OFI_RANK_BITS : Specifies the number of bits that will be used for matching the MPI rank. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20211015 115039.979 INFO             PET0 index= 247                                  MPIR_CVAR_CH4_OFI_TAG_BITS : Specifies the number of bits that will be used for matching the user tag. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20211015 115039.979 INFO             PET0 index= 248                             MPIR_CVAR_CH4_OFI_MAJOR_VERSION : Specifies the major version of the OFI library. The default is the major version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
20211015 115039.979 INFO             PET0 index= 249                             MPIR_CVAR_CH4_OFI_MINOR_VERSION : Specifies the major version of the OFI library. The default is the minor version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
20211015 115039.979 INFO             PET0 index= 250                                  MPIR_CVAR_CH4_OFI_MAX_VNIS : If set to positive, this CVAR specifies the maximum number of CH4 VNIs that OFI netmod exposes.
20211015 115039.979 INFO             PET0 index= 251                           MPIR_CVAR_CH4_OFI_MAX_RMA_SEP_CTX : If set to positive, this CVAR specifies the maximum number of transmit contexts RMA can utilize in a scalable endpoint. This value is effective only when scalable endpoint is available, otherwise it will be ignored.
20211015 115039.979 INFO             PET0 index= 252                          MPIR_CVAR_CH4_OFI_MAX_EAGAIN_RETRY : If set to positive, this CVAR specifies the maximum number of retries of an ofi operations before returning MPIX_ERR_EAGAIN. This value is effective only when the communicator has the MPI_OFI_set_eagain info hint set to true.
20211015 115039.979 INFO             PET0 index= 253                            MPIR_CVAR_CH4_OFI_NUM_AM_BUFFERS : Specifies the number of buffers for receiving active messages.
20211015 115039.979 INFO             PET0 index= 254                                        MPIR_CVAR_CH4_NETMOD : If non-empty, this cvar specifies which network module to use
20211015 115039.979 INFO             PET0 index= 255                                           MPIR_CVAR_CH4_SHM : If non-empty, this cvar specifies which shm module to use
20211015 115039.979 INFO             PET0 index= 256                                MPIR_CVAR_CH4_ROOTS_ONLY_PMI : Enables an optimized business card exchange over PMI for node root processes only.
20211015 115039.979 INFO             PET0 index= 257                            MPIR_CVAR_CH4_RUNTIME_CONF_DEBUG : If enabled, CH4-level runtime configurations are printed out
20211015 115039.979 INFO             PET0 index= 258                                      MPIR_CVAR_CH4_MT_MODEL : Specifies the CH4 multi-threading model. Possible values are: direct (default) handoff trylock
20211015 115039.979 INFO             PET0 index= 259                          MPIR_CVAR_CH4_COMM_CONNECT_TIMEOUT : The default time out period in seconds for a connection attempt to the server communicator where the named port exists but no pending accept. User can change the value for a specified connection through its info argument.
20211015 115039.979 INFO             PET0 index= 260                             MPIR_CVAR_CH4_RANDOM_ADDR_RETRY : The default number of retries for generating a random address. A retrying involves only local operations.
20211015 115039.979 INFO             PET0 index= 261                             MPIR_CVAR_CH4_SHM_SYMHEAP_RETRY : The default number of retries for allocating a symmetric heap in shared memory. A retrying involves collective communication over the group in the shared memory.
20211015 115039.979 INFO             PET0 index= 262                             MPIR_CVAR_CH4_RMA_MEM_EFFICIENT : If true, memory-saving mode is on, per-target object is released at the epoch end call. If false, performance-efficient mode is on, all allocated target objects are cached and freed at win_finalize.
20211015 115039.979 INFO             PET0 index= 263                                      MPIR_CVAR_ENABLE_HCOLL : Enable hcoll collective support.
20211015 115039.980 INFO             PET0 index= 264                                   MPIR_CVAR_COLL_SCHED_DUMP : Print schedule data for nonblocking collective operations.
20211015 115039.980 INFO             PET0 --- VMK::logSystem() end ---------------------------------
20211015 115039.980 INFO             PET0 main: --- VMK::log() start -------------------------------------
20211015 115039.980 INFO             PET0 main: vm located at: 0x7f9f6840cf90
20211015 115039.980 INFO             PET0 main: petCount=6 localPet=0 mypthid=0 currentSsiPe=-1
20211015 115039.980 INFO             PET0 main: Current system level OMP_NUM_THREADS setting for local PET: 12
20211015 115039.980 INFO             PET0 main: ssiCount=1 localSsi=0
20211015 115039.980 INFO             PET0 main: mpionly=1 threadsflag=0
20211015 115039.980 INFO             PET0 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20211015 115039.980 INFO             PET0 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20211015 115039.980 INFO             PET0 main:  PE=0 SSI=0 SSIPE=0
20211015 115039.980 INFO             PET0 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20211015 115039.980 INFO             PET0 main:  PE=1 SSI=0 SSIPE=1
20211015 115039.980 INFO             PET0 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20211015 115039.980 INFO             PET0 main:  PE=2 SSI=0 SSIPE=2
20211015 115039.980 INFO             PET0 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20211015 115039.980 INFO             PET0 main:  PE=3 SSI=0 SSIPE=3
20211015 115039.980 INFO             PET0 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20211015 115039.980 INFO             PET0 main:  PE=4 SSI=0 SSIPE=4
20211015 115039.980 INFO             PET0 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20211015 115039.980 INFO             PET0 main:  PE=5 SSI=0 SSIPE=5
20211015 115039.980 INFO             PET0 main: --- VMK::log() end ---------------------------------------
20211015 115039.980 INFO             PET0 Executing 'userm1_setvm'
20211015 115039.980 INFO             PET0 Executing 'userm1_register'
20211015 115039.980 INFO             PET0 Executing 'userm2_setvm'
20211015 115039.980 INFO             PET0 Executing 'userm2_register'
20211015 115039.980 INFO             PET0 model1: --- VMK::log() start -------------------------------------
20211015 115039.980 INFO             PET0 model1: vm located at: 0x7f9f68439700
20211015 115039.980 INFO             PET0 model1: petCount=6 localPet=0 mypthid=0 currentSsiPe=-1
20211015 115039.980 INFO             PET0 model1: Current system level OMP_NUM_THREADS setting for local PET: 12
20211015 115039.980 INFO             PET0 model1: ssiCount=1 localSsi=0
20211015 115039.980 INFO             PET0 model1: mpionly=1 threadsflag=0
20211015 115039.980 INFO             PET0 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20211015 115039.980 INFO             PET0 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20211015 115039.980 INFO             PET0 model1:  PE=0 SSI=0 SSIPE=0
20211015 115039.980 INFO             PET0 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20211015 115039.980 INFO             PET0 model1:  PE=1 SSI=0 SSIPE=1
20211015 115039.980 INFO             PET0 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20211015 115039.980 INFO             PET0 model1:  PE=2 SSI=0 SSIPE=2
20211015 115039.980 INFO             PET0 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20211015 115039.980 INFO             PET0 model1:  PE=3 SSI=0 SSIPE=3
20211015 115039.980 INFO             PET0 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20211015 115039.980 INFO             PET0 model1:  PE=4 SSI=0 SSIPE=4
20211015 115039.980 INFO             PET0 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20211015 115039.980 INFO             PET0 model1:  PE=5 SSI=0 SSIPE=5
20211015 115039.980 INFO             PET0 model1: --- VMK::log() end ---------------------------------------
20211015 115039.985 INFO             PET0 Entering 'user1_run'
20211015 115039.985 INFO             PET0  user1_run: on SSIPE:           -1  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20211015 115040.387 INFO             PET0  user1_run: on SSIPE:           -1  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20211015 115040.635 INFO             PET0  user1_run: on SSIPE:           -1  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20211015 115040.893 INFO             PET0  user1_run: on SSIPE:           -1  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20211015 115041.140 INFO             PET0  user1_run: on SSIPE:           -1  Filling data lbound:           1           1           1  ubound:        1667        1200          10
20211015 115041.395 INFO             PET0 Exiting 'user1_run'
20211015 115041.412 INFO             PET0 Entering 'user2_run'
20211015 115041.412 INFO             PET0 model2: --- VMK::log() start -------------------------------------
20211015 115041.412 INFO             PET0 model2: vm located at: 0x7f9f68439ea0
20211015 115041.412 INFO             PET0 model2: petCount=6 localPet=0 mypthid=0 currentSsiPe=-1
20211015 115041.412 INFO             PET0 model2: Current system level OMP_NUM_THREADS setting for local PET: 12
20211015 115041.412 INFO             PET0 model2: ssiCount=1 localSsi=0
20211015 115041.412 INFO             PET0 model2: mpionly=1 threadsflag=0
20211015 115041.412 INFO             PET0 model2: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20211015 115041.412 INFO             PET0 model2: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20211015 115041.412 INFO             PET0 model2:  PE=0 SSI=0 SSIPE=0
20211015 115041.412 INFO             PET0 model2: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20211015 115041.412 INFO             PET0 model2:  PE=1 SSI=0 SSIPE=1
20211015 115041.412 INFO             PET0 model2: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20211015 115041.412 INFO             PET0 model2:  PE=2 SSI=0 SSIPE=2
20211015 115041.412 INFO             PET0 model2: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20211015 115041.412 INFO             PET0 model2:  PE=3 SSI=0 SSIPE=3
20211015 115041.412 INFO             PET0 model2: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20211015 115041.412 INFO             PET0 model2:  PE=4 SSI=0 SSIPE=4
20211015 115041.412 INFO             PET0 model2: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20211015 115041.412 INFO             PET0 model2:  PE=5 SSI=0 SSIPE=5
20211015 115041.412 INFO             PET0 model2: --- VMK::log() end ---------------------------------------
20211015 115041.412 INFO             PET0  user2_run: ssiLocalDeCount=           6
20211015 115041.413 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:           -1  Testing data for localDe =           2  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20211015 115041.413 INFO             PET0  user2_run: OpenMP thread:           5  on SSIPE:           -1  Testing data for localDe =           5  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20211015 115041.413 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:           -1  Testing data for localDe =           1  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20211015 115041.423 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:           -1  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20211015 115041.433 INFO             PET0  user2_run: OpenMP thread:           4  on SSIPE:           -1  Testing data for localDe =           4  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20211015 115041.453 INFO             PET0  user2_run: OpenMP thread:           3  on SSIPE:           -1  Testing data for localDe =           3  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20211015 115042.894 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:           -1  Testing data for localDe =           2  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20211015 115042.894 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:           -1  Testing data for localDe =           1  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20211015 115042.894 INFO             PET0  user2_run: OpenMP thread:           5  on SSIPE:           -1  Testing data for localDe =           5  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20211015 115042.894 INFO             PET0  user2_run: OpenMP thread:           4  on SSIPE:           -1  Testing data for localDe =           4  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20211015 115042.894 INFO             PET0  user2_run: OpenMP thread:           3  on SSIPE:           -1  Testing data for localDe =           3  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20211015 115042.896 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:           -1  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20211015 115044.231 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:           -1  Testing data for localDe =           2  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20211015 115044.231 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:           -1  Testing data for localDe =           1  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20211015 115044.232 INFO             PET0  user2_run: OpenMP thread:           5  on SSIPE:           -1  Testing data for localDe =           5  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20211015 115044.232 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:           -1  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20211015 115044.232 INFO             PET0  user2_run: OpenMP thread:           3  on SSIPE:           -1  Testing data for localDe =           3  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20211015 115044.232 INFO             PET0  user2_run: OpenMP thread:           4  on SSIPE:           -1  Testing data for localDe =           4  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20211015 115045.572 INFO             PET0  user2_run: OpenMP thread:           5  on SSIPE:           -1  Testing data for localDe =           5  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20211015 115045.572 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:           -1  Testing data for localDe =           2  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20211015 115045.572 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:           -1  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20211015 115045.572 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:           -1  Testing data for localDe =           1  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20211015 115045.573 INFO             PET0  user2_run: OpenMP thread:           4  on SSIPE:           -1  Testing data for localDe =           4  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20211015 115045.573 INFO             PET0  user2_run: OpenMP thread:           3  on SSIPE:           -1  Testing data for localDe =           3  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20211015 115046.914 INFO             PET0  user2_run: OpenMP thread:           2  on SSIPE:           -1  Testing data for localDe =           2  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20211015 115046.916 INFO             PET0  user2_run: OpenMP thread:           4  on SSIPE:           -1  Testing data for localDe =           4  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20211015 115046.916 INFO             PET0  user2_run: OpenMP thread:           5  on SSIPE:           -1  Testing data for localDe =           5  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20211015 115046.916 INFO             PET0  user2_run: OpenMP thread:           1  on SSIPE:           -1  Testing data for localDe =           1  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20211015 115046.921 INFO             PET0  user2_run: OpenMP thread:           0  on SSIPE:           -1  Testing data for localDe =           0  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20211015 115046.924 INFO             PET0  user2_run: OpenMP thread:           3  on SSIPE:           -1  Testing data for localDe =           3  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20211015 115048.257 INFO             PET0  user2_run: All data correct.
20211015 115048.257 INFO             PET0 Exiting 'user2_run'
20211015 115048.318 INFO             PET0  NUMBER_OF_PROCESSORS           6
20211015 115048.318 INFO             PET0  PASS  System Test ESMF_FieldSharedDeSSISTest, ESMF_FieldSharedDeSSISTest.F90, line 276
20211015 115048.318 INFO             PET0 Finalizing ESMF
20211015 115039.974 INFO             PET1 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20211015 115039.976 INFO             PET1 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20211015 115039.976 INFO             PET1 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20211015 115039.976 INFO             PET1 !!! FOR PRODUCTION RUNS, USE:                      !!!
20211015 115039.976 INFO             PET1 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20211015 115039.976 INFO             PET1 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20211015 115039.976 INFO             PET1 Running with ESMF Version   : ESMF_8_2_0_beta_snapshot_22-10-g6974ca47a6
20211015 115039.976 INFO             PET1 ESMF library build date/time: "Oct 15 2021" "11:35:17"
20211015 115039.976 INFO             PET1 ESMF library build location : /Volumes/esmf/rocky/esmf-testing/gfortran_9.3.0_mpich3_O_release_8.2.0
20211015 115039.976 INFO             PET1 ESMF_COMM                   : mpich3
20211015 115039.976 INFO             PET1 ESMF_MOAB                   : enabled
20211015 115039.976 INFO             PET1 ESMF_LAPACK                 : enabled
20211015 115039.976 INFO             PET1 ESMF_NETCDF                 : enabled
20211015 115039.976 INFO             PET1 ESMF_PNETCDF                : disabled
20211015 115039.976 INFO             PET1 ESMF_PIO                    : enabled
20211015 115039.976 INFO             PET1 ESMF_YAMLCPP                : enabled
20211015 115039.976 INFO             PET1 --- VMK::logSystem() start -------------------------------
20211015 115039.976 INFO             PET1 esmfComm=mpich3
20211015 115039.976 INFO             PET1 isPthreadsEnabled=0
20211015 115039.976 INFO             PET1 isOpenMPEnabled=1
20211015 115039.976 INFO             PET1 isOpenACCEnabled=0
20211015 115039.976 INFO             PET1 isSsiSharedMemoryEnabled=1
20211015 115039.976 INFO             PET1 ssiCount=1 peCount=6
20211015 115039.976 INFO             PET1 PE=0 SSI=0 SSIPE=0
20211015 115039.976 INFO             PET1 PE=1 SSI=0 SSIPE=1
20211015 115039.976 INFO             PET1 PE=2 SSI=0 SSIPE=2
20211015 115039.976 INFO             PET1 PE=3 SSI=0 SSIPE=3
20211015 115039.976 INFO             PET1 PE=4 SSI=0 SSIPE=4
20211015 115039.976 INFO             PET1 PE=5 SSI=0 SSIPE=5
20211015 115039.976 INFO             PET1 --- VMK::logSystem() MPI Control Variables ---------------
20211015 115039.976 INFO             PET1 index=   0                          MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the short message algorithm will be used if the send buffer size is < this value (in bytes). (See also: MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE)
20211015 115039.976 INFO             PET1 index=   1                           MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the long message algorithm will be used if the send buffer size is >= this value (in bytes) (See also: MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE)
20211015 115039.976 INFO             PET1 index=   2                         MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM : Variable to select allgather algorithmauto               - Internal algorithm selectionbrucks             - Force brucks algorithmnb                 - Force nonblocking algorithmrecursive_doubling - Force recursive doubling algorithmring               - Force ring algorithm
20211015 115039.976 INFO             PET1 index=   3                         MPIR_CVAR_ALLGATHER_INTER_ALGORITHM : Variable to select allgather algorithmauto                      - Internal algorithm selectionlocal_gather_remote_bcast - Force local-gather-remote-bcast algorithmnb                        - Force nonblocking algorithm
20211015 115039.976 INFO             PET1 index=   4                       MPIR_CVAR_ALLGATHER_DEVICE_COLLECTIVE : If set to true, MPI_Allgather will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level allgather function will not be called.
20211015 115039.976 INFO             PET1 index=   5                      MPIR_CVAR_ALLGATHERV_PIPELINE_MSG_SIZE : The smallest message size that will be used for the pipelined, large-message, ring algorithm in the MPI_Allgatherv implementation.
20211015 115039.976 INFO             PET1 index=   6                        MPIR_CVAR_ALLGATHERV_INTRA_ALGORITHM : Variable to select allgatherv algorithmauto               - Internal algorithm selectionbrucks             - Force brucks algorithmnb                 - Force nonblocking algorithmrecursive_doubling - Force recursive doubling algorithmring               - Force ring algorithm
20211015 115039.976 INFO             PET1 index=   7                        MPIR_CVAR_ALLGATHERV_INTER_ALGORITHM : Variable to select allgatherv algorithmauto                      - Internal algorithm selectionnb                        - Force nonblocking algorithmremote_gather_local_bcast - Force remote-gather-local-bcast algorithm
20211015 115039.976 INFO             PET1 index=   8                      MPIR_CVAR_ALLGATHERV_DEVICE_COLLECTIVE : If set to true, MPI_Allgatherv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level allgatherv function will not be called.
20211015 115039.976 INFO             PET1 index=   9                          MPIR_CVAR_ALLREDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20211015 115039.976 INFO             PET1 index=  10                            MPIR_CVAR_ENABLE_SMP_COLLECTIVES : Enable SMP aware collective communication.
20211015 115039.976 INFO             PET1 index=  11                              MPIR_CVAR_ENABLE_SMP_ALLREDUCE : Enable SMP aware allreduce.
20211015 115039.976 INFO             PET1 index=  12                        MPIR_CVAR_MAX_SMP_ALLREDUCE_MSG_SIZE : Maximum message size for which SMP-aware allreduce is used.  A value of '0' uses SMP-aware allreduce for all message sizes.
20211015 115039.976 INFO             PET1 index=  13                         MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM : Variable to select allreduce algorithmauto                     - Internal algorithm selectionnb                       - Force nonblocking algorithmrecursive_doubling       - Force recursive doubling algorithmreduce_scatter_allgather - Force reduce scatter allgather algorithm
20211015 115039.976 INFO             PET1 index=  14                         MPIR_CVAR_ALLREDUCE_INTER_ALGORITHM : Variable to select allreduce algorithmauto                  - Internal algorithm selectionnb                    - Force nonblocking algorithmreduce_exchange_bcast - Force reduce-exchange-bcast algorithm
20211015 115039.976 INFO             PET1 index=  15                       MPIR_CVAR_ALLREDUCE_DEVICE_COLLECTIVE : If set to true, MPI_Allreduce will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level allreduce function will not be called.
20211015 115039.976 INFO             PET1 index=  16                           MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE : the short message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value (See also: MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE)
20211015 115039.976 INFO             PET1 index=  17                          MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE : the medium message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value and larger than MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE (See also: MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE)
20211015 115039.976 INFO             PET1 index=  18                                 MPIR_CVAR_ALLTOALL_THROTTLE : max no. of irecvs/isends posted at a time in some alltoall algorithms. Setting it to 0 causes all irecvs/isends to be posted at once
20211015 115039.976 INFO             PET1 index=  19                          MPIR_CVAR_ALLTOALL_INTRA_ALGORITHM : Variable to select alltoall algorithmauto                      - Internal algorithm selectionbrucks                    - Force brucks algorithmnb                        - Force nonblocking algorithmpairwise                  - Force pairwise algorithmpairwise_sendrecv_replace - Force pairwise sendrecv replace algorithmscattered                 - Force scattered algorithm
20211015 115039.976 INFO             PET1 index=  20                          MPIR_CVAR_ALLTOALL_INTER_ALGORITHM : Variable to select alltoall algorithmauto              - Internal algorithm selectionnb                - Force nonblocking algorithmpairwise_exchange - Force pairwise exchange algorithm
20211015 115039.976 INFO             PET1 index=  21                        MPIR_CVAR_ALLTOALL_DEVICE_COLLECTIVE : If set to true, MPI_Alltoall will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level alltoall function will not be called.
20211015 115039.976 INFO             PET1 index=  22                         MPIR_CVAR_ALLTOALLV_INTRA_ALGORITHM : Variable to select alltoallv algorithmauto                      - Internal algorithm selectionnb                        - Force nonblocking algorithmpairwise_sendrecv_replace - Force pairwise_sendrecv_replace algorithmscattered                 - Force scattered algorithm
20211015 115039.976 INFO             PET1 index=  23                         MPIR_CVAR_ALLTOALLV_INTER_ALGORITHM : Variable to select alltoallv algorithmauto              - Internal algorithm selectionpairwise_exchange - Force pairwise exchange algorithmnb                - Force nonblocking algorithm
20211015 115039.976 INFO             PET1 index=  24                       MPIR_CVAR_ALLTOALLV_DEVICE_COLLECTIVE : If set to true, MPI_Alltoallv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level alltoallv function will not be called.
20211015 115039.976 INFO             PET1 index=  25                         MPIR_CVAR_ALLTOALLW_INTRA_ALGORITHM : Variable to select alltoallw algorithmauto                      - Internal algorithm selectionnb                        - Force nonblocking algorithmpairwise_sendrecv_replace - Force pairwise sendrecv replace algorithmscattered                 - Force scattered algorithm
20211015 115039.976 INFO             PET1 index=  26                         MPIR_CVAR_ALLTOALLW_INTER_ALGORITHM : Variable to select alltoallw algorithmauto              - Internal algorithm selectionnb                - Force nonblocking algorithmpairwise_exchange - Force pairwise exchange algorithm
20211015 115039.976 INFO             PET1 index=  27                       MPIR_CVAR_ALLTOALLW_DEVICE_COLLECTIVE : If set to true, MPI_Alltoallw will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level alltoallw function will not be called.
20211015 115039.976 INFO             PET1 index=  28                                MPIR_CVAR_ENABLE_SMP_BARRIER : Enable SMP aware barrier.
20211015 115039.976 INFO             PET1 index=  29                           MPIR_CVAR_BARRIER_INTRA_ALGORITHM : Variable to select barrier algorithmauto               - Internal algorithm selectionnb                 - Force nonblocking algorithmrecursive_doubling - Force recursive doubling algorithm
20211015 115039.976 INFO             PET1 index=  30                           MPIR_CVAR_BARRIER_INTER_ALGORITHM : Variable to select barrier algorithmauto  - Internal algorithm selectionbcast - Force bcast algorithmnb    - Force nonblocking algorithm
20211015 115039.976 INFO             PET1 index=  31                         MPIR_CVAR_BARRIER_DEVICE_COLLECTIVE : If set to true, MPI_Barrier will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level barrier function will not be called.
20211015 115039.976 INFO             PET1 index=  32                                   MPIR_CVAR_BCAST_MIN_PROCS : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_SHORT_MSG_SIZE, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20211015 115039.976 INFO             PET1 index=  33                              MPIR_CVAR_BCAST_SHORT_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20211015 115039.976 INFO             PET1 index=  34                               MPIR_CVAR_BCAST_LONG_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_SHORT_MSG_SIZE)
20211015 115039.976 INFO             PET1 index=  35                                  MPIR_CVAR_ENABLE_SMP_BCAST : Enable SMP aware broadcast (See also: MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE)
20211015 115039.976 INFO             PET1 index=  36                            MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE : Maximum message size for which SMP-aware broadcast is used.  A value of '0' uses SMP-aware broadcast for all message sizes. (See also: MPIR_CVAR_ENABLE_SMP_BCAST)
20211015 115039.976 INFO             PET1 index=  37                             MPIR_CVAR_BCAST_INTRA_ALGORITHM : Variable to select bcast algorithmauto                                    - Internal algorithm selectionbinomial                                - Force Binomial Treenb                                      - Force nonblocking algorithmscatter_recursive_doubling_allgather    - Force Scatter Recursive-Doubling Allgatherscatter_ring_allgather                  - Force Scatter Ring
20211015 115039.976 INFO             PET1 index=  38                             MPIR_CVAR_BCAST_INTER_ALGORITHM : Variable to select bcast algorithmauto                    - Internal algorithm selectionnb                      - Force nonblocking algorithmremote_send_local_bcast - Force remote-send-local-bcast algorithm
20211015 115039.976 INFO             PET1 index=  39                           MPIR_CVAR_BCAST_DEVICE_COLLECTIVE : If set to true, MPI_Bcast will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually.  If set to false, the device-level bcast function will not be called.
20211015 115039.977 INFO             PET1 index=  40                            MPIR_CVAR_EXSCAN_INTRA_ALGORITHM : Variable to select allgather algorithmauto               - Internal algorithm selectionnb                 - Force nonblocking algorithmrecursive_doubling - Force recursive doubling algorithm
20211015 115039.977 INFO             PET1 index=  41                          MPIR_CVAR_EXSCAN_DEVICE_COLLECTIVE : If set to true, MPI_Exscan will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level exscan function will not be called.
20211015 115039.977 INFO             PET1 index=  42                       MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_VSMALL_MSG_SIZE)
20211015 115039.977 INFO             PET1 index=  43                            MPIR_CVAR_GATHER_INTRA_ALGORITHM : Variable to select gather algorithmauto     - Internal algorithm selectionbinomial - Force binomial algorithmnb       - Force nonblocking algorithm
20211015 115039.977 INFO             PET1 index=  44                            MPIR_CVAR_GATHER_INTER_ALGORITHM : Variable to select gather algorithmauto                     - Internal algorithm selectionlinear                   - Force linear algorithmlocal_gather_remote_send - Force local-gather-remote-send algorithmnb                       - Force nonblocking algorithm
20211015 115039.977 INFO             PET1 index=  45                          MPIR_CVAR_GATHER_DEVICE_COLLECTIVE : If set to true, MPI_Gather will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level gather function will not be called.
20211015 115039.977 INFO             PET1 index=  46                            MPIR_CVAR_GATHER_VSMALL_MSG_SIZE : use a temporary buffer for intracommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE)
20211015 115039.977 INFO             PET1 index=  47                           MPIR_CVAR_GATHERV_INTRA_ALGORITHM : Variable to select gatherv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithmnb     - Force nonblocking algorithm
20211015 115039.977 INFO             PET1 index=  48                           MPIR_CVAR_GATHERV_INTER_ALGORITHM : Variable to select gatherv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithmnb     - Force nonblocking algorithm
20211015 115039.977 INFO             PET1 index=  49                         MPIR_CVAR_GATHERV_DEVICE_COLLECTIVE : If set to true, MPI_Gatherv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level gatherv function will not be called.
20211015 115039.977 INFO             PET1 index=  50                     MPIR_CVAR_GATHERV_INTER_SSEND_MIN_PROCS : Use Ssend (synchronous send) for intercommunicator MPI_Gatherv if the "group B" size is >= this value.  Specifying "-1" always avoids using Ssend.  For backwards compatibility, specifying "0" uses the default value.
20211015 115039.977 INFO             PET1 index=  51                           MPIR_CVAR_IALLGATHER_RECEXCH_KVAL : k value for recursive exchange based iallgather
20211015 115039.977 INFO             PET1 index=  52                            MPIR_CVAR_IALLGATHER_BRUCKS_KVAL : k value for radix in brucks based iallgather
20211015 115039.977 INFO             PET1 index=  53                        MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM : Variable to select iallgather algorithmauto               - Internal algorithm selectionbrucks             - Force brucks algorithmrecursive_doubling - Force recursive doubling algorithmring               - Force ring algorithmrecexch_distance_doubling    - Force generic transport recursive exchange with neighbours doubling in distance in each phaserecexch_distance_halving  - Force generic transport recursive exchange with neighbours halving in distance in each phasegentran_brucks     - Force generic transport based brucks algorithm
20211015 115039.977 INFO             PET1 index=  54                        MPIR_CVAR_IALLGATHER_INTER_ALGORITHM : Variable to select iallgather algorithmauto                      - Internal algorithm selectionlocal_gather_remote_bcast - Force local-gather-remote-bcast algorithm
20211015 115039.977 INFO             PET1 index=  55                      MPIR_CVAR_IALLGATHER_DEVICE_COLLECTIVE : If set to true, MPI_Iallgather will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iallgather function will not be called.
20211015 115039.977 INFO             PET1 index=  56                          MPIR_CVAR_IALLGATHERV_RECEXCH_KVAL : k value for recursive exchange based iallgatherv
20211015 115039.977 INFO             PET1 index=  57                       MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM : Variable to select iallgatherv algorithmauto               - Internal algorithm selectionbrucks             - Force brucks algorithmrecursive_doubling - Force recursive doubling algorithmring               - Force ring algorithmrecexch_distance_doubling    - Force generic transport recursive exchange with neighbours doubling in distance in each phaserecexch_distance_halving     - Force generic transport recursive exchange with neighbours halving in distance in each phasegentran_ring              - Force generic transport ring algorithm
20211015 115039.977 INFO             PET1 index=  58                       MPIR_CVAR_IALLGATHERV_INTER_ALGORITHM : Variable to select iallgatherv algorithmauto                      - Internal algorithm selectionremote_gather_local_bcast - Force remote-gather-local-bcast algorithm
20211015 115039.977 INFO             PET1 index=  59                     MPIR_CVAR_IALLGATHERV_DEVICE_COLLECTIVE : If set to true, MPI_Iallgatherv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iallgatherv function will not be called.
20211015 115039.977 INFO             PET1 index=  60                              MPIR_CVAR_IALLREDUCE_TREE_KVAL : k value for tree based iallreduce (for tree_kary and tree_knomial)
20211015 115039.977 INFO             PET1 index=  61               MPIR_CVAR_IALLREDUCE_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based iallreduce (tree_kary and tree_knomial). Default value is 0, that is, no pipelining by default
20211015 115039.977 INFO             PET1 index=  62                  MPIR_CVAR_IALLREDUCE_TREE_BUFFER_PER_CHILD : If set to true, a rank in tree_kary and tree_knomial algorithms will allocate a dedicated buffer for every child it receives data from. This would mean more memory consumption but it would allow preposting of the receives and hence reduce the number of unexpected messages. If set to false, there is only one buffer that is used to receive the data from all the children. The receives are therefore serialized, that is, only one receive can be posted at a time.
20211015 115039.977 INFO             PET1 index=  63                           MPIR_CVAR_IALLREDUCE_RECEXCH_KVAL : k value for recursive exchange based iallreduce
20211015 115039.977 INFO             PET1 index=  64                        MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM : Variable to select iallreduce algorithmauto                     - Internal algorithm selectionnaive                    - Force naive algorithmrecursive_doubling       - Force recursive doubling algorithmreduce_scatter_allgather - Force reduce scatter allgather algorithmrecexch_single_buffer    - Force generic transport recursive exchange with single buffer for receivesrecexch_multiple_buffer  - Force generic transport recursive exchange with multiple buffers for receives
20211015 115039.977 INFO             PET1 index=  65                        MPIR_CVAR_IALLREDUCE_INTER_ALGORITHM : Variable to select iallreduce algorithmauto                      - Internal algorithm selectionremote_reduce_local_bcast - Force remote-reduce-local-bcast algorithm
20211015 115039.977 INFO             PET1 index=  66                      MPIR_CVAR_IALLREDUCE_DEVICE_COLLECTIVE : If set to true, MPI_Iallreduce will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iallreduce function will not be called.
20211015 115039.977 INFO             PET1 index=  67                         MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM : Variable to select ialltoall algorithmauto              - Internal algorithm selectionbrucks            - Force brucks algorithminplace           - Force inplace algorithmpairwise          - Force pairwise algorithmpermuted_sendrecv - Force permuted sendrecv algorithm
20211015 115039.977 INFO             PET1 index=  68                         MPIR_CVAR_IALLTOALL_INTER_ALGORITHM : Variable to select ialltoall algorithmauto              - Internal algorithm selectionpairwise_exchange - Force pairwise exchange algorithm
20211015 115039.977 INFO             PET1 index=  69                       MPIR_CVAR_IALLTOALL_DEVICE_COLLECTIVE : If set to true, MPI_Ialltoall will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ialltoall function will not be called.
20211015 115039.977 INFO             PET1 index=  70                        MPIR_CVAR_IALLTOALLV_INTRA_ALGORITHM : Variable to select ialltoallv algorithmauto              - Internal algorithm selectionblocked           - Force blocked algorithminplace           - Force inplace algorithmpairwise_exchange - Force pairwise exchange algorithm
20211015 115039.977 INFO             PET1 index=  71                        MPIR_CVAR_IALLTOALLV_INTER_ALGORITHM : Variable to select ialltoallv algorithmauto - Internal algorithm selection
20211015 115039.977 INFO             PET1 index=  72                      MPIR_CVAR_IALLTOALLV_DEVICE_COLLECTIVE : If set to true, MPI_Ialltoallv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ialltoallv function will not be called.
20211015 115039.977 INFO             PET1 index=  73                        MPIR_CVAR_IALLTOALLW_INTRA_ALGORITHM : Variable to select ialltoallw algorithmauto              - Internal algorithm selectionblocked           - Force blocked algorithminplace           - Force inplace algorithmpairwise_exchange - Force pairwise exchange algorithm
20211015 115039.977 INFO             PET1 index=  74                        MPIR_CVAR_IALLTOALLW_INTER_ALGORITHM : Variable to select ialltoallw algorithmauto - Internal algorithm selection
20211015 115039.977 INFO             PET1 index=  75                      MPIR_CVAR_IALLTOALLW_DEVICE_COLLECTIVE : If set to true, MPI_Ialltoallw will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ialltoallw function will not be called.
20211015 115039.977 INFO             PET1 index=  76                             MPIR_CVAR_IBARRIER_RECEXCH_KVAL : k value for recursive exchange based ibarrier
20211015 115039.977 INFO             PET1 index=  77                          MPIR_CVAR_IBARRIER_INTRA_ALGORITHM : Variable to select ibarrier algorithmauto               - Internal algorithm selectionrecursive_doubling - Force recursive doubling algorithmrecexch            - Force generic transport based recursive exchange algorithm
20211015 115039.977 INFO             PET1 index=  78                          MPIR_CVAR_IBARRIER_INTER_ALGORITHM : Variable to select ibarrier algorithmauto  - Internal algorithm selectionbcast - Force bcast algorithm
20211015 115039.977 INFO             PET1 index=  79                        MPIR_CVAR_IBARRIER_DEVICE_COLLECTIVE : If set to true, MPI_Ibarrier will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ibarrier function will not be called.
20211015 115039.977 INFO             PET1 index=  80                                  MPIR_CVAR_IBCAST_TREE_KVAL : k value for tree (kary, knomial, etc.) based ibcast
20211015 115039.977 INFO             PET1 index=  81                                  MPIR_CVAR_IBCAST_TREE_TYPE : Tree type for tree based ibcast kary      - kary tree type knomial_1 - knomial_1 tree type knomial_2 - knomial_2 tree type
20211015 115039.977 INFO             PET1 index=  82                   MPIR_CVAR_IBCAST_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based ibcast. Default value is 0, that is, no pipelining by default
20211015 115039.977 INFO             PET1 index=  83                            MPIR_CVAR_IBCAST_RING_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in ibcast ring algorithm. Default value is 0, that is, no pipelining by default
20211015 115039.977 INFO             PET1 index=  84                            MPIR_CVAR_IBCAST_INTRA_ALGORITHM : Variable to select ibcast algorithmauto                                 - Internal algorithm selectionbinomial                             - Force Binomial algorithmscatter_recursive_doubling_allgather - Force Scatter Recursive Doubling Allgather algorithmscatter_ring_allgather               - Force Scatter Ring Allgather algorithmtree                                 - Force Generic Transport Tree algorithmscatter_recexch_allgather            - Force Generic Transport Scatter followed by Recursive Exchange Allgather algorithmring                                 - Force Generic Transport Ring algorithm
20211015 115039.977 INFO             PET1 index=  85                               MPIR_CVAR_IBCAST_SCATTER_KVAL : k value for tree based scatter in scatter_recexch_allgather algorithm
20211015 115039.977 INFO             PET1 index=  86                     MPIR_CVAR_IBCAST_ALLGATHER_RECEXCH_KVAL : k value for recursive exchange based allgather in scatter_recexch_allgather algorithm
20211015 115039.977 INFO             PET1 index=  87                            MPIR_CVAR_IBCAST_INTER_ALGORITHM : Variable to select ibcast algorithmauto - Internal algorithm selectionflat - Force flat algorithm
20211015 115039.977 INFO             PET1 index=  88                          MPIR_CVAR_IBCAST_DEVICE_COLLECTIVE : If set to true, MPI_Ibcast will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually.  If set to false, the device-level ibcast function will not be called.
20211015 115039.977 INFO             PET1 index=  89                           MPIR_CVAR_IEXSCAN_INTRA_ALGORITHM : Variable to select iexscan algorithmauto               - Internal algorithm selectionrecursive_doubling - Force recursive doubling algorithm
20211015 115039.977 INFO             PET1 index=  90                         MPIR_CVAR_IEXSCAN_DEVICE_COLLECTIVE : If set to true, MPI_Iexscan will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iexscan function will not be called.
20211015 115039.977 INFO             PET1 index=  91                           MPIR_CVAR_IGATHER_INTRA_ALGORITHM : Variable to select igather algorithmauto     - Internal algorithm selectionbinomial - Force binomial algorithmtree     - Force genetric transport based tree algorithm
20211015 115039.977 INFO             PET1 index=  92                                 MPIR_CVAR_IGATHER_TREE_KVAL : k value for tree based igather
20211015 115039.977 INFO             PET1 index=  93                           MPIR_CVAR_IGATHER_INTER_ALGORITHM : Variable to select igather algorithmauto        - Internal algorithm selectionlong_inter  - Force long inter algorithmshort_inter - Force short inter algorithm
20211015 115039.977 INFO             PET1 index=  94                         MPIR_CVAR_IGATHER_DEVICE_COLLECTIVE : If set to true, MPI_Igather will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level igather function will not be called.
20211015 115039.977 INFO             PET1 index=  95                          MPIR_CVAR_IGATHERV_INTRA_ALGORITHM : Variable to select igatherv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET1 index=  96                          MPIR_CVAR_IGATHERV_INTER_ALGORITHM : Variable to select igatherv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET1 index=  97                        MPIR_CVAR_IGATHERV_DEVICE_COLLECTIVE : If set to true, MPI_Igatherv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level igatherv function will not be called.
20211015 115039.977 INFO             PET1 index=  98               MPIR_CVAR_INEIGHBOR_ALLGATHER_INTRA_ALGORITHM : Variable to select ineighbor_allgather algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET1 index=  99               MPIR_CVAR_INEIGHBOR_ALLGATHER_INTER_ALGORITHM : Variable to select ineighbor_allgather algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET1 index= 100             MPIR_CVAR_INEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE : If set to true, MPI_ineighbor_allgather will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ineighbor_allgather function will not be called.
20211015 115039.977 INFO             PET1 index= 101              MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTRA_ALGORITHM : Variable to select ineighbor_allgatherv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET1 index= 102              MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTER_ALGORITHM : Variable to select ineighbor_allgatherv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET1 index= 103            MPIR_CVAR_INEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE : If set to true, MPI_ineighbor_allgatherv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ineighbor_allgatherv function will not be called.
20211015 115039.977 INFO             PET1 index= 104                MPIR_CVAR_INEIGHBOR_ALLTOALL_INTRA_ALGORITHM : Variable to select ineighbor_alltoall algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET1 index= 105                MPIR_CVAR_INEIGHBOR_ALLTOALL_INTER_ALGORITHM : Variable to select ineighbor_alltoall algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET1 index= 106              MPIR_CVAR_INEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE : If set to true, MPI_ineighbor_alltoall will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ineighbor_alltoall function will not be called.
20211015 115039.977 INFO             PET1 index= 107               MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTRA_ALGORITHM : Variable to select ineighbor_alltoallv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET1 index= 108               MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTER_ALGORITHM : Variable to select ineighbor_alltoallv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET1 index= 109             MPIR_CVAR_INEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE : If set to true, MPI_ineighbor_alltoallv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ineighbor_alltoallv function will not be called.
20211015 115039.978 INFO             PET1 index= 110               MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTRA_ALGORITHM : Variable to select ineighbor_alltoallw algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET1 index= 111               MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTER_ALGORITHM : Variable to select ineighbor_alltoallw algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET1 index= 112             MPIR_CVAR_INEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE : If set to true, MPI_ineighbor_alltoallw will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ineighbor_alltoallw function will not be called.
20211015 115039.978 INFO             PET1 index= 113                                 MPIR_CVAR_IREDUCE_TREE_KVAL : k value for tree (kary, knomial, etc.) based ireduce
20211015 115039.978 INFO             PET1 index= 114                                 MPIR_CVAR_IREDUCE_TREE_TYPE : Tree type for tree based ireduce kary      - kary tree knomial_1 - knomial_1 tree knomial_2 - knomial_2 tree
20211015 115039.978 INFO             PET1 index= 115                  MPIR_CVAR_IREDUCE_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based ireduce. Default value is 0, that is, no pipelining by default
20211015 115039.978 INFO             PET1 index= 116                           MPIR_CVAR_IREDUCE_RING_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in ireduce ring algorithm. Default value is 0, that is, no pipelining by default
20211015 115039.978 INFO             PET1 index= 117                     MPIR_CVAR_IREDUCE_TREE_BUFFER_PER_CHILD : If set to true, a rank in tree algorithms will allocate a dedicated buffer for every child it receives data from. This would mean more memory consumption but it would allow preposting of the receives and hence reduce the number of unexpected messages. If set to false, there is only one buffer that is used to receive the data from all the children. The receives are therefore serialized, that is, only one receive can be posted at a time.
20211015 115039.978 INFO             PET1 index= 118                           MPIR_CVAR_IREDUCE_INTRA_ALGORITHM : Variable to select ireduce algorithmauto                  - Internal algorithm selectionbinomial              - Force binomial algorithmreduce_scatter_gather - Force reduce scatter gather algorithmtree                  - Force Generic Transport Treering                  - Force Generic Transport Ring
20211015 115039.978 INFO             PET1 index= 119                           MPIR_CVAR_IREDUCE_INTER_ALGORITHM : Variable to select ireduce algorithmauto                     - Internal algorithm selectionlocal_reduce_remote_send - Force local-reduce-remote-send algorithm
20211015 115039.978 INFO             PET1 index= 120                         MPIR_CVAR_IREDUCE_DEVICE_COLLECTIVE : If set to true, MPI_Ireduce will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ireduce function will not be called.
20211015 115039.978 INFO             PET1 index= 121                      MPIR_CVAR_IREDUCE_SCATTER_RECEXCH_KVAL : k value for recursive exchange based ireduce_scatter
20211015 115039.978 INFO             PET1 index= 122                   MPIR_CVAR_IREDUCE_SCATTER_INTRA_ALGORITHM : Variable to select ireduce_scatter algorithmauto               - Internal algorithm selectionnoncommutative     - Force noncommutative algorithmrecursive_doubling - Force recursive doubling algorithmpairwise           - Force pairwise algorithmrecursive_halving  - Force recursive halving algorithmrecexch  - Force generic transport recursive exchange algorithm
20211015 115039.978 INFO             PET1 index= 123                   MPIR_CVAR_IREDUCE_SCATTER_INTER_ALGORITHM : Variable to select ireduce_scatter algorithmauto                         - Internal algorithm selectionremote_reduce_local_scatterv - Force remote-reduce-local-scatterv algorithm
20211015 115039.978 INFO             PET1 index= 124                 MPIR_CVAR_IREDUCE_SCATTER_DEVICE_COLLECTIVE : If set to true, MPI_Ireduce_scatter will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ireduce_scatter function will not be called.
20211015 115039.978 INFO             PET1 index= 125                MPIR_CVAR_IREDUCE_SCATTER_BLOCK_RECEXCH_KVAL : k value for recursive exchange based ireduce_scatter_block
20211015 115039.978 INFO             PET1 index= 126             MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM : Variable to select ireduce_scatter_block algorithmauto               - Internal algorithm selectionnoncommutative     - Force noncommutative algorithmrecursive_doubling - Force recursive doubling algorithmpairwise           - Force pairwise algorithmrecursive_halving  - Force recursive halving algorithmrecexch  - Force generic transport recursive exchange algorithm
20211015 115039.978 INFO             PET1 index= 127             MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTER_ALGORITHM : Variable to select ireduce_scatter_block algorithmauto                         - Internal algorithm selectionremote_reduce_local_scatterv - Force remote-reduce-local-scatterv algorithm
20211015 115039.978 INFO             PET1 index= 128           MPIR_CVAR_IREDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE : If set to true, MPI_Ireduce_scatter_block will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ireduce_scatter_block function will not be called.
20211015 115039.978 INFO             PET1 index= 129                             MPIR_CVAR_ISCAN_INTRA_ALGORITHM : Variable to select allgather algorithmauto               - Internal algorithm selectionrecursive_doubling - Force recursive doubling algorithm
20211015 115039.978 INFO             PET1 index= 130                           MPIR_CVAR_ISCAN_DEVICE_COLLECTIVE : If set to true, MPI_Iscan will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iscan function will not be called.
20211015 115039.978 INFO             PET1 index= 131                          MPIR_CVAR_ISCATTER_INTRA_ALGORITHM : Variable to select iscatter algorithmauto     - Internal algorithm selectionbinomial - Force binomial algorithmtree     - Force genetric transport based tree algorithm
20211015 115039.978 INFO             PET1 index= 132                                MPIR_CVAR_ISCATTER_TREE_KVAL : k value for tree based iscatter
20211015 115039.978 INFO             PET1 index= 133                          MPIR_CVAR_ISCATTER_INTER_ALGORITHM : Variable to select iscatter algorithmauto                      - Internal algorithm selectionlinear                    - Force linear algorithmremote_send_local_scatter - Force remote-send-local-scatter algorithm
20211015 115039.978 INFO             PET1 index= 134                        MPIR_CVAR_ISCATTER_DEVICE_COLLECTIVE : If set to true, MPI_Iscatter will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iscatter function will not be called.
20211015 115039.978 INFO             PET1 index= 135                         MPIR_CVAR_ISCATTERV_INTRA_ALGORITHM : Variable to select iscatterv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET1 index= 136                         MPIR_CVAR_ISCATTERV_INTER_ALGORITHM : Variable to select iscatterv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET1 index= 137                       MPIR_CVAR_ISCATTERV_DEVICE_COLLECTIVE : If set to true, MPI_Iscatterv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iscatterv function will not be called.
20211015 115039.978 INFO             PET1 index= 138                MPIR_CVAR_NEIGHBOR_ALLGATHER_INTRA_ALGORITHM : Variable to select ineighbor_allgather algorithmauto - Internal algorithm selectionnb   - Force nonblocking algorithm
20211015 115039.978 INFO             PET1 index= 139                MPIR_CVAR_NEIGHBOR_ALLGATHER_INTER_ALGORITHM : Variable to select ineighbor_allgather algorithmauto - Internal algorithm selectionnb   - Force nonblocking algorithm
20211015 115039.978 INFO             PET1 index= 140              MPIR_CVAR_NEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE : If set to true, MPI_neighbor_allgather will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level neighbor_allgather function will not be called.
20211015 115039.978 INFO             PET1 index= 141               MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTRA_ALGORITHM : Variable to select neighbor_allgatherv algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET1 index= 142               MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTER_ALGORITHM : Variable to select neighbor_allgatherv algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET1 index= 143             MPIR_CVAR_NEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE : If set to true, MPI_neighbor_allgatherv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level neighbor_allgatherv function will not be called.
20211015 115039.978 INFO             PET1 index= 144                 MPIR_CVAR_NEIGHBOR_ALLTOALL_INTRA_ALGORITHM : Variable to select neighbor_alltoall algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET1 index= 145                 MPIR_CVAR_NEIGHBOR_ALLTOALL_INTER_ALGORITHM : Variable to select neighbor_alltoall algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET1 index= 146               MPIR_CVAR_NEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE : If set to true, MPI_neighbor_alltoall will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level neighbor_alltoall function will not be called.
20211015 115039.978 INFO             PET1 index= 147                MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTRA_ALGORITHM : Variable to select neighbor_alltoallv algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET1 index= 148                MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTER_ALGORITHM : Variable to select neighbor_alltoallv algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET1 index= 149              MPIR_CVAR_NEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE : If set to true, MPI_neighbor_alltoallv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level neighbor_alltoallv function will not be called.
20211015 115039.978 INFO             PET1 index= 150                MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTRA_ALGORITHM : Variable to select neighbor_alltoallw algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET1 index= 151                MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTER_ALGORITHM : Variable to select neighbor_alltoallw algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET1 index= 152              MPIR_CVAR_NEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE : If set to true, MPI_neighbor_alltoallw will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level neighbor_alltoallw function will not be called.
20211015 115039.978 INFO             PET1 index= 153                             MPIR_CVAR_REDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20211015 115039.978 INFO             PET1 index= 154                                 MPIR_CVAR_ENABLE_SMP_REDUCE : Enable SMP aware reduce.
20211015 115039.978 INFO             PET1 index= 155                           MPIR_CVAR_MAX_SMP_REDUCE_MSG_SIZE : Maximum message size for which SMP-aware reduce is used.  A value of '0' uses SMP-aware reduce for all message sizes.
20211015 115039.978 INFO             PET1 index= 156                            MPIR_CVAR_REDUCE_INTRA_ALGORITHM : Variable to select reduce algorithmauto                  - Internal algorithm selectionbinomial              - Force binomial algorithmnb                    - Force nonblocking algorithmreduce_scatter_gather - Force reduce scatter gather algorithm
20211015 115039.978 INFO             PET1 index= 157                            MPIR_CVAR_REDUCE_INTER_ALGORITHM : Variable to select reduce algorithmauto                     - Internal algorithm selectionlocal_reduce_remote_send - Force local-reduce-remote-send algorithmnb                       - Force nonblocking algorithm
20211015 115039.978 INFO             PET1 index= 158                          MPIR_CVAR_REDUCE_DEVICE_COLLECTIVE : If set to true, MPI_Reduce will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level reduce function will not be called.
20211015 115039.978 INFO             PET1 index= 159          MPIR_CVAR_REDUCE_SCATTER_COMMUTATIVE_LONG_MSG_SIZE : the long message algorithm will be used if the operation is commutative and the send buffer size is >= this value (in bytes)
20211015 115039.978 INFO             PET1 index= 160                    MPIR_CVAR_REDUCE_SCATTER_INTRA_ALGORITHM : Variable to select reduce_scatter algorithmauto               - Internal algorithm selectionnb                 - Force nonblocking algorithmnoncommutative     - Force noncommutative algorithmpairwise           - Force pairwise algorithmrecursive_doubling - Force recursive doubling algorithmrecursive_halving  - Force recursive halving algorithm
20211015 115039.978 INFO             PET1 index= 161                    MPIR_CVAR_REDUCE_SCATTER_INTER_ALGORITHM : Variable to select reduce_scatter algorithmauto                        - Internal algorithm selectionnb                          - Force nonblocking algorithmremote_reduce_local_scatter - Force remote-reduce-local-scatter algorithm
20211015 115039.978 INFO             PET1 index= 162                  MPIR_CVAR_REDUCE_SCATTER_DEVICE_COLLECTIVE : If set to true, MPI_Redscat will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level reduce_scatter function will not be called.
20211015 115039.978 INFO             PET1 index= 163              MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM : Variable to select reduce_scatter_block algorithmauto               - Internal algorithm selectionnoncommutative     - Force noncommutative algorithmrecursive_doubling - Force recursive doubling algorithmpairwise           - Force pairwise algorithmrecursive_halving  - Force recursive halving algorithmnb                 - Force nonblocking algorithm
20211015 115039.978 INFO             PET1 index= 164              MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTER_ALGORITHM : Variable to select reduce_scatter_block algorithmauto                        - Internal algorithm selectionnb                          - Force nonblocking algorithmremote_reduce_local_scatter - Force remote-reduce-local-scatter algorithm
20211015 115039.978 INFO             PET1 index= 165            MPIR_CVAR_REDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE : If set to true, MPI_Reduce_scatter_block will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level reduce_scatter_block function will not be called.
20211015 115039.978 INFO             PET1 index= 166                              MPIR_CVAR_SCAN_INTRA_ALGORITHM : Variable to select allgather algorithmauto               - Internal algorithm selectionnb                 - Force nonblocking algorithmrecursive_doubling - Force recursive doubling algorithm
20211015 115039.978 INFO             PET1 index= 167                            MPIR_CVAR_SCAN_DEVICE_COLLECTIVE : If set to true, MPI_Scan will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level scan function will not be called.
20211015 115039.978 INFO             PET1 index= 168                      MPIR_CVAR_SCATTER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Scatter if the send buffer size is < this value (in bytes)
20211015 115039.978 INFO             PET1 index= 169                           MPIR_CVAR_SCATTER_INTRA_ALGORITHM : Variable to select scatter algorithmauto     - Internal algorithm selectionbinomial - Force binomial algorithmnb       - Force nonblocking algorithm
20211015 115039.978 INFO             PET1 index= 170                           MPIR_CVAR_SCATTER_INTER_ALGORITHM : Variable to select scatter algorithmauto                      - Internal algorithm selectionlinear                    - Force linear algorithmnb                        - Force nonblocking algorithmremote_send_local_scatter - Force remote-send-local-scatter algorithm
20211015 115039.978 INFO             PET1 index= 171                         MPIR_CVAR_SCATTER_DEVICE_COLLECTIVE : If set to true, MPI_Scatter will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level scatter function will not be called.
20211015 115039.978 INFO             PET1 index= 172                          MPIR_CVAR_SCATTERV_INTRA_ALGORITHM : Variable to select scatterv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithmnb     - Force nonblocking algorithm
20211015 115039.978 INFO             PET1 index= 173                          MPIR_CVAR_SCATTERV_INTER_ALGORITHM : Variable to select scatterv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithmnb     - Force nonblocking algorithm
20211015 115039.978 INFO             PET1 index= 174                        MPIR_CVAR_SCATTERV_DEVICE_COLLECTIVE : If set to true, MPI_scatterv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level scatterv function will not be called.
20211015 115039.978 INFO             PET1 index= 175                                MPIR_CVAR_DEVICE_COLLECTIVES : If set to true, MPI collectives will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually.  If set to false, the device-level collective function will not be called.
20211015 115039.978 INFO             PET1 index= 176                                MPIR_CVAR_PROGRESS_MAX_COLLS : Maximum number of collective operations at a time that the progress engine should make progress on
20211015 115039.978 INFO             PET1 index= 177                              MPIR_CVAR_COMM_SPLIT_USE_QSORT : Use qsort(3) in the implementation of MPI_Comm_split instead of bubble sort.
20211015 115039.978 INFO             PET1 index= 178                                  MPIR_CVAR_CTXID_EAGER_SIZE : The MPIR_CVAR_CTXID_EAGER_SIZE environment variable allows you to specify how many words in the context ID mask will be set aside for the eager allocation protocol.  If the application is running out of context IDs, reducing this value may help.
20211015 115039.978 INFO             PET1 index= 179                                    MPIR_CVAR_PROCTABLE_SIZE : Size of the "MPIR" debugger interface proctable (process table).
20211015 115039.978 INFO             PET1 index= 180                                   MPIR_CVAR_PROCTABLE_PRINT : If true, dump the proctable entries at MPII_Wait_for_debugger-time.
20211015 115039.978 INFO             PET1 index= 181                                 MPIR_CVAR_PRINT_ERROR_STACK : If true, print an error stack trace at error handling time.
20211015 115039.978 INFO             PET1 index= 182                                  MPIR_CVAR_CHOP_ERROR_STACK : If >0, truncate error stack output lines this many characters wide.  If 0, do not truncate, and if <0 use a sensible default.
20211015 115039.978 INFO             PET1 index= 183                            MPIR_CVAR_SUPPRESS_ABORT_MESSAGE : Disable printing of abort error message.
20211015 115039.978 INFO             PET1 index= 184                                           MPIR_CVAR_MEMDUMP : If true, list any memory that was allocated by MPICH and that remains allocated when MPI_Finalize completes.
20211015 115039.978 INFO             PET1 index= 185                          MPIR_CVAR_MEM_CATEGORY_INFORMATION : If true, print a summary of memory allocation by category. The category definitions are found in mpl_trmem.h.
20211015 115039.978 INFO             PET1 index= 186                                    MPIR_CVAR_ASYNC_PROGRESS : If set to true, MPICH will initiate an additional thread to make asynchronous progress on all communication operations including point-to-point, collective, one-sided operations and I/O.  Setting this variable will automatically increase the thread-safety level to MPI_THREAD_MULTIPLE.  While this improves the progress semantics, it might cause a small amount of performance overhead for regular MPI operations.  The user is encouraged to leave one or more hardware threads vacant in order to prevent contention between the application threads and the progress thread(s).  The impact of oversubscription is highly system dependent but may be substantial in some cases, hence this recommendation.
20211015 115039.978 INFO             PET1 index= 187                              MPIR_CVAR_DEFAULT_THREAD_LEVEL : Sets the default thread level to use when using MPI_INIT. This variable is case-insensitive.
20211015 115039.978 INFO             PET1 index= 188                                        MPIR_CVAR_DEBUG_HOLD : If true, causes processes to wait in MPI_Init and MPI_Initthread for a debugger to be attached.  Once the debugger has attached, the variable 'hold' should be set to 0 in order to allow the process to continue (e.g., in gdb, "set hold=0").
20211015 115039.978 INFO             PET1 index= 189                                    MPIR_CVAR_ERROR_CHECKING : If true, perform checks for errors, typically to verify valid inputs to MPI routines.  Only effective when MPICH is configured with --enable-error-checking=runtime .
20211015 115039.978 INFO             PET1 index= 190                                  MPIR_CVAR_NETLOC_NODE_FILE : Subnet json file
20211015 115039.978 INFO             PET1 index= 191                                      MPIR_CVAR_DIMS_VERBOSE : If true, enable verbose output about the actions of the implementation of MPI_Dims_create.
20211015 115039.978 INFO             PET1 index= 192                              MPIR_CVAR_NAMESERV_FILE_PUBDIR : Sets the directory to use for MPI service publishing in the file nameserv implementation.  Allows the user to override where the publish and lookup information is placed for connect/accept based applications.
20211015 115039.979 INFO             PET1 index= 193                           MPIR_CVAR_ABORT_ON_LEAKED_HANDLES : If true, MPI will call MPI_Abort at MPI_Finalize if any MPI object handles have been leaked.  For example, if MPI_Comm_dup is called without calling a corresponding MPI_Comm_free.  For uninteresting reasons, enabling this option may prevent all known object leaks from being reported.  MPICH must have been configure with "--enable-g=handlealloc" or better in order for this functionality to work.
20211015 115039.979 INFO             PET1 index= 194                                           MPIR_CVAR_NOLOCAL : If true, force all processes to operate as though all processes are located on another node.  For example, this disables shared memory communication hierarchical collectives.
20211015 115039.979 INFO             PET1 index= 195                                  MPIR_CVAR_ODD_EVEN_CLIQUES : If true, odd procs on a node are seen as local to each other, and even procs on a node are seen as local to each other.  Used for debugging on a single machine. Deprecated in favor of MPIR_CVAR_NUM_CLIQUES.
20211015 115039.979 INFO             PET1 index= 196                                       MPIR_CVAR_NUM_CLIQUES : Specify the number of cliques that should be used to partition procs on a local node. Procs with the same clique number are seen as local to each other. Used for debugging on a single machine.
20211015 115039.979 INFO             PET1 index= 197                                  MPIR_CVAR_COLL_ALIAS_CHECK : Enable checking of aliasing in collective operations
20211015 115039.979 INFO             PET1 index= 198                                 MPIR_CVAR_REQUEST_POLL_FREQ : How frequent to poll during completion calls (wait/test) in terms of number of processed requests before polling.
20211015 115039.979 INFO             PET1 index= 199                                MPIR_CVAR_REQUEST_BATCH_SIZE : The number of requests to make completion as a batch in MPI_Waitall and MPI_Testall implementation. A large number is likely to cause more cache misses.
20211015 115039.979 INFO             PET1 index= 200                                MPIR_CVAR_POLLS_BEFORE_YIELD : When MPICH is in a busy waiting loop, it will periodically call a function to yield the processor.  This cvar sets the number of loops before the yield function is called.  A value of 0 disables yielding.
20211015 115039.979 INFO             PET1 index= 201                          MPIR_CVAR_NEMESIS_MXM_BULK_CONNECT : If true, force mxm to connect all processes at initialization time.
20211015 115039.979 INFO             PET1 index= 202                       MPIR_CVAR_NEMESIS_MXM_BULK_DISCONNECT : If true, force mxm to disconnect all processes at finalization time.
20211015 115039.979 INFO             PET1 index= 203                              MPIR_CVAR_NEMESIS_MXM_HUGEPAGE : If true, mxm tries detecting hugepage support.  On HPC-X 2.3 and earlier, this might cause problems on Ubuntu and other platforms even if the system provides hugepage support.
20211015 115039.979 INFO             PET1 index= 204                                  MPIR_CVAR_OFI_USE_PROVIDER : If non-null, choose an OFI provider by name. If using with the CH4 device and using a provider that supports an older version of the libfabric API then the default version of the installed library, specifying the OFI version via the appropriate CVARs is also recommended.
20211015 115039.979 INFO             PET1 index= 205                                MPIR_CVAR_OFI_DUMP_PROVIDERS : If true, dump provider information at init
20211015 115039.979 INFO             PET1 index= 206                            MPIR_CVAR_CH3_INTERFACE_HOSTNAME : If non-NULL, this cvar specifies the IP address that other processes should use when connecting to this process. This cvar is mutually exclusive with the MPIR_CVAR_CH3_NETWORK_IFACE cvar and it is an error to set them both.
20211015 115039.979 INFO             PET1 index= 207                                    MPIR_CVAR_CH3_PORT_RANGE : The MPIR_CVAR_CH3_PORT_RANGE environment variable allows you to specify the range of TCP ports to be used by the process manager and the MPICH library. The format of this variable is <low>:<high>.  To specify any available port, use 0:0.
20211015 115039.979 INFO             PET1 index= 208                         MPIR_CVAR_NEMESIS_TCP_NETWORK_IFACE : If non-NULL, this cvar specifies which pseudo-ethernet interface the tcp netmod should use (e.g., "eth1", "ib0"). Note, this is a Linux-specific cvar. This cvar is mutually exclusive with the MPIR_CVAR_CH3_INTERFACE_HOSTNAME cvar and it is an error to set them both.
20211015 115039.979 INFO             PET1 index= 209                   MPIR_CVAR_NEMESIS_TCP_HOST_LOOKUP_RETRIES : This cvar controls the number of times to retry the gethostbyname() function before giving up.
20211015 115039.979 INFO             PET1 index= 210                            MPIR_CVAR_NEMESIS_ENABLE_CKPOINT : If true, enables checkpointing support and returns an error if checkpointing library cannot be initialized.
20211015 115039.979 INFO             PET1 index= 211                          MPIR_CVAR_NEMESIS_SHM_EAGER_MAX_SZ : This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for shared memory. If this cvar is set to -1, then Nemesis will choose an appropriate value.
20211015 115039.979 INFO             PET1 index= 212                    MPIR_CVAR_NEMESIS_SHM_READY_EAGER_MAX_SZ : This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for ready-send messages.  If this cvar is set to -1, then ready messages will always be sent eagerly.  If this cvar is set to -2, then Nemesis will choose an appropriate value.
20211015 115039.979 INFO             PET1 index= 213                                         MPIR_CVAR_ENABLE_FT : Enable fault tolerance functions
20211015 115039.979 INFO             PET1 index= 214                         MPIR_CVAR_NEMESIS_LMT_DMA_THRESHOLD : Messages larger than this size will use the "dma" (knem) intranode LMT implementation, if it is enabled and available.
20211015 115039.979 INFO             PET1 index= 215                                    MPIR_CVAR_NEMESIS_NETMOD : If non-empty, this cvar specifies which network module should be used for communication. This variable is case-insensitive.
20211015 115039.979 INFO             PET1 index= 216                                  MPIR_CVAR_CH3_ENABLE_HCOLL : If true, enable HCOLL collectives.
20211015 115039.979 INFO             PET1 index= 217                          MPIR_CVAR_CH3_COMM_CONNECT_TIMEOUT : The default time out period in seconds for a connection attempt to the server communicator where the named port exists but no pending accept. User can change the value for a specified connection through its info argument.
20211015 115039.979 INFO             PET1 index= 218               MPIR_CVAR_CH3_RMA_OP_PIGGYBACK_LOCK_DATA_SIZE : Specify the threshold of data size of a RMA operation which can be piggybacked with a LOCK message. It is always a positive value and should not be smaller than MPIDI_RMA_IMMED_BYTES. If user sets it as a small value, for middle and large data size, we will lose performance because of always waiting for round-trip of LOCK synchronization; if user sets it as a large value, we need to consume more memory on target side to buffer this lock request when lock is not satisfied.
20211015 115039.979 INFO             PET1 index= 219                      MPIR_CVAR_CH3_RMA_ACTIVE_REQ_THRESHOLD : Threshold of number of active requests to trigger blocking waiting in operation routines. When the value is negative, we never blockingly wait in operation routines. When the value is zero, we always trigger blocking waiting in operation routines to wait until no. of active requests becomes zero. When the value is positive, we do blocking waiting in operation routines to wait until no. of active requests being reduced to this value.
20211015 115039.979 INFO             PET1 index= 220               MPIR_CVAR_CH3_RMA_POKE_PROGRESS_REQ_THRESHOLD : Threshold at which the RMA implementation attempts to complete requests while completing RMA operations and while using the lazy synchonization approach.  Change this value if programs fail because they run out of requests or other internal resources
20211015 115039.979 INFO             PET1 index= 221                MPIR_CVAR_CH3_RMA_SCALABLE_FENCE_PROCESS_NUM : Specify the threshold of switching the algorithm used in FENCE from the basic algorithm to the scalable algorithm. The value can be nagative, zero or positive. When the number of processes is larger than or equal to this value, FENCE will use a scalable algorithm which do not use O(P) data structure; when the number of processes is smaller than the value, FENCE will use a basic but fast algorithm which requires an O(P) data structure.
20211015 115039.979 INFO             PET1 index= 222            MPIR_CVAR_CH3_RMA_DELAY_ISSUING_FOR_PIGGYBACKING : Specify if delay issuing of RMA operations for piggybacking LOCK/UNLOCK/FLUSH is enabled. It can be either 0 or 1. When it is set to 1, the issuing of LOCK message is delayed until origin process see the first RMA operation and piggyback LOCK with that operation, and the origin process always keeps the current last operation until the ending synchronization call in order to piggyback UNLOCK/FLUSH with that operation. When it is set to 0, in WIN_LOCK/UNLOCK case, the LOCK message is sent out as early as possible, in WIN_LOCK_ALL/UNLOCK_ALL case, the origin process still tries to piggyback LOCK message with the first operation; for UNLOCK/FLUSH message, the origin process no longer keeps the current last operation but only piggyback UNLOCK/FLUSH if there is an operation avaliable in the ending synchronization call.
20211015 115039.979 INFO             PET1 index= 223                                MPIR_CVAR_CH3_RMA_SLOTS_SIZE : Number of RMA slots during window creation. Each slot contains a linked list of target elements. The distribution of ranks among slots follows a round-robin pattern. Requires a positive value.
20211015 115039.979 INFO             PET1 index= 224                    MPIR_CVAR_CH3_RMA_TARGET_LOCK_DATA_BYTES : Size (in bytes) of available lock data this window can provided. If current buffered lock data is more than this value, the process will drop the upcoming operation data. Requires a positive calue.
20211015 115039.979 INFO             PET1 index= 225                            MPIR_CVAR_CH3_EAGER_MAX_MSG_SIZE : This cvar controls the message size at which CH3 switches from eager to rendezvous mode.
20211015 115039.979 INFO             PET1 index= 226                          MPIR_CVAR_CH3_RMA_OP_WIN_POOL_SIZE : Size of the window-private RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediately.  Requires a positive value.
20211015 115039.979 INFO             PET1 index= 227                       MPIR_CVAR_CH3_RMA_OP_GLOBAL_POOL_SIZE : Size of the Global RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediatly.  Requires a positive value.
20211015 115039.979 INFO             PET1 index= 228                      MPIR_CVAR_CH3_RMA_TARGET_WIN_POOL_SIZE : Size of the window-private RMA target pool (in number of targets) that stores information about RMA targets that could not be issued immediately.  Requires a positive value.
20211015 115039.979 INFO             PET1 index= 229                   MPIR_CVAR_CH3_RMA_TARGET_GLOBAL_POOL_SIZE : Size of the Global RMA targets pool (in number of targets) that stores information about RMA targets that could not be issued immediatly.  Requires a positive value.
20211015 115039.979 INFO             PET1 index= 230           MPIR_CVAR_CH3_RMA_TARGET_LOCK_ENTRY_WIN_POOL_SIZE : Size of the window-private RMA lock entries pool (in number of lock entries) that stores information about RMA lock requests that could not be satisfied immediatly.  Requires a positive value.
20211015 115039.979 INFO             PET1 index= 231                     MPIR_CVAR_CH4_OFI_CAPABILITY_SETS_DEBUG : Prints out the configuration of each capability selected via the capability sets interface.
20211015 115039.979 INFO             PET1 index= 232                               MPIR_CVAR_CH4_OFI_ENABLE_DATA : Enable immediate data fields in OFI to transmit source rank outside of the match bits
20211015 115039.979 INFO             PET1 index= 233                           MPIR_CVAR_CH4_OFI_ENABLE_AV_TABLE : If true, the OFI addressing information will be stored with an FI_AV_TABLE. If false, an FI_AV_MAP will be used.
20211015 115039.979 INFO             PET1 index= 234                 MPIR_CVAR_CH4_OFI_ENABLE_SCALABLE_ENDPOINTS : If true, use OFI scalable endpoints.
20211015 115039.979 INFO             PET1 index= 235                    MPIR_CVAR_CH4_OFI_ENABLE_SHARED_CONTEXTS : If set to false (zero), MPICH does not use OFI shared contexts. If set to -1, it is determined by the OFI capability sets based on the provider. Otherwise, MPICH tries to use OFI shared contexts. If they are unavailable, it'll fall back to the mode without shared contexts.
20211015 115039.979 INFO             PET1 index= 236                        MPIR_CVAR_CH4_OFI_ENABLE_MR_SCALABLE : If true, MR_SCALABLE for OFI memory regions. If false, MR_BASIC for OFI memory regions.
20211015 115039.979 INFO             PET1 index= 237                             MPIR_CVAR_CH4_OFI_ENABLE_TAGGED : If true, use tagged message transmission functions in OFI.
20211015 115039.979 INFO             PET1 index= 238                                 MPIR_CVAR_CH4_OFI_ENABLE_AM : If true, enable OFI active message support.
20211015 115039.979 INFO             PET1 index= 239                                MPIR_CVAR_CH4_OFI_ENABLE_RMA : If true, enable OFI RMA support.
20211015 115039.979 INFO             PET1 index= 240                            MPIR_CVAR_CH4_OFI_ENABLE_ATOMICS : If true, enable OFI Atomics support.
20211015 115039.979 INFO             PET1 index= 241                       MPIR_CVAR_CH4_OFI_FETCH_ATOMIC_IOVECS : Specifies the maximum number of iovecs that can be used by the OFI provider for fetch_atomic operations. The default value is -1, indicating that no value is set.
20211015 115039.979 INFO             PET1 index= 242                 MPIR_CVAR_CH4_OFI_ENABLE_DATA_AUTO_PROGRESS : If true, enable MPI data auto progress.
20211015 115039.979 INFO             PET1 index= 243              MPIR_CVAR_CH4_OFI_ENABLE_CONTROL_AUTO_PROGRESS : If true, enable MPI control auto progress.
20211015 115039.979 INFO             PET1 index= 244                       MPIR_CVAR_CH4_OFI_ENABLE_PT2PT_NOPACK : If true, enable iovec for pt2pt.
20211015 115039.979 INFO             PET1 index= 245                           MPIR_CVAR_CH4_OFI_CONTEXT_ID_BITS : Specifies the number of bits that will be used for matching the context ID. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20211015 115039.979 INFO             PET1 index= 246                                 MPIR_CVAR_CH4_OFI_RANK_BITS : Specifies the number of bits that will be used for matching the MPI rank. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20211015 115039.979 INFO             PET1 index= 247                                  MPIR_CVAR_CH4_OFI_TAG_BITS : Specifies the number of bits that will be used for matching the user tag. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20211015 115039.979 INFO             PET1 index= 248                             MPIR_CVAR_CH4_OFI_MAJOR_VERSION : Specifies the major version of the OFI library. The default is the major version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
20211015 115039.979 INFO             PET1 index= 249                             MPIR_CVAR_CH4_OFI_MINOR_VERSION : Specifies the major version of the OFI library. The default is the minor version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
20211015 115039.979 INFO             PET1 index= 250                                  MPIR_CVAR_CH4_OFI_MAX_VNIS : If set to positive, this CVAR specifies the maximum number of CH4 VNIs that OFI netmod exposes.
20211015 115039.979 INFO             PET1 index= 251                           MPIR_CVAR_CH4_OFI_MAX_RMA_SEP_CTX : If set to positive, this CVAR specifies the maximum number of transmit contexts RMA can utilize in a scalable endpoint. This value is effective only when scalable endpoint is available, otherwise it will be ignored.
20211015 115039.979 INFO             PET1 index= 252                          MPIR_CVAR_CH4_OFI_MAX_EAGAIN_RETRY : If set to positive, this CVAR specifies the maximum number of retries of an ofi operations before returning MPIX_ERR_EAGAIN. This value is effective only when the communicator has the MPI_OFI_set_eagain info hint set to true.
20211015 115039.979 INFO             PET1 index= 253                            MPIR_CVAR_CH4_OFI_NUM_AM_BUFFERS : Specifies the number of buffers for receiving active messages.
20211015 115039.979 INFO             PET1 index= 254                                        MPIR_CVAR_CH4_NETMOD : If non-empty, this cvar specifies which network module to use
20211015 115039.979 INFO             PET1 index= 255                                           MPIR_CVAR_CH4_SHM : If non-empty, this cvar specifies which shm module to use
20211015 115039.979 INFO             PET1 index= 256                                MPIR_CVAR_CH4_ROOTS_ONLY_PMI : Enables an optimized business card exchange over PMI for node root processes only.
20211015 115039.979 INFO             PET1 index= 257                            MPIR_CVAR_CH4_RUNTIME_CONF_DEBUG : If enabled, CH4-level runtime configurations are printed out
20211015 115039.979 INFO             PET1 index= 258                                      MPIR_CVAR_CH4_MT_MODEL : Specifies the CH4 multi-threading model. Possible values are: direct (default) handoff trylock
20211015 115039.979 INFO             PET1 index= 259                          MPIR_CVAR_CH4_COMM_CONNECT_TIMEOUT : The default time out period in seconds for a connection attempt to the server communicator where the named port exists but no pending accept. User can change the value for a specified connection through its info argument.
20211015 115039.979 INFO             PET1 index= 260                             MPIR_CVAR_CH4_RANDOM_ADDR_RETRY : The default number of retries for generating a random address. A retrying involves only local operations.
20211015 115039.979 INFO             PET1 index= 261                             MPIR_CVAR_CH4_SHM_SYMHEAP_RETRY : The default number of retries for allocating a symmetric heap in shared memory. A retrying involves collective communication over the group in the shared memory.
20211015 115039.979 INFO             PET1 index= 262                             MPIR_CVAR_CH4_RMA_MEM_EFFICIENT : If true, memory-saving mode is on, per-target object is released at the epoch end call. If false, performance-efficient mode is on, all allocated target objects are cached and freed at win_finalize.
20211015 115039.979 INFO             PET1 index= 263                                      MPIR_CVAR_ENABLE_HCOLL : Enable hcoll collective support.
20211015 115039.979 INFO             PET1 index= 264                                   MPIR_CVAR_COLL_SCHED_DUMP : Print schedule data for nonblocking collective operations.
20211015 115039.979 INFO             PET1 --- VMK::logSystem() end ---------------------------------
20211015 115039.979 INFO             PET1 main: --- VMK::log() start -------------------------------------
20211015 115039.979 INFO             PET1 main: vm located at: 0x7fafa8c0cf90
20211015 115039.979 INFO             PET1 main: petCount=6 localPet=1 mypthid=0 currentSsiPe=-1
20211015 115039.979 INFO             PET1 main: Current system level OMP_NUM_THREADS setting for local PET: 12
20211015 115039.979 INFO             PET1 main: ssiCount=1 localSsi=0
20211015 115039.979 INFO             PET1 main: mpionly=1 threadsflag=0
20211015 115039.979 INFO             PET1 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20211015 115039.979 INFO             PET1 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20211015 115039.979 INFO             PET1 main:  PE=0 SSI=0 SSIPE=0
20211015 115039.979 INFO             PET1 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20211015 115039.979 INFO             PET1 main:  PE=1 SSI=0 SSIPE=1
20211015 115039.979 INFO             PET1 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20211015 115039.979 INFO             PET1 main:  PE=2 SSI=0 SSIPE=2
20211015 115039.979 INFO             PET1 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20211015 115039.979 INFO             PET1 main:  PE=3 SSI=0 SSIPE=3
20211015 115039.979 INFO             PET1 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20211015 115039.979 INFO             PET1 main:  PE=4 SSI=0 SSIPE=4
20211015 115039.979 INFO             PET1 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20211015 115039.979 INFO             PET1 main:  PE=5 SSI=0 SSIPE=5
20211015 115039.979 INFO             PET1 main: --- VMK::log() end ---------------------------------------
20211015 115039.980 INFO             PET1 Executing 'userm1_setvm'
20211015 115039.980 INFO             PET1 Executing 'userm1_register'
20211015 115039.980 INFO             PET1 Executing 'userm2_setvm'
20211015 115039.980 INFO             PET1 Executing 'userm2_register'
20211015 115039.980 INFO             PET1 model1: --- VMK::log() start -------------------------------------
20211015 115039.980 INFO             PET1 model1: vm located at: 0x7fafa9b0db20
20211015 115039.980 INFO             PET1 model1: petCount=6 localPet=1 mypthid=0 currentSsiPe=-1
20211015 115039.980 INFO             PET1 model1: Current system level OMP_NUM_THREADS setting for local PET: 12
20211015 115039.980 INFO             PET1 model1: ssiCount=1 localSsi=0
20211015 115039.980 INFO             PET1 model1: mpionly=1 threadsflag=0
20211015 115039.980 INFO             PET1 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20211015 115039.980 INFO             PET1 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20211015 115039.980 INFO             PET1 model1:  PE=0 SSI=0 SSIPE=0
20211015 115039.980 INFO             PET1 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20211015 115039.980 INFO             PET1 model1:  PE=1 SSI=0 SSIPE=1
20211015 115039.980 INFO             PET1 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20211015 115039.980 INFO             PET1 model1:  PE=2 SSI=0 SSIPE=2
20211015 115039.980 INFO             PET1 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20211015 115039.980 INFO             PET1 model1:  PE=3 SSI=0 SSIPE=3
20211015 115039.980 INFO             PET1 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20211015 115039.980 INFO             PET1 model1:  PE=4 SSI=0 SSIPE=4
20211015 115039.980 INFO             PET1 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20211015 115039.980 INFO             PET1 model1:  PE=5 SSI=0 SSIPE=5
20211015 115039.980 INFO             PET1 model1: --- VMK::log() end ---------------------------------------
20211015 115039.985 INFO             PET1 Entering 'user1_run'
20211015 115039.985 INFO             PET1  user1_run: on SSIPE:           -1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20211015 115040.388 INFO             PET1  user1_run: on SSIPE:           -1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20211015 115040.636 INFO             PET1  user1_run: on SSIPE:           -1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20211015 115040.896 INFO             PET1  user1_run: on SSIPE:           -1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20211015 115041.145 INFO             PET1  user1_run: on SSIPE:           -1  Filling data lbound:        1668           1           1  ubound:        3334        1200          10
20211015 115041.401 INFO             PET1 Exiting 'user1_run'
20211015 115041.412 INFO             PET1 Entering 'user2_run'
20211015 115041.412 INFO             PET1 model2: --- VMK::log() start -------------------------------------
20211015 115041.412 INFO             PET1 model2: vm located at: 0x7fafa9b0e2a0
20211015 115041.412 INFO             PET1 model2: petCount=6 localPet=1 mypthid=0 currentSsiPe=-1
20211015 115041.412 INFO             PET1 model2: Current system level OMP_NUM_THREADS setting for local PET: 12
20211015 115041.412 INFO             PET1 model2: ssiCount=1 localSsi=0
20211015 115041.412 INFO             PET1 model2: mpionly=1 threadsflag=0
20211015 115041.412 INFO             PET1 model2: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20211015 115041.412 INFO             PET1 model2: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20211015 115041.412 INFO             PET1 model2:  PE=0 SSI=0 SSIPE=0
20211015 115041.412 INFO             PET1 model2: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20211015 115041.412 INFO             PET1 model2:  PE=1 SSI=0 SSIPE=1
20211015 115041.412 INFO             PET1 model2: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20211015 115041.412 INFO             PET1 model2:  PE=2 SSI=0 SSIPE=2
20211015 115041.412 INFO             PET1 model2: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20211015 115041.412 INFO             PET1 model2:  PE=3 SSI=0 SSIPE=3
20211015 115041.412 INFO             PET1 model2: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20211015 115041.412 INFO             PET1 model2:  PE=4 SSI=0 SSIPE=4
20211015 115041.412 INFO             PET1 model2: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20211015 115041.412 INFO             PET1 model2:  PE=5 SSI=0 SSIPE=5
20211015 115041.412 INFO             PET1 model2: --- VMK::log() end ---------------------------------------
20211015 115041.412 INFO             PET1  user2_run: ssiLocalDeCount=           6
20211015 115041.413 INFO             PET1  user2_run: OpenMP thread:           1  on SSIPE:           -1  Testing data for localDe =           1  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20211015 115041.413 INFO             PET1  user2_run: OpenMP thread:           3  on SSIPE:           -1  Testing data for localDe =           3  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20211015 115041.423 INFO             PET1  user2_run: OpenMP thread:           4  on SSIPE:           -1  Testing data for localDe =           4  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20211015 115041.433 INFO             PET1  user2_run: OpenMP thread:           0  on SSIPE:           -1  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20211015 115041.453 INFO             PET1  user2_run: OpenMP thread:           5  on SSIPE:           -1  Testing data for localDe =           5  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20211015 115041.473 INFO             PET1  user2_run: OpenMP thread:           2  on SSIPE:           -1  Testing data for localDe =           2  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20211015 115042.972 INFO             PET1  user2_run: OpenMP thread:           0  on SSIPE:           -1  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20211015 115042.973 INFO             PET1  user2_run: OpenMP thread:           1  on SSIPE:           -1  Testing data for localDe =           1  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20211015 115042.974 INFO             PET1  user2_run: OpenMP thread:           3  on SSIPE:           -1  Testing data for localDe =           3  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20211015 115042.974 INFO             PET1  user2_run: OpenMP thread:           4  on SSIPE:           -1  Testing data for localDe =           4  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20211015 115042.974 INFO             PET1  user2_run: OpenMP thread:           5  on SSIPE:           -1  Testing data for localDe =           5  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20211015 115042.982 INFO             PET1  user2_run: OpenMP thread:           2  on SSIPE:           -1  Testing data for localDe =           2  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20211015 115044.287 INFO             PET1  user2_run: OpenMP thread:           0  on SSIPE:           -1  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20211015 115044.287 INFO             PET1  user2_run: OpenMP thread:           1  on SSIPE:           -1  Testing data for localDe =           1  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20211015 115044.288 INFO             PET1  user2_run: OpenMP thread:           3  on SSIPE:           -1  Testing data for localDe =           3  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20211015 115044.289 INFO             PET1  user2_run: OpenMP thread:           4  on SSIPE:           -1  Testing data for localDe =           4  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20211015 115044.290 INFO             PET1  user2_run: OpenMP thread:           2  on SSIPE:           -1  Testing data for localDe =           2  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20211015 115044.291 INFO             PET1  user2_run: OpenMP thread:           5  on SSIPE:           -1  Testing data for localDe =           5  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20211015 115045.629 INFO             PET1  user2_run: OpenMP thread:           0  on SSIPE:           -1  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20211015 115045.632 INFO             PET1  user2_run: OpenMP thread:           1  on SSIPE:           -1  Testing data for localDe =           1  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20211015 115045.632 INFO             PET1  user2_run: OpenMP thread:           4  on SSIPE:           -1  Testing data for localDe =           4  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20211015 115045.633 INFO             PET1  user2_run: OpenMP thread:           3  on SSIPE:           -1  Testing data for localDe =           3  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20211015 115045.633 INFO             PET1  user2_run: OpenMP thread:           5  on SSIPE:           -1  Testing data for localDe =           5  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20211015 115045.637 INFO             PET1  user2_run: OpenMP thread:           2  on SSIPE:           -1  Testing data for localDe =           2  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20211015 115046.961 INFO             PET1  user2_run: OpenMP thread:           0  on SSIPE:           -1  Testing data for localDe =           0  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20211015 115046.961 INFO             PET1  user2_run: OpenMP thread:           1  on SSIPE:           -1  Testing data for localDe =           1  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20211015 115046.962 INFO             PET1  user2_run: OpenMP thread:           4  on SSIPE:           -1  Testing data for localDe =           4  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20211015 115046.964 INFO             PET1  user2_run: OpenMP thread:           3  on SSIPE:           -1  Testing data for localDe =           3  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20211015 115046.971 INFO             PET1  user2_run: OpenMP thread:           5  on SSIPE:           -1  Testing data for localDe =           5  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20211015 115046.971 INFO             PET1  user2_run: OpenMP thread:           2  on SSIPE:           -1  Testing data for localDe =           2  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20211015 115048.268 INFO             PET1  user2_run: All data correct.
20211015 115048.268 INFO             PET1 Exiting 'user2_run'
20211015 115048.353 INFO             PET1  NUMBER_OF_PROCESSORS           6
20211015 115048.353 INFO             PET1  PASS  System Test ESMF_FieldSharedDeSSISTest, ESMF_FieldSharedDeSSISTest.F90, line 276
20211015 115048.353 INFO             PET1 Finalizing ESMF
20211015 115039.974 INFO             PET2 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20211015 115039.975 INFO             PET2 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20211015 115039.975 INFO             PET2 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20211015 115039.975 INFO             PET2 !!! FOR PRODUCTION RUNS, USE:                      !!!
20211015 115039.975 INFO             PET2 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20211015 115039.975 INFO             PET2 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20211015 115039.975 INFO             PET2 Running with ESMF Version   : ESMF_8_2_0_beta_snapshot_22-10-g6974ca47a6
20211015 115039.975 INFO             PET2 ESMF library build date/time: "Oct 15 2021" "11:35:17"
20211015 115039.975 INFO             PET2 ESMF library build location : /Volumes/esmf/rocky/esmf-testing/gfortran_9.3.0_mpich3_O_release_8.2.0
20211015 115039.975 INFO             PET2 ESMF_COMM                   : mpich3
20211015 115039.975 INFO             PET2 ESMF_MOAB                   : enabled
20211015 115039.975 INFO             PET2 ESMF_LAPACK                 : enabled
20211015 115039.975 INFO             PET2 ESMF_NETCDF                 : enabled
20211015 115039.975 INFO             PET2 ESMF_PNETCDF                : disabled
20211015 115039.975 INFO             PET2 ESMF_PIO                    : enabled
20211015 115039.975 INFO             PET2 ESMF_YAMLCPP                : enabled
20211015 115039.976 INFO             PET2 --- VMK::logSystem() start -------------------------------
20211015 115039.976 INFO             PET2 esmfComm=mpich3
20211015 115039.976 INFO             PET2 isPthreadsEnabled=0
20211015 115039.976 INFO             PET2 isOpenMPEnabled=1
20211015 115039.976 INFO             PET2 isOpenACCEnabled=0
20211015 115039.976 INFO             PET2 isSsiSharedMemoryEnabled=1
20211015 115039.976 INFO             PET2 ssiCount=1 peCount=6
20211015 115039.976 INFO             PET2 PE=0 SSI=0 SSIPE=0
20211015 115039.976 INFO             PET2 PE=1 SSI=0 SSIPE=1
20211015 115039.976 INFO             PET2 PE=2 SSI=0 SSIPE=2
20211015 115039.976 INFO             PET2 PE=3 SSI=0 SSIPE=3
20211015 115039.976 INFO             PET2 PE=4 SSI=0 SSIPE=4
20211015 115039.976 INFO             PET2 PE=5 SSI=0 SSIPE=5
20211015 115039.976 INFO             PET2 --- VMK::logSystem() MPI Control Variables ---------------
20211015 115039.976 INFO             PET2 index=   0                          MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the short message algorithm will be used if the send buffer size is < this value (in bytes). (See also: MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE)
20211015 115039.976 INFO             PET2 index=   1                           MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the long message algorithm will be used if the send buffer size is >= this value (in bytes) (See also: MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE)
20211015 115039.976 INFO             PET2 index=   2                         MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM : Variable to select allgather algorithmauto               - Internal algorithm selectionbrucks             - Force brucks algorithmnb                 - Force nonblocking algorithmrecursive_doubling - Force recursive doubling algorithmring               - Force ring algorithm
20211015 115039.976 INFO             PET2 index=   3                         MPIR_CVAR_ALLGATHER_INTER_ALGORITHM : Variable to select allgather algorithmauto                      - Internal algorithm selectionlocal_gather_remote_bcast - Force local-gather-remote-bcast algorithmnb                        - Force nonblocking algorithm
20211015 115039.976 INFO             PET2 index=   4                       MPIR_CVAR_ALLGATHER_DEVICE_COLLECTIVE : If set to true, MPI_Allgather will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level allgather function will not be called.
20211015 115039.976 INFO             PET2 index=   5                      MPIR_CVAR_ALLGATHERV_PIPELINE_MSG_SIZE : The smallest message size that will be used for the pipelined, large-message, ring algorithm in the MPI_Allgatherv implementation.
20211015 115039.976 INFO             PET2 index=   6                        MPIR_CVAR_ALLGATHERV_INTRA_ALGORITHM : Variable to select allgatherv algorithmauto               - Internal algorithm selectionbrucks             - Force brucks algorithmnb                 - Force nonblocking algorithmrecursive_doubling - Force recursive doubling algorithmring               - Force ring algorithm
20211015 115039.976 INFO             PET2 index=   7                        MPIR_CVAR_ALLGATHERV_INTER_ALGORITHM : Variable to select allgatherv algorithmauto                      - Internal algorithm selectionnb                        - Force nonblocking algorithmremote_gather_local_bcast - Force remote-gather-local-bcast algorithm
20211015 115039.976 INFO             PET2 index=   8                      MPIR_CVAR_ALLGATHERV_DEVICE_COLLECTIVE : If set to true, MPI_Allgatherv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level allgatherv function will not be called.
20211015 115039.976 INFO             PET2 index=   9                          MPIR_CVAR_ALLREDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20211015 115039.976 INFO             PET2 index=  10                            MPIR_CVAR_ENABLE_SMP_COLLECTIVES : Enable SMP aware collective communication.
20211015 115039.976 INFO             PET2 index=  11                              MPIR_CVAR_ENABLE_SMP_ALLREDUCE : Enable SMP aware allreduce.
20211015 115039.976 INFO             PET2 index=  12                        MPIR_CVAR_MAX_SMP_ALLREDUCE_MSG_SIZE : Maximum message size for which SMP-aware allreduce is used.  A value of '0' uses SMP-aware allreduce for all message sizes.
20211015 115039.976 INFO             PET2 index=  13                         MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM : Variable to select allreduce algorithmauto                     - Internal algorithm selectionnb                       - Force nonblocking algorithmrecursive_doubling       - Force recursive doubling algorithmreduce_scatter_allgather - Force reduce scatter allgather algorithm
20211015 115039.976 INFO             PET2 index=  14                         MPIR_CVAR_ALLREDUCE_INTER_ALGORITHM : Variable to select allreduce algorithmauto                  - Internal algorithm selectionnb                    - Force nonblocking algorithmreduce_exchange_bcast - Force reduce-exchange-bcast algorithm
20211015 115039.976 INFO             PET2 index=  15                       MPIR_CVAR_ALLREDUCE_DEVICE_COLLECTIVE : If set to true, MPI_Allreduce will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level allreduce function will not be called.
20211015 115039.976 INFO             PET2 index=  16                           MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE : the short message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value (See also: MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE)
20211015 115039.976 INFO             PET2 index=  17                          MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE : the medium message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value and larger than MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE (See also: MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE)
20211015 115039.976 INFO             PET2 index=  18                                 MPIR_CVAR_ALLTOALL_THROTTLE : max no. of irecvs/isends posted at a time in some alltoall algorithms. Setting it to 0 causes all irecvs/isends to be posted at once
20211015 115039.976 INFO             PET2 index=  19                          MPIR_CVAR_ALLTOALL_INTRA_ALGORITHM : Variable to select alltoall algorithmauto                      - Internal algorithm selectionbrucks                    - Force brucks algorithmnb                        - Force nonblocking algorithmpairwise                  - Force pairwise algorithmpairwise_sendrecv_replace - Force pairwise sendrecv replace algorithmscattered                 - Force scattered algorithm
20211015 115039.976 INFO             PET2 index=  20                          MPIR_CVAR_ALLTOALL_INTER_ALGORITHM : Variable to select alltoall algorithmauto              - Internal algorithm selectionnb                - Force nonblocking algorithmpairwise_exchange - Force pairwise exchange algorithm
20211015 115039.976 INFO             PET2 index=  21                        MPIR_CVAR_ALLTOALL_DEVICE_COLLECTIVE : If set to true, MPI_Alltoall will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level alltoall function will not be called.
20211015 115039.976 INFO             PET2 index=  22                         MPIR_CVAR_ALLTOALLV_INTRA_ALGORITHM : Variable to select alltoallv algorithmauto                      - Internal algorithm selectionnb                        - Force nonblocking algorithmpairwise_sendrecv_replace - Force pairwise_sendrecv_replace algorithmscattered                 - Force scattered algorithm
20211015 115039.976 INFO             PET2 index=  23                         MPIR_CVAR_ALLTOALLV_INTER_ALGORITHM : Variable to select alltoallv algorithmauto              - Internal algorithm selectionpairwise_exchange - Force pairwise exchange algorithmnb                - Force nonblocking algorithm
20211015 115039.976 INFO             PET2 index=  24                       MPIR_CVAR_ALLTOALLV_DEVICE_COLLECTIVE : If set to true, MPI_Alltoallv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level alltoallv function will not be called.
20211015 115039.976 INFO             PET2 index=  25                         MPIR_CVAR_ALLTOALLW_INTRA_ALGORITHM : Variable to select alltoallw algorithmauto                      - Internal algorithm selectionnb                        - Force nonblocking algorithmpairwise_sendrecv_replace - Force pairwise sendrecv replace algorithmscattered                 - Force scattered algorithm
20211015 115039.976 INFO             PET2 index=  26                         MPIR_CVAR_ALLTOALLW_INTER_ALGORITHM : Variable to select alltoallw algorithmauto              - Internal algorithm selectionnb                - Force nonblocking algorithmpairwise_exchange - Force pairwise exchange algorithm
20211015 115039.976 INFO             PET2 index=  27                       MPIR_CVAR_ALLTOALLW_DEVICE_COLLECTIVE : If set to true, MPI_Alltoallw will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level alltoallw function will not be called.
20211015 115039.976 INFO             PET2 index=  28                                MPIR_CVAR_ENABLE_SMP_BARRIER : Enable SMP aware barrier.
20211015 115039.976 INFO             PET2 index=  29                           MPIR_CVAR_BARRIER_INTRA_ALGORITHM : Variable to select barrier algorithmauto               - Internal algorithm selectionnb                 - Force nonblocking algorithmrecursive_doubling - Force recursive doubling algorithm
20211015 115039.976 INFO             PET2 index=  30                           MPIR_CVAR_BARRIER_INTER_ALGORITHM : Variable to select barrier algorithmauto  - Internal algorithm selectionbcast - Force bcast algorithmnb    - Force nonblocking algorithm
20211015 115039.976 INFO             PET2 index=  31                         MPIR_CVAR_BARRIER_DEVICE_COLLECTIVE : If set to true, MPI_Barrier will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level barrier function will not be called.
20211015 115039.976 INFO             PET2 index=  32                                   MPIR_CVAR_BCAST_MIN_PROCS : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_SHORT_MSG_SIZE, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20211015 115039.976 INFO             PET2 index=  33                              MPIR_CVAR_BCAST_SHORT_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20211015 115039.976 INFO             PET2 index=  34                               MPIR_CVAR_BCAST_LONG_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_SHORT_MSG_SIZE)
20211015 115039.976 INFO             PET2 index=  35                                  MPIR_CVAR_ENABLE_SMP_BCAST : Enable SMP aware broadcast (See also: MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE)
20211015 115039.976 INFO             PET2 index=  36                            MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE : Maximum message size for which SMP-aware broadcast is used.  A value of '0' uses SMP-aware broadcast for all message sizes. (See also: MPIR_CVAR_ENABLE_SMP_BCAST)
20211015 115039.976 INFO             PET2 index=  37                             MPIR_CVAR_BCAST_INTRA_ALGORITHM : Variable to select bcast algorithmauto                                    - Internal algorithm selectionbinomial                                - Force Binomial Treenb                                      - Force nonblocking algorithmscatter_recursive_doubling_allgather    - Force Scatter Recursive-Doubling Allgatherscatter_ring_allgather                  - Force Scatter Ring
20211015 115039.976 INFO             PET2 index=  38                             MPIR_CVAR_BCAST_INTER_ALGORITHM : Variable to select bcast algorithmauto                    - Internal algorithm selectionnb                      - Force nonblocking algorithmremote_send_local_bcast - Force remote-send-local-bcast algorithm
20211015 115039.977 INFO             PET2 index=  39                           MPIR_CVAR_BCAST_DEVICE_COLLECTIVE : If set to true, MPI_Bcast will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually.  If set to false, the device-level bcast function will not be called.
20211015 115039.977 INFO             PET2 index=  40                            MPIR_CVAR_EXSCAN_INTRA_ALGORITHM : Variable to select allgather algorithmauto               - Internal algorithm selectionnb                 - Force nonblocking algorithmrecursive_doubling - Force recursive doubling algorithm
20211015 115039.977 INFO             PET2 index=  41                          MPIR_CVAR_EXSCAN_DEVICE_COLLECTIVE : If set to true, MPI_Exscan will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level exscan function will not be called.
20211015 115039.977 INFO             PET2 index=  42                       MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_VSMALL_MSG_SIZE)
20211015 115039.977 INFO             PET2 index=  43                            MPIR_CVAR_GATHER_INTRA_ALGORITHM : Variable to select gather algorithmauto     - Internal algorithm selectionbinomial - Force binomial algorithmnb       - Force nonblocking algorithm
20211015 115039.977 INFO             PET2 index=  44                            MPIR_CVAR_GATHER_INTER_ALGORITHM : Variable to select gather algorithmauto                     - Internal algorithm selectionlinear                   - Force linear algorithmlocal_gather_remote_send - Force local-gather-remote-send algorithmnb                       - Force nonblocking algorithm
20211015 115039.977 INFO             PET2 index=  45                          MPIR_CVAR_GATHER_DEVICE_COLLECTIVE : If set to true, MPI_Gather will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level gather function will not be called.
20211015 115039.977 INFO             PET2 index=  46                            MPIR_CVAR_GATHER_VSMALL_MSG_SIZE : use a temporary buffer for intracommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE)
20211015 115039.977 INFO             PET2 index=  47                           MPIR_CVAR_GATHERV_INTRA_ALGORITHM : Variable to select gatherv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithmnb     - Force nonblocking algorithm
20211015 115039.977 INFO             PET2 index=  48                           MPIR_CVAR_GATHERV_INTER_ALGORITHM : Variable to select gatherv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithmnb     - Force nonblocking algorithm
20211015 115039.977 INFO             PET2 index=  49                         MPIR_CVAR_GATHERV_DEVICE_COLLECTIVE : If set to true, MPI_Gatherv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level gatherv function will not be called.
20211015 115039.977 INFO             PET2 index=  50                     MPIR_CVAR_GATHERV_INTER_SSEND_MIN_PROCS : Use Ssend (synchronous send) for intercommunicator MPI_Gatherv if the "group B" size is >= this value.  Specifying "-1" always avoids using Ssend.  For backwards compatibility, specifying "0" uses the default value.
20211015 115039.977 INFO             PET2 index=  51                           MPIR_CVAR_IALLGATHER_RECEXCH_KVAL : k value for recursive exchange based iallgather
20211015 115039.977 INFO             PET2 index=  52                            MPIR_CVAR_IALLGATHER_BRUCKS_KVAL : k value for radix in brucks based iallgather
20211015 115039.977 INFO             PET2 index=  53                        MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM : Variable to select iallgather algorithmauto               - Internal algorithm selectionbrucks             - Force brucks algorithmrecursive_doubling - Force recursive doubling algorithmring               - Force ring algorithmrecexch_distance_doubling    - Force generic transport recursive exchange with neighbours doubling in distance in each phaserecexch_distance_halving  - Force generic transport recursive exchange with neighbours halving in distance in each phasegentran_brucks     - Force generic transport based brucks algorithm
20211015 115039.977 INFO             PET2 index=  54                        MPIR_CVAR_IALLGATHER_INTER_ALGORITHM : Variable to select iallgather algorithmauto                      - Internal algorithm selectionlocal_gather_remote_bcast - Force local-gather-remote-bcast algorithm
20211015 115039.977 INFO             PET2 index=  55                      MPIR_CVAR_IALLGATHER_DEVICE_COLLECTIVE : If set to true, MPI_Iallgather will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iallgather function will not be called.
20211015 115039.977 INFO             PET2 index=  56                          MPIR_CVAR_IALLGATHERV_RECEXCH_KVAL : k value for recursive exchange based iallgatherv
20211015 115039.977 INFO             PET2 index=  57                       MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM : Variable to select iallgatherv algorithmauto               - Internal algorithm selectionbrucks             - Force brucks algorithmrecursive_doubling - Force recursive doubling algorithmring               - Force ring algorithmrecexch_distance_doubling    - Force generic transport recursive exchange with neighbours doubling in distance in each phaserecexch_distance_halving     - Force generic transport recursive exchange with neighbours halving in distance in each phasegentran_ring              - Force generic transport ring algorithm
20211015 115039.977 INFO             PET2 index=  58                       MPIR_CVAR_IALLGATHERV_INTER_ALGORITHM : Variable to select iallgatherv algorithmauto                      - Internal algorithm selectionremote_gather_local_bcast - Force remote-gather-local-bcast algorithm
20211015 115039.977 INFO             PET2 index=  59                     MPIR_CVAR_IALLGATHERV_DEVICE_COLLECTIVE : If set to true, MPI_Iallgatherv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iallgatherv function will not be called.
20211015 115039.977 INFO             PET2 index=  60                              MPIR_CVAR_IALLREDUCE_TREE_KVAL : k value for tree based iallreduce (for tree_kary and tree_knomial)
20211015 115039.977 INFO             PET2 index=  61               MPIR_CVAR_IALLREDUCE_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based iallreduce (tree_kary and tree_knomial). Default value is 0, that is, no pipelining by default
20211015 115039.977 INFO             PET2 index=  62                  MPIR_CVAR_IALLREDUCE_TREE_BUFFER_PER_CHILD : If set to true, a rank in tree_kary and tree_knomial algorithms will allocate a dedicated buffer for every child it receives data from. This would mean more memory consumption but it would allow preposting of the receives and hence reduce the number of unexpected messages. If set to false, there is only one buffer that is used to receive the data from all the children. The receives are therefore serialized, that is, only one receive can be posted at a time.
20211015 115039.977 INFO             PET2 index=  63                           MPIR_CVAR_IALLREDUCE_RECEXCH_KVAL : k value for recursive exchange based iallreduce
20211015 115039.977 INFO             PET2 index=  64                        MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM : Variable to select iallreduce algorithmauto                     - Internal algorithm selectionnaive                    - Force naive algorithmrecursive_doubling       - Force recursive doubling algorithmreduce_scatter_allgather - Force reduce scatter allgather algorithmrecexch_single_buffer    - Force generic transport recursive exchange with single buffer for receivesrecexch_multiple_buffer  - Force generic transport recursive exchange with multiple buffers for receives
20211015 115039.977 INFO             PET2 index=  65                        MPIR_CVAR_IALLREDUCE_INTER_ALGORITHM : Variable to select iallreduce algorithmauto                      - Internal algorithm selectionremote_reduce_local_bcast - Force remote-reduce-local-bcast algorithm
20211015 115039.977 INFO             PET2 index=  66                      MPIR_CVAR_IALLREDUCE_DEVICE_COLLECTIVE : If set to true, MPI_Iallreduce will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iallreduce function will not be called.
20211015 115039.977 INFO             PET2 index=  67                         MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM : Variable to select ialltoall algorithmauto              - Internal algorithm selectionbrucks            - Force brucks algorithminplace           - Force inplace algorithmpairwise          - Force pairwise algorithmpermuted_sendrecv - Force permuted sendrecv algorithm
20211015 115039.977 INFO             PET2 index=  68                         MPIR_CVAR_IALLTOALL_INTER_ALGORITHM : Variable to select ialltoall algorithmauto              - Internal algorithm selectionpairwise_exchange - Force pairwise exchange algorithm
20211015 115039.977 INFO             PET2 index=  69                       MPIR_CVAR_IALLTOALL_DEVICE_COLLECTIVE : If set to true, MPI_Ialltoall will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ialltoall function will not be called.
20211015 115039.977 INFO             PET2 index=  70                        MPIR_CVAR_IALLTOALLV_INTRA_ALGORITHM : Variable to select ialltoallv algorithmauto              - Internal algorithm selectionblocked           - Force blocked algorithminplace           - Force inplace algorithmpairwise_exchange - Force pairwise exchange algorithm
20211015 115039.977 INFO             PET2 index=  71                        MPIR_CVAR_IALLTOALLV_INTER_ALGORITHM : Variable to select ialltoallv algorithmauto - Internal algorithm selection
20211015 115039.977 INFO             PET2 index=  72                      MPIR_CVAR_IALLTOALLV_DEVICE_COLLECTIVE : If set to true, MPI_Ialltoallv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ialltoallv function will not be called.
20211015 115039.977 INFO             PET2 index=  73                        MPIR_CVAR_IALLTOALLW_INTRA_ALGORITHM : Variable to select ialltoallw algorithmauto              - Internal algorithm selectionblocked           - Force blocked algorithminplace           - Force inplace algorithmpairwise_exchange - Force pairwise exchange algorithm
20211015 115039.977 INFO             PET2 index=  74                        MPIR_CVAR_IALLTOALLW_INTER_ALGORITHM : Variable to select ialltoallw algorithmauto - Internal algorithm selection
20211015 115039.977 INFO             PET2 index=  75                      MPIR_CVAR_IALLTOALLW_DEVICE_COLLECTIVE : If set to true, MPI_Ialltoallw will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ialltoallw function will not be called.
20211015 115039.977 INFO             PET2 index=  76                             MPIR_CVAR_IBARRIER_RECEXCH_KVAL : k value for recursive exchange based ibarrier
20211015 115039.977 INFO             PET2 index=  77                          MPIR_CVAR_IBARRIER_INTRA_ALGORITHM : Variable to select ibarrier algorithmauto               - Internal algorithm selectionrecursive_doubling - Force recursive doubling algorithmrecexch            - Force generic transport based recursive exchange algorithm
20211015 115039.977 INFO             PET2 index=  78                          MPIR_CVAR_IBARRIER_INTER_ALGORITHM : Variable to select ibarrier algorithmauto  - Internal algorithm selectionbcast - Force bcast algorithm
20211015 115039.977 INFO             PET2 index=  79                        MPIR_CVAR_IBARRIER_DEVICE_COLLECTIVE : If set to true, MPI_Ibarrier will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ibarrier function will not be called.
20211015 115039.977 INFO             PET2 index=  80                                  MPIR_CVAR_IBCAST_TREE_KVAL : k value for tree (kary, knomial, etc.) based ibcast
20211015 115039.977 INFO             PET2 index=  81                                  MPIR_CVAR_IBCAST_TREE_TYPE : Tree type for tree based ibcast kary      - kary tree type knomial_1 - knomial_1 tree type knomial_2 - knomial_2 tree type
20211015 115039.977 INFO             PET2 index=  82                   MPIR_CVAR_IBCAST_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based ibcast. Default value is 0, that is, no pipelining by default
20211015 115039.977 INFO             PET2 index=  83                            MPIR_CVAR_IBCAST_RING_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in ibcast ring algorithm. Default value is 0, that is, no pipelining by default
20211015 115039.977 INFO             PET2 index=  84                            MPIR_CVAR_IBCAST_INTRA_ALGORITHM : Variable to select ibcast algorithmauto                                 - Internal algorithm selectionbinomial                             - Force Binomial algorithmscatter_recursive_doubling_allgather - Force Scatter Recursive Doubling Allgather algorithmscatter_ring_allgather               - Force Scatter Ring Allgather algorithmtree                                 - Force Generic Transport Tree algorithmscatter_recexch_allgather            - Force Generic Transport Scatter followed by Recursive Exchange Allgather algorithmring                                 - Force Generic Transport Ring algorithm
20211015 115039.977 INFO             PET2 index=  85                               MPIR_CVAR_IBCAST_SCATTER_KVAL : k value for tree based scatter in scatter_recexch_allgather algorithm
20211015 115039.977 INFO             PET2 index=  86                     MPIR_CVAR_IBCAST_ALLGATHER_RECEXCH_KVAL : k value for recursive exchange based allgather in scatter_recexch_allgather algorithm
20211015 115039.977 INFO             PET2 index=  87                            MPIR_CVAR_IBCAST_INTER_ALGORITHM : Variable to select ibcast algorithmauto - Internal algorithm selectionflat - Force flat algorithm
20211015 115039.977 INFO             PET2 index=  88                          MPIR_CVAR_IBCAST_DEVICE_COLLECTIVE : If set to true, MPI_Ibcast will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually.  If set to false, the device-level ibcast function will not be called.
20211015 115039.977 INFO             PET2 index=  89                           MPIR_CVAR_IEXSCAN_INTRA_ALGORITHM : Variable to select iexscan algorithmauto               - Internal algorithm selectionrecursive_doubling - Force recursive doubling algorithm
20211015 115039.977 INFO             PET2 index=  90                         MPIR_CVAR_IEXSCAN_DEVICE_COLLECTIVE : If set to true, MPI_Iexscan will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iexscan function will not be called.
20211015 115039.977 INFO             PET2 index=  91                           MPIR_CVAR_IGATHER_INTRA_ALGORITHM : Variable to select igather algorithmauto     - Internal algorithm selectionbinomial - Force binomial algorithmtree     - Force genetric transport based tree algorithm
20211015 115039.977 INFO             PET2 index=  92                                 MPIR_CVAR_IGATHER_TREE_KVAL : k value for tree based igather
20211015 115039.977 INFO             PET2 index=  93                           MPIR_CVAR_IGATHER_INTER_ALGORITHM : Variable to select igather algorithmauto        - Internal algorithm selectionlong_inter  - Force long inter algorithmshort_inter - Force short inter algorithm
20211015 115039.977 INFO             PET2 index=  94                         MPIR_CVAR_IGATHER_DEVICE_COLLECTIVE : If set to true, MPI_Igather will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level igather function will not be called.
20211015 115039.977 INFO             PET2 index=  95                          MPIR_CVAR_IGATHERV_INTRA_ALGORITHM : Variable to select igatherv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET2 index=  96                          MPIR_CVAR_IGATHERV_INTER_ALGORITHM : Variable to select igatherv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET2 index=  97                        MPIR_CVAR_IGATHERV_DEVICE_COLLECTIVE : If set to true, MPI_Igatherv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level igatherv function will not be called.
20211015 115039.977 INFO             PET2 index=  98               MPIR_CVAR_INEIGHBOR_ALLGATHER_INTRA_ALGORITHM : Variable to select ineighbor_allgather algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET2 index=  99               MPIR_CVAR_INEIGHBOR_ALLGATHER_INTER_ALGORITHM : Variable to select ineighbor_allgather algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET2 index= 100             MPIR_CVAR_INEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE : If set to true, MPI_ineighbor_allgather will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ineighbor_allgather function will not be called.
20211015 115039.977 INFO             PET2 index= 101              MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTRA_ALGORITHM : Variable to select ineighbor_allgatherv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET2 index= 102              MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTER_ALGORITHM : Variable to select ineighbor_allgatherv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET2 index= 103            MPIR_CVAR_INEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE : If set to true, MPI_ineighbor_allgatherv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ineighbor_allgatherv function will not be called.
20211015 115039.977 INFO             PET2 index= 104                MPIR_CVAR_INEIGHBOR_ALLTOALL_INTRA_ALGORITHM : Variable to select ineighbor_alltoall algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET2 index= 105                MPIR_CVAR_INEIGHBOR_ALLTOALL_INTER_ALGORITHM : Variable to select ineighbor_alltoall algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET2 index= 106              MPIR_CVAR_INEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE : If set to true, MPI_ineighbor_alltoall will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ineighbor_alltoall function will not be called.
20211015 115039.977 INFO             PET2 index= 107               MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTRA_ALGORITHM : Variable to select ineighbor_alltoallv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET2 index= 108               MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTER_ALGORITHM : Variable to select ineighbor_alltoallv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET2 index= 109             MPIR_CVAR_INEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE : If set to true, MPI_ineighbor_alltoallv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ineighbor_alltoallv function will not be called.
20211015 115039.977 INFO             PET2 index= 110               MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTRA_ALGORITHM : Variable to select ineighbor_alltoallw algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET2 index= 111               MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTER_ALGORITHM : Variable to select ineighbor_alltoallw algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET2 index= 112             MPIR_CVAR_INEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE : If set to true, MPI_ineighbor_alltoallw will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ineighbor_alltoallw function will not be called.
20211015 115039.977 INFO             PET2 index= 113                                 MPIR_CVAR_IREDUCE_TREE_KVAL : k value for tree (kary, knomial, etc.) based ireduce
20211015 115039.977 INFO             PET2 index= 114                                 MPIR_CVAR_IREDUCE_TREE_TYPE : Tree type for tree based ireduce kary      - kary tree knomial_1 - knomial_1 tree knomial_2 - knomial_2 tree
20211015 115039.977 INFO             PET2 index= 115                  MPIR_CVAR_IREDUCE_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based ireduce. Default value is 0, that is, no pipelining by default
20211015 115039.977 INFO             PET2 index= 116                           MPIR_CVAR_IREDUCE_RING_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in ireduce ring algorithm. Default value is 0, that is, no pipelining by default
20211015 115039.977 INFO             PET2 index= 117                     MPIR_CVAR_IREDUCE_TREE_BUFFER_PER_CHILD : If set to true, a rank in tree algorithms will allocate a dedicated buffer for every child it receives data from. This would mean more memory consumption but it would allow preposting of the receives and hence reduce the number of unexpected messages. If set to false, there is only one buffer that is used to receive the data from all the children. The receives are therefore serialized, that is, only one receive can be posted at a time.
20211015 115039.977 INFO             PET2 index= 118                           MPIR_CVAR_IREDUCE_INTRA_ALGORITHM : Variable to select ireduce algorithmauto                  - Internal algorithm selectionbinomial              - Force binomial algorithmreduce_scatter_gather - Force reduce scatter gather algorithmtree                  - Force Generic Transport Treering                  - Force Generic Transport Ring
20211015 115039.977 INFO             PET2 index= 119                           MPIR_CVAR_IREDUCE_INTER_ALGORITHM : Variable to select ireduce algorithmauto                     - Internal algorithm selectionlocal_reduce_remote_send - Force local-reduce-remote-send algorithm
20211015 115039.977 INFO             PET2 index= 120                         MPIR_CVAR_IREDUCE_DEVICE_COLLECTIVE : If set to true, MPI_Ireduce will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ireduce function will not be called.
20211015 115039.977 INFO             PET2 index= 121                      MPIR_CVAR_IREDUCE_SCATTER_RECEXCH_KVAL : k value for recursive exchange based ireduce_scatter
20211015 115039.977 INFO             PET2 index= 122                   MPIR_CVAR_IREDUCE_SCATTER_INTRA_ALGORITHM : Variable to select ireduce_scatter algorithmauto               - Internal algorithm selectionnoncommutative     - Force noncommutative algorithmrecursive_doubling - Force recursive doubling algorithmpairwise           - Force pairwise algorithmrecursive_halving  - Force recursive halving algorithmrecexch  - Force generic transport recursive exchange algorithm
20211015 115039.977 INFO             PET2 index= 123                   MPIR_CVAR_IREDUCE_SCATTER_INTER_ALGORITHM : Variable to select ireduce_scatter algorithmauto                         - Internal algorithm selectionremote_reduce_local_scatterv - Force remote-reduce-local-scatterv algorithm
20211015 115039.977 INFO             PET2 index= 124                 MPIR_CVAR_IREDUCE_SCATTER_DEVICE_COLLECTIVE : If set to true, MPI_Ireduce_scatter will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ireduce_scatter function will not be called.
20211015 115039.978 INFO             PET2 index= 125                MPIR_CVAR_IREDUCE_SCATTER_BLOCK_RECEXCH_KVAL : k value for recursive exchange based ireduce_scatter_block
20211015 115039.978 INFO             PET2 index= 126             MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM : Variable to select ireduce_scatter_block algorithmauto               - Internal algorithm selectionnoncommutative     - Force noncommutative algorithmrecursive_doubling - Force recursive doubling algorithmpairwise           - Force pairwise algorithmrecursive_halving  - Force recursive halving algorithmrecexch  - Force generic transport recursive exchange algorithm
20211015 115039.978 INFO             PET2 index= 127             MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTER_ALGORITHM : Variable to select ireduce_scatter_block algorithmauto                         - Internal algorithm selectionremote_reduce_local_scatterv - Force remote-reduce-local-scatterv algorithm
20211015 115039.978 INFO             PET2 index= 128           MPIR_CVAR_IREDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE : If set to true, MPI_Ireduce_scatter_block will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ireduce_scatter_block function will not be called.
20211015 115039.978 INFO             PET2 index= 129                             MPIR_CVAR_ISCAN_INTRA_ALGORITHM : Variable to select allgather algorithmauto               - Internal algorithm selectionrecursive_doubling - Force recursive doubling algorithm
20211015 115039.978 INFO             PET2 index= 130                           MPIR_CVAR_ISCAN_DEVICE_COLLECTIVE : If set to true, MPI_Iscan will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iscan function will not be called.
20211015 115039.978 INFO             PET2 index= 131                          MPIR_CVAR_ISCATTER_INTRA_ALGORITHM : Variable to select iscatter algorithmauto     - Internal algorithm selectionbinomial - Force binomial algorithmtree     - Force genetric transport based tree algorithm
20211015 115039.978 INFO             PET2 index= 132                                MPIR_CVAR_ISCATTER_TREE_KVAL : k value for tree based iscatter
20211015 115039.978 INFO             PET2 index= 133                          MPIR_CVAR_ISCATTER_INTER_ALGORITHM : Variable to select iscatter algorithmauto                      - Internal algorithm selectionlinear                    - Force linear algorithmremote_send_local_scatter - Force remote-send-local-scatter algorithm
20211015 115039.978 INFO             PET2 index= 134                        MPIR_CVAR_ISCATTER_DEVICE_COLLECTIVE : If set to true, MPI_Iscatter will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iscatter function will not be called.
20211015 115039.978 INFO             PET2 index= 135                         MPIR_CVAR_ISCATTERV_INTRA_ALGORITHM : Variable to select iscatterv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET2 index= 136                         MPIR_CVAR_ISCATTERV_INTER_ALGORITHM : Variable to select iscatterv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET2 index= 137                       MPIR_CVAR_ISCATTERV_DEVICE_COLLECTIVE : If set to true, MPI_Iscatterv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iscatterv function will not be called.
20211015 115039.978 INFO             PET2 index= 138                MPIR_CVAR_NEIGHBOR_ALLGATHER_INTRA_ALGORITHM : Variable to select ineighbor_allgather algorithmauto - Internal algorithm selectionnb   - Force nonblocking algorithm
20211015 115039.978 INFO             PET2 index= 139                MPIR_CVAR_NEIGHBOR_ALLGATHER_INTER_ALGORITHM : Variable to select ineighbor_allgather algorithmauto - Internal algorithm selectionnb   - Force nonblocking algorithm
20211015 115039.978 INFO             PET2 index= 140              MPIR_CVAR_NEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE : If set to true, MPI_neighbor_allgather will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level neighbor_allgather function will not be called.
20211015 115039.978 INFO             PET2 index= 141               MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTRA_ALGORITHM : Variable to select neighbor_allgatherv algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET2 index= 142               MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTER_ALGORITHM : Variable to select neighbor_allgatherv algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET2 index= 143             MPIR_CVAR_NEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE : If set to true, MPI_neighbor_allgatherv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level neighbor_allgatherv function will not be called.
20211015 115039.978 INFO             PET2 index= 144                 MPIR_CVAR_NEIGHBOR_ALLTOALL_INTRA_ALGORITHM : Variable to select neighbor_alltoall algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET2 index= 145                 MPIR_CVAR_NEIGHBOR_ALLTOALL_INTER_ALGORITHM : Variable to select neighbor_alltoall algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET2 index= 146               MPIR_CVAR_NEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE : If set to true, MPI_neighbor_alltoall will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level neighbor_alltoall function will not be called.
20211015 115039.978 INFO             PET2 index= 147                MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTRA_ALGORITHM : Variable to select neighbor_alltoallv algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET2 index= 148                MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTER_ALGORITHM : Variable to select neighbor_alltoallv algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET2 index= 149              MPIR_CVAR_NEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE : If set to true, MPI_neighbor_alltoallv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level neighbor_alltoallv function will not be called.
20211015 115039.978 INFO             PET2 index= 150                MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTRA_ALGORITHM : Variable to select neighbor_alltoallw algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET2 index= 151                MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTER_ALGORITHM : Variable to select neighbor_alltoallw algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET2 index= 152              MPIR_CVAR_NEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE : If set to true, MPI_neighbor_alltoallw will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level neighbor_alltoallw function will not be called.
20211015 115039.978 INFO             PET2 index= 153                             MPIR_CVAR_REDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20211015 115039.978 INFO             PET2 index= 154                                 MPIR_CVAR_ENABLE_SMP_REDUCE : Enable SMP aware reduce.
20211015 115039.978 INFO             PET2 index= 155                           MPIR_CVAR_MAX_SMP_REDUCE_MSG_SIZE : Maximum message size for which SMP-aware reduce is used.  A value of '0' uses SMP-aware reduce for all message sizes.
20211015 115039.978 INFO             PET2 index= 156                            MPIR_CVAR_REDUCE_INTRA_ALGORITHM : Variable to select reduce algorithmauto                  - Internal algorithm selectionbinomial              - Force binomial algorithmnb                    - Force nonblocking algorithmreduce_scatter_gather - Force reduce scatter gather algorithm
20211015 115039.978 INFO             PET2 index= 157                            MPIR_CVAR_REDUCE_INTER_ALGORITHM : Variable to select reduce algorithmauto                     - Internal algorithm selectionlocal_reduce_remote_send - Force local-reduce-remote-send algorithmnb                       - Force nonblocking algorithm
20211015 115039.978 INFO             PET2 index= 158                          MPIR_CVAR_REDUCE_DEVICE_COLLECTIVE : If set to true, MPI_Reduce will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level reduce function will not be called.
20211015 115039.978 INFO             PET2 index= 159          MPIR_CVAR_REDUCE_SCATTER_COMMUTATIVE_LONG_MSG_SIZE : the long message algorithm will be used if the operation is commutative and the send buffer size is >= this value (in bytes)
20211015 115039.978 INFO             PET2 index= 160                    MPIR_CVAR_REDUCE_SCATTER_INTRA_ALGORITHM : Variable to select reduce_scatter algorithmauto               - Internal algorithm selectionnb                 - Force nonblocking algorithmnoncommutative     - Force noncommutative algorithmpairwise           - Force pairwise algorithmrecursive_doubling - Force recursive doubling algorithmrecursive_halving  - Force recursive halving algorithm
20211015 115039.978 INFO             PET2 index= 161                    MPIR_CVAR_REDUCE_SCATTER_INTER_ALGORITHM : Variable to select reduce_scatter algorithmauto                        - Internal algorithm selectionnb                          - Force nonblocking algorithmremote_reduce_local_scatter - Force remote-reduce-local-scatter algorithm
20211015 115039.978 INFO             PET2 index= 162                  MPIR_CVAR_REDUCE_SCATTER_DEVICE_COLLECTIVE : If set to true, MPI_Redscat will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level reduce_scatter function will not be called.
20211015 115039.978 INFO             PET2 index= 163              MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM : Variable to select reduce_scatter_block algorithmauto               - Internal algorithm selectionnoncommutative     - Force noncommutative algorithmrecursive_doubling - Force recursive doubling algorithmpairwise           - Force pairwise algorithmrecursive_halving  - Force recursive halving algorithmnb                 - Force nonblocking algorithm
20211015 115039.978 INFO             PET2 index= 164              MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTER_ALGORITHM : Variable to select reduce_scatter_block algorithmauto                        - Internal algorithm selectionnb                          - Force nonblocking algorithmremote_reduce_local_scatter - Force remote-reduce-local-scatter algorithm
20211015 115039.978 INFO             PET2 index= 165            MPIR_CVAR_REDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE : If set to true, MPI_Reduce_scatter_block will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level reduce_scatter_block function will not be called.
20211015 115039.978 INFO             PET2 index= 166                              MPIR_CVAR_SCAN_INTRA_ALGORITHM : Variable to select allgather algorithmauto               - Internal algorithm selectionnb                 - Force nonblocking algorithmrecursive_doubling - Force recursive doubling algorithm
20211015 115039.978 INFO             PET2 index= 167                            MPIR_CVAR_SCAN_DEVICE_COLLECTIVE : If set to true, MPI_Scan will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level scan function will not be called.
20211015 115039.978 INFO             PET2 index= 168                      MPIR_CVAR_SCATTER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Scatter if the send buffer size is < this value (in bytes)
20211015 115039.978 INFO             PET2 index= 169                           MPIR_CVAR_SCATTER_INTRA_ALGORITHM : Variable to select scatter algorithmauto     - Internal algorithm selectionbinomial - Force binomial algorithmnb       - Force nonblocking algorithm
20211015 115039.978 INFO             PET2 index= 170                           MPIR_CVAR_SCATTER_INTER_ALGORITHM : Variable to select scatter algorithmauto                      - Internal algorithm selectionlinear                    - Force linear algorithmnb                        - Force nonblocking algorithmremote_send_local_scatter - Force remote-send-local-scatter algorithm
20211015 115039.978 INFO             PET2 index= 171                         MPIR_CVAR_SCATTER_DEVICE_COLLECTIVE : If set to true, MPI_Scatter will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level scatter function will not be called.
20211015 115039.978 INFO             PET2 index= 172                          MPIR_CVAR_SCATTERV_INTRA_ALGORITHM : Variable to select scatterv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithmnb     - Force nonblocking algorithm
20211015 115039.978 INFO             PET2 index= 173                          MPIR_CVAR_SCATTERV_INTER_ALGORITHM : Variable to select scatterv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithmnb     - Force nonblocking algorithm
20211015 115039.978 INFO             PET2 index= 174                        MPIR_CVAR_SCATTERV_DEVICE_COLLECTIVE : If set to true, MPI_scatterv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level scatterv function will not be called.
20211015 115039.978 INFO             PET2 index= 175                                MPIR_CVAR_DEVICE_COLLECTIVES : If set to true, MPI collectives will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually.  If set to false, the device-level collective function will not be called.
20211015 115039.978 INFO             PET2 index= 176                                MPIR_CVAR_PROGRESS_MAX_COLLS : Maximum number of collective operations at a time that the progress engine should make progress on
20211015 115039.978 INFO             PET2 index= 177                              MPIR_CVAR_COMM_SPLIT_USE_QSORT : Use qsort(3) in the implementation of MPI_Comm_split instead of bubble sort.
20211015 115039.978 INFO             PET2 index= 178                                  MPIR_CVAR_CTXID_EAGER_SIZE : The MPIR_CVAR_CTXID_EAGER_SIZE environment variable allows you to specify how many words in the context ID mask will be set aside for the eager allocation protocol.  If the application is running out of context IDs, reducing this value may help.
20211015 115039.978 INFO             PET2 index= 179                                    MPIR_CVAR_PROCTABLE_SIZE : Size of the "MPIR" debugger interface proctable (process table).
20211015 115039.978 INFO             PET2 index= 180                                   MPIR_CVAR_PROCTABLE_PRINT : If true, dump the proctable entries at MPII_Wait_for_debugger-time.
20211015 115039.978 INFO             PET2 index= 181                                 MPIR_CVAR_PRINT_ERROR_STACK : If true, print an error stack trace at error handling time.
20211015 115039.978 INFO             PET2 index= 182                                  MPIR_CVAR_CHOP_ERROR_STACK : If >0, truncate error stack output lines this many characters wide.  If 0, do not truncate, and if <0 use a sensible default.
20211015 115039.978 INFO             PET2 index= 183                            MPIR_CVAR_SUPPRESS_ABORT_MESSAGE : Disable printing of abort error message.
20211015 115039.978 INFO             PET2 index= 184                                           MPIR_CVAR_MEMDUMP : If true, list any memory that was allocated by MPICH and that remains allocated when MPI_Finalize completes.
20211015 115039.978 INFO             PET2 index= 185                          MPIR_CVAR_MEM_CATEGORY_INFORMATION : If true, print a summary of memory allocation by category. The category definitions are found in mpl_trmem.h.
20211015 115039.978 INFO             PET2 index= 186                                    MPIR_CVAR_ASYNC_PROGRESS : If set to true, MPICH will initiate an additional thread to make asynchronous progress on all communication operations including point-to-point, collective, one-sided operations and I/O.  Setting this variable will automatically increase the thread-safety level to MPI_THREAD_MULTIPLE.  While this improves the progress semantics, it might cause a small amount of performance overhead for regular MPI operations.  The user is encouraged to leave one or more hardware threads vacant in order to prevent contention between the application threads and the progress thread(s).  The impact of oversubscription is highly system dependent but may be substantial in some cases, hence this recommendation.
20211015 115039.978 INFO             PET2 index= 187                              MPIR_CVAR_DEFAULT_THREAD_LEVEL : Sets the default thread level to use when using MPI_INIT. This variable is case-insensitive.
20211015 115039.978 INFO             PET2 index= 188                                        MPIR_CVAR_DEBUG_HOLD : If true, causes processes to wait in MPI_Init and MPI_Initthread for a debugger to be attached.  Once the debugger has attached, the variable 'hold' should be set to 0 in order to allow the process to continue (e.g., in gdb, "set hold=0").
20211015 115039.978 INFO             PET2 index= 189                                    MPIR_CVAR_ERROR_CHECKING : If true, perform checks for errors, typically to verify valid inputs to MPI routines.  Only effective when MPICH is configured with --enable-error-checking=runtime .
20211015 115039.978 INFO             PET2 index= 190                                  MPIR_CVAR_NETLOC_NODE_FILE : Subnet json file
20211015 115039.978 INFO             PET2 index= 191                                      MPIR_CVAR_DIMS_VERBOSE : If true, enable verbose output about the actions of the implementation of MPI_Dims_create.
20211015 115039.978 INFO             PET2 index= 192                              MPIR_CVAR_NAMESERV_FILE_PUBDIR : Sets the directory to use for MPI service publishing in the file nameserv implementation.  Allows the user to override where the publish and lookup information is placed for connect/accept based applications.
20211015 115039.978 INFO             PET2 index= 193                           MPIR_CVAR_ABORT_ON_LEAKED_HANDLES : If true, MPI will call MPI_Abort at MPI_Finalize if any MPI object handles have been leaked.  For example, if MPI_Comm_dup is called without calling a corresponding MPI_Comm_free.  For uninteresting reasons, enabling this option may prevent all known object leaks from being reported.  MPICH must have been configure with "--enable-g=handlealloc" or better in order for this functionality to work.
20211015 115039.978 INFO             PET2 index= 194                                           MPIR_CVAR_NOLOCAL : If true, force all processes to operate as though all processes are located on another node.  For example, this disables shared memory communication hierarchical collectives.
20211015 115039.978 INFO             PET2 index= 195                                  MPIR_CVAR_ODD_EVEN_CLIQUES : If true, odd procs on a node are seen as local to each other, and even procs on a node are seen as local to each other.  Used for debugging on a single machine. Deprecated in favor of MPIR_CVAR_NUM_CLIQUES.
20211015 115039.978 INFO             PET2 index= 196                                       MPIR_CVAR_NUM_CLIQUES : Specify the number of cliques that should be used to partition procs on a local node. Procs with the same clique number are seen as local to each other. Used for debugging on a single machine.
20211015 115039.978 INFO             PET2 index= 197                                  MPIR_CVAR_COLL_ALIAS_CHECK : Enable checking of aliasing in collective operations
20211015 115039.978 INFO             PET2 index= 198                                 MPIR_CVAR_REQUEST_POLL_FREQ : How frequent to poll during completion calls (wait/test) in terms of number of processed requests before polling.
20211015 115039.978 INFO             PET2 index= 199                                MPIR_CVAR_REQUEST_BATCH_SIZE : The number of requests to make completion as a batch in MPI_Waitall and MPI_Testall implementation. A large number is likely to cause more cache misses.
20211015 115039.978 INFO             PET2 index= 200                                MPIR_CVAR_POLLS_BEFORE_YIELD : When MPICH is in a busy waiting loop, it will periodically call a function to yield the processor.  This cvar sets the number of loops before the yield function is called.  A value of 0 disables yielding.
20211015 115039.978 INFO             PET2 index= 201                          MPIR_CVAR_NEMESIS_MXM_BULK_CONNECT : If true, force mxm to connect all processes at initialization time.
20211015 115039.978 INFO             PET2 index= 202                       MPIR_CVAR_NEMESIS_MXM_BULK_DISCONNECT : If true, force mxm to disconnect all processes at finalization time.
20211015 115039.978 INFO             PET2 index= 203                              MPIR_CVAR_NEMESIS_MXM_HUGEPAGE : If true, mxm tries detecting hugepage support.  On HPC-X 2.3 and earlier, this might cause problems on Ubuntu and other platforms even if the system provides hugepage support.
20211015 115039.978 INFO             PET2 index= 204                                  MPIR_CVAR_OFI_USE_PROVIDER : If non-null, choose an OFI provider by name. If using with the CH4 device and using a provider that supports an older version of the libfabric API then the default version of the installed library, specifying the OFI version via the appropriate CVARs is also recommended.
20211015 115039.978 INFO             PET2 index= 205                                MPIR_CVAR_OFI_DUMP_PROVIDERS : If true, dump provider information at init
20211015 115039.978 INFO             PET2 index= 206                            MPIR_CVAR_CH3_INTERFACE_HOSTNAME : If non-NULL, this cvar specifies the IP address that other processes should use when connecting to this process. This cvar is mutually exclusive with the MPIR_CVAR_CH3_NETWORK_IFACE cvar and it is an error to set them both.
20211015 115039.978 INFO             PET2 index= 207                                    MPIR_CVAR_CH3_PORT_RANGE : The MPIR_CVAR_CH3_PORT_RANGE environment variable allows you to specify the range of TCP ports to be used by the process manager and the MPICH library. The format of this variable is <low>:<high>.  To specify any available port, use 0:0.
20211015 115039.978 INFO             PET2 index= 208                         MPIR_CVAR_NEMESIS_TCP_NETWORK_IFACE : If non-NULL, this cvar specifies which pseudo-ethernet interface the tcp netmod should use (e.g., "eth1", "ib0"). Note, this is a Linux-specific cvar. This cvar is mutually exclusive with the MPIR_CVAR_CH3_INTERFACE_HOSTNAME cvar and it is an error to set them both.
20211015 115039.978 INFO             PET2 index= 209                   MPIR_CVAR_NEMESIS_TCP_HOST_LOOKUP_RETRIES : This cvar controls the number of times to retry the gethostbyname() function before giving up.
20211015 115039.978 INFO             PET2 index= 210                            MPIR_CVAR_NEMESIS_ENABLE_CKPOINT : If true, enables checkpointing support and returns an error if checkpointing library cannot be initialized.
20211015 115039.978 INFO             PET2 index= 211                          MPIR_CVAR_NEMESIS_SHM_EAGER_MAX_SZ : This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for shared memory. If this cvar is set to -1, then Nemesis will choose an appropriate value.
20211015 115039.978 INFO             PET2 index= 212                    MPIR_CVAR_NEMESIS_SHM_READY_EAGER_MAX_SZ : This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for ready-send messages.  If this cvar is set to -1, then ready messages will always be sent eagerly.  If this cvar is set to -2, then Nemesis will choose an appropriate value.
20211015 115039.978 INFO             PET2 index= 213                                         MPIR_CVAR_ENABLE_FT : Enable fault tolerance functions
20211015 115039.978 INFO             PET2 index= 214                         MPIR_CVAR_NEMESIS_LMT_DMA_THRESHOLD : Messages larger than this size will use the "dma" (knem) intranode LMT implementation, if it is enabled and available.
20211015 115039.978 INFO             PET2 index= 215                                    MPIR_CVAR_NEMESIS_NETMOD : If non-empty, this cvar specifies which network module should be used for communication. This variable is case-insensitive.
20211015 115039.978 INFO             PET2 index= 216                                  MPIR_CVAR_CH3_ENABLE_HCOLL : If true, enable HCOLL collectives.
20211015 115039.978 INFO             PET2 index= 217                          MPIR_CVAR_CH3_COMM_CONNECT_TIMEOUT : The default time out period in seconds for a connection attempt to the server communicator where the named port exists but no pending accept. User can change the value for a specified connection through its info argument.
20211015 115039.979 INFO             PET2 index= 218               MPIR_CVAR_CH3_RMA_OP_PIGGYBACK_LOCK_DATA_SIZE : Specify the threshold of data size of a RMA operation which can be piggybacked with a LOCK message. It is always a positive value and should not be smaller than MPIDI_RMA_IMMED_BYTES. If user sets it as a small value, for middle and large data size, we will lose performance because of always waiting for round-trip of LOCK synchronization; if user sets it as a large value, we need to consume more memory on target side to buffer this lock request when lock is not satisfied.
20211015 115039.979 INFO             PET2 index= 219                      MPIR_CVAR_CH3_RMA_ACTIVE_REQ_THRESHOLD : Threshold of number of active requests to trigger blocking waiting in operation routines. When the value is negative, we never blockingly wait in operation routines. When the value is zero, we always trigger blocking waiting in operation routines to wait until no. of active requests becomes zero. When the value is positive, we do blocking waiting in operation routines to wait until no. of active requests being reduced to this value.
20211015 115039.979 INFO             PET2 index= 220               MPIR_CVAR_CH3_RMA_POKE_PROGRESS_REQ_THRESHOLD : Threshold at which the RMA implementation attempts to complete requests while completing RMA operations and while using the lazy synchonization approach.  Change this value if programs fail because they run out of requests or other internal resources
20211015 115039.979 INFO             PET2 index= 221                MPIR_CVAR_CH3_RMA_SCALABLE_FENCE_PROCESS_NUM : Specify the threshold of switching the algorithm used in FENCE from the basic algorithm to the scalable algorithm. The value can be nagative, zero or positive. When the number of processes is larger than or equal to this value, FENCE will use a scalable algorithm which do not use O(P) data structure; when the number of processes is smaller than the value, FENCE will use a basic but fast algorithm which requires an O(P) data structure.
20211015 115039.979 INFO             PET2 index= 222            MPIR_CVAR_CH3_RMA_DELAY_ISSUING_FOR_PIGGYBACKING : Specify if delay issuing of RMA operations for piggybacking LOCK/UNLOCK/FLUSH is enabled. It can be either 0 or 1. When it is set to 1, the issuing of LOCK message is delayed until origin process see the first RMA operation and piggyback LOCK with that operation, and the origin process always keeps the current last operation until the ending synchronization call in order to piggyback UNLOCK/FLUSH with that operation. When it is set to 0, in WIN_LOCK/UNLOCK case, the LOCK message is sent out as early as possible, in WIN_LOCK_ALL/UNLOCK_ALL case, the origin process still tries to piggyback LOCK message with the first operation; for UNLOCK/FLUSH message, the origin process no longer keeps the current last operation but only piggyback UNLOCK/FLUSH if there is an operation avaliable in the ending synchronization call.
20211015 115039.979 INFO             PET2 index= 223                                MPIR_CVAR_CH3_RMA_SLOTS_SIZE : Number of RMA slots during window creation. Each slot contains a linked list of target elements. The distribution of ranks among slots follows a round-robin pattern. Requires a positive value.
20211015 115039.979 INFO             PET2 index= 224                    MPIR_CVAR_CH3_RMA_TARGET_LOCK_DATA_BYTES : Size (in bytes) of available lock data this window can provided. If current buffered lock data is more than this value, the process will drop the upcoming operation data. Requires a positive calue.
20211015 115039.979 INFO             PET2 index= 225                            MPIR_CVAR_CH3_EAGER_MAX_MSG_SIZE : This cvar controls the message size at which CH3 switches from eager to rendezvous mode.
20211015 115039.979 INFO             PET2 index= 226                          MPIR_CVAR_CH3_RMA_OP_WIN_POOL_SIZE : Size of the window-private RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediately.  Requires a positive value.
20211015 115039.979 INFO             PET2 index= 227                       MPIR_CVAR_CH3_RMA_OP_GLOBAL_POOL_SIZE : Size of the Global RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediatly.  Requires a positive value.
20211015 115039.979 INFO             PET2 index= 228                      MPIR_CVAR_CH3_RMA_TARGET_WIN_POOL_SIZE : Size of the window-private RMA target pool (in number of targets) that stores information about RMA targets that could not be issued immediately.  Requires a positive value.
20211015 115039.979 INFO             PET2 index= 229                   MPIR_CVAR_CH3_RMA_TARGET_GLOBAL_POOL_SIZE : Size of the Global RMA targets pool (in number of targets) that stores information about RMA targets that could not be issued immediatly.  Requires a positive value.
20211015 115039.979 INFO             PET2 index= 230           MPIR_CVAR_CH3_RMA_TARGET_LOCK_ENTRY_WIN_POOL_SIZE : Size of the window-private RMA lock entries pool (in number of lock entries) that stores information about RMA lock requests that could not be satisfied immediatly.  Requires a positive value.
20211015 115039.979 INFO             PET2 index= 231                     MPIR_CVAR_CH4_OFI_CAPABILITY_SETS_DEBUG : Prints out the configuration of each capability selected via the capability sets interface.
20211015 115039.979 INFO             PET2 index= 232                               MPIR_CVAR_CH4_OFI_ENABLE_DATA : Enable immediate data fields in OFI to transmit source rank outside of the match bits
20211015 115039.979 INFO             PET2 index= 233                           MPIR_CVAR_CH4_OFI_ENABLE_AV_TABLE : If true, the OFI addressing information will be stored with an FI_AV_TABLE. If false, an FI_AV_MAP will be used.
20211015 115039.979 INFO             PET2 index= 234                 MPIR_CVAR_CH4_OFI_ENABLE_SCALABLE_ENDPOINTS : If true, use OFI scalable endpoints.
20211015 115039.979 INFO             PET2 index= 235                    MPIR_CVAR_CH4_OFI_ENABLE_SHARED_CONTEXTS : If set to false (zero), MPICH does not use OFI shared contexts. If set to -1, it is determined by the OFI capability sets based on the provider. Otherwise, MPICH tries to use OFI shared contexts. If they are unavailable, it'll fall back to the mode without shared contexts.
20211015 115039.979 INFO             PET2 index= 236                        MPIR_CVAR_CH4_OFI_ENABLE_MR_SCALABLE : If true, MR_SCALABLE for OFI memory regions. If false, MR_BASIC for OFI memory regions.
20211015 115039.979 INFO             PET2 index= 237                             MPIR_CVAR_CH4_OFI_ENABLE_TAGGED : If true, use tagged message transmission functions in OFI.
20211015 115039.979 INFO             PET2 index= 238                                 MPIR_CVAR_CH4_OFI_ENABLE_AM : If true, enable OFI active message support.
20211015 115039.979 INFO             PET2 index= 239                                MPIR_CVAR_CH4_OFI_ENABLE_RMA : If true, enable OFI RMA support.
20211015 115039.979 INFO             PET2 index= 240                            MPIR_CVAR_CH4_OFI_ENABLE_ATOMICS : If true, enable OFI Atomics support.
20211015 115039.979 INFO             PET2 index= 241                       MPIR_CVAR_CH4_OFI_FETCH_ATOMIC_IOVECS : Specifies the maximum number of iovecs that can be used by the OFI provider for fetch_atomic operations. The default value is -1, indicating that no value is set.
20211015 115039.979 INFO             PET2 index= 242                 MPIR_CVAR_CH4_OFI_ENABLE_DATA_AUTO_PROGRESS : If true, enable MPI data auto progress.
20211015 115039.979 INFO             PET2 index= 243              MPIR_CVAR_CH4_OFI_ENABLE_CONTROL_AUTO_PROGRESS : If true, enable MPI control auto progress.
20211015 115039.979 INFO             PET2 index= 244                       MPIR_CVAR_CH4_OFI_ENABLE_PT2PT_NOPACK : If true, enable iovec for pt2pt.
20211015 115039.979 INFO             PET2 index= 245                           MPIR_CVAR_CH4_OFI_CONTEXT_ID_BITS : Specifies the number of bits that will be used for matching the context ID. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20211015 115039.979 INFO             PET2 index= 246                                 MPIR_CVAR_CH4_OFI_RANK_BITS : Specifies the number of bits that will be used for matching the MPI rank. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20211015 115039.979 INFO             PET2 index= 247                                  MPIR_CVAR_CH4_OFI_TAG_BITS : Specifies the number of bits that will be used for matching the user tag. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20211015 115039.979 INFO             PET2 index= 248                             MPIR_CVAR_CH4_OFI_MAJOR_VERSION : Specifies the major version of the OFI library. The default is the major version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
20211015 115039.979 INFO             PET2 index= 249                             MPIR_CVAR_CH4_OFI_MINOR_VERSION : Specifies the major version of the OFI library. The default is the minor version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
20211015 115039.979 INFO             PET2 index= 250                                  MPIR_CVAR_CH4_OFI_MAX_VNIS : If set to positive, this CVAR specifies the maximum number of CH4 VNIs that OFI netmod exposes.
20211015 115039.979 INFO             PET2 index= 251                           MPIR_CVAR_CH4_OFI_MAX_RMA_SEP_CTX : If set to positive, this CVAR specifies the maximum number of transmit contexts RMA can utilize in a scalable endpoint. This value is effective only when scalable endpoint is available, otherwise it will be ignored.
20211015 115039.979 INFO             PET2 index= 252                          MPIR_CVAR_CH4_OFI_MAX_EAGAIN_RETRY : If set to positive, this CVAR specifies the maximum number of retries of an ofi operations before returning MPIX_ERR_EAGAIN. This value is effective only when the communicator has the MPI_OFI_set_eagain info hint set to true.
20211015 115039.979 INFO             PET2 index= 253                            MPIR_CVAR_CH4_OFI_NUM_AM_BUFFERS : Specifies the number of buffers for receiving active messages.
20211015 115039.979 INFO             PET2 index= 254                                        MPIR_CVAR_CH4_NETMOD : If non-empty, this cvar specifies which network module to use
20211015 115039.979 INFO             PET2 index= 255                                           MPIR_CVAR_CH4_SHM : If non-empty, this cvar specifies which shm module to use
20211015 115039.979 INFO             PET2 index= 256                                MPIR_CVAR_CH4_ROOTS_ONLY_PMI : Enables an optimized business card exchange over PMI for node root processes only.
20211015 115039.979 INFO             PET2 index= 257                            MPIR_CVAR_CH4_RUNTIME_CONF_DEBUG : If enabled, CH4-level runtime configurations are printed out
20211015 115039.979 INFO             PET2 index= 258                                      MPIR_CVAR_CH4_MT_MODEL : Specifies the CH4 multi-threading model. Possible values are: direct (default) handoff trylock
20211015 115039.979 INFO             PET2 index= 259                          MPIR_CVAR_CH4_COMM_CONNECT_TIMEOUT : The default time out period in seconds for a connection attempt to the server communicator where the named port exists but no pending accept. User can change the value for a specified connection through its info argument.
20211015 115039.979 INFO             PET2 index= 260                             MPIR_CVAR_CH4_RANDOM_ADDR_RETRY : The default number of retries for generating a random address. A retrying involves only local operations.
20211015 115039.979 INFO             PET2 index= 261                             MPIR_CVAR_CH4_SHM_SYMHEAP_RETRY : The default number of retries for allocating a symmetric heap in shared memory. A retrying involves collective communication over the group in the shared memory.
20211015 115039.979 INFO             PET2 index= 262                             MPIR_CVAR_CH4_RMA_MEM_EFFICIENT : If true, memory-saving mode is on, per-target object is released at the epoch end call. If false, performance-efficient mode is on, all allocated target objects are cached and freed at win_finalize.
20211015 115039.979 INFO             PET2 index= 263                                      MPIR_CVAR_ENABLE_HCOLL : Enable hcoll collective support.
20211015 115039.979 INFO             PET2 index= 264                                   MPIR_CVAR_COLL_SCHED_DUMP : Print schedule data for nonblocking collective operations.
20211015 115039.979 INFO             PET2 --- VMK::logSystem() end ---------------------------------
20211015 115039.979 INFO             PET2 main: --- VMK::log() start -------------------------------------
20211015 115039.979 INFO             PET2 main: vm located at: 0x7ff34f40cf90
20211015 115039.979 INFO             PET2 main: petCount=6 localPet=2 mypthid=0 currentSsiPe=-1
20211015 115039.979 INFO             PET2 main: Current system level OMP_NUM_THREADS setting for local PET: 12
20211015 115039.979 INFO             PET2 main: ssiCount=1 localSsi=0
20211015 115039.979 INFO             PET2 main: mpionly=1 threadsflag=0
20211015 115039.979 INFO             PET2 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20211015 115039.979 INFO             PET2 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20211015 115039.979 INFO             PET2 main:  PE=0 SSI=0 SSIPE=0
20211015 115039.979 INFO             PET2 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20211015 115039.979 INFO             PET2 main:  PE=1 SSI=0 SSIPE=1
20211015 115039.979 INFO             PET2 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20211015 115039.979 INFO             PET2 main:  PE=2 SSI=0 SSIPE=2
20211015 115039.979 INFO             PET2 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20211015 115039.979 INFO             PET2 main:  PE=3 SSI=0 SSIPE=3
20211015 115039.979 INFO             PET2 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20211015 115039.979 INFO             PET2 main:  PE=4 SSI=0 SSIPE=4
20211015 115039.979 INFO             PET2 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20211015 115039.979 INFO             PET2 main:  PE=5 SSI=0 SSIPE=5
20211015 115039.979 INFO             PET2 main: --- VMK::log() end ---------------------------------------
20211015 115039.980 INFO             PET2 Executing 'userm1_setvm'
20211015 115039.980 INFO             PET2 Executing 'userm1_register'
20211015 115039.980 INFO             PET2 Executing 'userm2_setvm'
20211015 115039.980 INFO             PET2 Executing 'userm2_register'
20211015 115039.980 INFO             PET2 model1: --- VMK::log() start -------------------------------------
20211015 115039.980 INFO             PET2 model1: vm located at: 0x7ff34f603480
20211015 115039.980 INFO             PET2 model1: petCount=6 localPet=2 mypthid=0 currentSsiPe=-1
20211015 115039.980 INFO             PET2 model1: Current system level OMP_NUM_THREADS setting for local PET: 12
20211015 115039.980 INFO             PET2 model1: ssiCount=1 localSsi=0
20211015 115039.980 INFO             PET2 model1: mpionly=1 threadsflag=0
20211015 115039.980 INFO             PET2 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20211015 115039.980 INFO             PET2 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20211015 115039.980 INFO             PET2 model1:  PE=0 SSI=0 SSIPE=0
20211015 115039.980 INFO             PET2 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20211015 115039.980 INFO             PET2 model1:  PE=1 SSI=0 SSIPE=1
20211015 115039.980 INFO             PET2 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20211015 115039.980 INFO             PET2 model1:  PE=2 SSI=0 SSIPE=2
20211015 115039.980 INFO             PET2 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20211015 115039.980 INFO             PET2 model1:  PE=3 SSI=0 SSIPE=3
20211015 115039.980 INFO             PET2 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20211015 115039.980 INFO             PET2 model1:  PE=4 SSI=0 SSIPE=4
20211015 115039.980 INFO             PET2 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20211015 115039.980 INFO             PET2 model1:  PE=5 SSI=0 SSIPE=5
20211015 115039.980 INFO             PET2 model1: --- VMK::log() end ---------------------------------------
20211015 115039.985 INFO             PET2 Entering 'user1_run'
20211015 115039.985 INFO             PET2  user1_run: on SSIPE:           -1  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20211015 115040.390 INFO             PET2  user1_run: on SSIPE:           -1  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20211015 115040.638 INFO             PET2  user1_run: on SSIPE:           -1  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20211015 115040.902 INFO             PET2  user1_run: on SSIPE:           -1  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20211015 115041.151 INFO             PET2  user1_run: on SSIPE:           -1  Filling data lbound:        3335           1           1  ubound:        5001        1200          10
20211015 115041.407 INFO             PET2 Exiting 'user1_run'
20211015 115041.412 INFO             PET2 Entering 'user2_run'
20211015 115041.412 INFO             PET2 model2: --- VMK::log() start -------------------------------------
20211015 115041.412 INFO             PET2 model2: vm located at: 0x7ff34f603c20
20211015 115041.412 INFO             PET2 model2: petCount=6 localPet=2 mypthid=0 currentSsiPe=-1
20211015 115041.412 INFO             PET2 model2: Current system level OMP_NUM_THREADS setting for local PET: 12
20211015 115041.412 INFO             PET2 model2: ssiCount=1 localSsi=0
20211015 115041.412 INFO             PET2 model2: mpionly=1 threadsflag=0
20211015 115041.412 INFO             PET2 model2: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20211015 115041.412 INFO             PET2 model2: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20211015 115041.412 INFO             PET2 model2:  PE=0 SSI=0 SSIPE=0
20211015 115041.412 INFO             PET2 model2: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20211015 115041.412 INFO             PET2 model2:  PE=1 SSI=0 SSIPE=1
20211015 115041.412 INFO             PET2 model2: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20211015 115041.412 INFO             PET2 model2:  PE=2 SSI=0 SSIPE=2
20211015 115041.412 INFO             PET2 model2: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20211015 115041.412 INFO             PET2 model2:  PE=3 SSI=0 SSIPE=3
20211015 115041.412 INFO             PET2 model2: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20211015 115041.412 INFO             PET2 model2:  PE=4 SSI=0 SSIPE=4
20211015 115041.412 INFO             PET2 model2: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20211015 115041.412 INFO             PET2 model2:  PE=5 SSI=0 SSIPE=5
20211015 115041.412 INFO             PET2 model2: --- VMK::log() end ---------------------------------------
20211015 115041.412 INFO             PET2  user2_run: ssiLocalDeCount=           6
20211015 115041.413 INFO             PET2  user2_run: OpenMP thread:           2  on SSIPE:           -1  Testing data for localDe =           2  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20211015 115041.413 INFO             PET2  user2_run: OpenMP thread:           4  on SSIPE:           -1  Testing data for localDe =           4  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20211015 115041.423 INFO             PET2  user2_run: OpenMP thread:           5  on SSIPE:           -1  Testing data for localDe =           5  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20211015 115041.433 INFO             PET2  user2_run: OpenMP thread:           1  on SSIPE:           -1  Testing data for localDe =           1  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20211015 115041.453 INFO             PET2  user2_run: OpenMP thread:           0  on SSIPE:           -1  Testing data for localDe =           0  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20211015 115041.473 INFO             PET2  user2_run: OpenMP thread:           3  on SSIPE:           -1  Testing data for localDe =           3  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20211015 115042.904 INFO             PET2  user2_run: OpenMP thread:           0  on SSIPE:           -1  Testing data for localDe =           0  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20211015 115042.905 INFO             PET2  user2_run: OpenMP thread:           4  on SSIPE:           -1  Testing data for localDe =           4  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20211015 115042.906 INFO             PET2  user2_run: OpenMP thread:           2  on SSIPE:           -1  Testing data for localDe =           2  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20211015 115042.908 INFO             PET2  user2_run: OpenMP thread:           5  on SSIPE:           -1  Testing data for localDe =           5  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20211015 115042.915 INFO             PET2  user2_run: OpenMP thread:           1  on SSIPE:           -1  Testing data for localDe =           1  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20211015 115042.921 INFO             PET2  user2_run: OpenMP thread:           3  on SSIPE:           -1  Testing data for localDe =           3  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20211015 115044.249 INFO             PET2  user2_run: OpenMP thread:           0  on SSIPE:           -1  Testing data for localDe =           0  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20211015 115044.251 INFO             PET2  user2_run: OpenMP thread:           4  on SSIPE:           -1  Testing data for localDe =           4  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20211015 115044.251 INFO             PET2  user2_run: OpenMP thread:           2  on SSIPE:           -1  Testing data for localDe =           2  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20211015 115044.252 INFO             PET2  user2_run: OpenMP thread:           5  on SSIPE:           -1  Testing data for localDe =           5  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20211015 115044.252 INFO             PET2  user2_run: OpenMP thread:           1  on SSIPE:           -1  Testing data for localDe =           1  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20211015 115044.257 INFO             PET2  user2_run: OpenMP thread:           3  on SSIPE:           -1  Testing data for localDe =           3  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20211015 115045.582 INFO             PET2  user2_run: OpenMP thread:           0  on SSIPE:           -1  Testing data for localDe =           0  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20211015 115045.582 INFO             PET2  user2_run: OpenMP thread:           5  on SSIPE:           -1  Testing data for localDe =           5  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20211015 115045.582 INFO             PET2  user2_run: OpenMP thread:           4  on SSIPE:           -1  Testing data for localDe =           4  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20211015 115045.583 INFO             PET2  user2_run: OpenMP thread:           2  on SSIPE:           -1  Testing data for localDe =           2  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20211015 115045.583 INFO             PET2  user2_run: OpenMP thread:           1  on SSIPE:           -1  Testing data for localDe =           1  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20211015 115045.592 INFO             PET2  user2_run: OpenMP thread:           3  on SSIPE:           -1  Testing data for localDe =           3  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20211015 115046.914 INFO             PET2  user2_run: OpenMP thread:           4  on SSIPE:           -1  Testing data for localDe =           4  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20211015 115046.914 INFO             PET2  user2_run: OpenMP thread:           0  on SSIPE:           -1  Testing data for localDe =           0  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20211015 115046.915 INFO             PET2  user2_run: OpenMP thread:           2  on SSIPE:           -1  Testing data for localDe =           2  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20211015 115046.916 INFO             PET2  user2_run: OpenMP thread:           5  on SSIPE:           -1  Testing data for localDe =           5  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20211015 115046.916 INFO             PET2  user2_run: OpenMP thread:           1  on SSIPE:           -1  Testing data for localDe =           1  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20211015 115046.921 INFO             PET2  user2_run: OpenMP thread:           3  on SSIPE:           -1  Testing data for localDe =           3  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20211015 115048.258 INFO             PET2  user2_run: All data correct.
20211015 115048.258 INFO             PET2 Exiting 'user2_run'
20211015 115048.317 INFO             PET2  NUMBER_OF_PROCESSORS           6
20211015 115048.317 INFO             PET2  PASS  System Test ESMF_FieldSharedDeSSISTest, ESMF_FieldSharedDeSSISTest.F90, line 276
20211015 115048.317 INFO             PET2 Finalizing ESMF
20211015 115039.974 INFO             PET3 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20211015 115039.975 INFO             PET3 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20211015 115039.975 INFO             PET3 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20211015 115039.975 INFO             PET3 !!! FOR PRODUCTION RUNS, USE:                      !!!
20211015 115039.975 INFO             PET3 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20211015 115039.975 INFO             PET3 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20211015 115039.975 INFO             PET3 Running with ESMF Version   : ESMF_8_2_0_beta_snapshot_22-10-g6974ca47a6
20211015 115039.975 INFO             PET3 ESMF library build date/time: "Oct 15 2021" "11:35:17"
20211015 115039.975 INFO             PET3 ESMF library build location : /Volumes/esmf/rocky/esmf-testing/gfortran_9.3.0_mpich3_O_release_8.2.0
20211015 115039.976 INFO             PET3 ESMF_COMM                   : mpich3
20211015 115039.976 INFO             PET3 ESMF_MOAB                   : enabled
20211015 115039.976 INFO             PET3 ESMF_LAPACK                 : enabled
20211015 115039.976 INFO             PET3 ESMF_NETCDF                 : enabled
20211015 115039.976 INFO             PET3 ESMF_PNETCDF                : disabled
20211015 115039.976 INFO             PET3 ESMF_PIO                    : enabled
20211015 115039.976 INFO             PET3 ESMF_YAMLCPP                : enabled
20211015 115039.976 INFO             PET3 --- VMK::logSystem() start -------------------------------
20211015 115039.976 INFO             PET3 esmfComm=mpich3
20211015 115039.976 INFO             PET3 isPthreadsEnabled=0
20211015 115039.976 INFO             PET3 isOpenMPEnabled=1
20211015 115039.976 INFO             PET3 isOpenACCEnabled=0
20211015 115039.976 INFO             PET3 isSsiSharedMemoryEnabled=1
20211015 115039.976 INFO             PET3 ssiCount=1 peCount=6
20211015 115039.976 INFO             PET3 PE=0 SSI=0 SSIPE=0
20211015 115039.976 INFO             PET3 PE=1 SSI=0 SSIPE=1
20211015 115039.976 INFO             PET3 PE=2 SSI=0 SSIPE=2
20211015 115039.976 INFO             PET3 PE=3 SSI=0 SSIPE=3
20211015 115039.976 INFO             PET3 PE=4 SSI=0 SSIPE=4
20211015 115039.976 INFO             PET3 PE=5 SSI=0 SSIPE=5
20211015 115039.976 INFO             PET3 --- VMK::logSystem() MPI Control Variables ---------------
20211015 115039.976 INFO             PET3 index=   0                          MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the short message algorithm will be used if the send buffer size is < this value (in bytes). (See also: MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE)
20211015 115039.976 INFO             PET3 index=   1                           MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the long message algorithm will be used if the send buffer size is >= this value (in bytes) (See also: MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE)
20211015 115039.976 INFO             PET3 index=   2                         MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM : Variable to select allgather algorithmauto               - Internal algorithm selectionbrucks             - Force brucks algorithmnb                 - Force nonblocking algorithmrecursive_doubling - Force recursive doubling algorithmring               - Force ring algorithm
20211015 115039.976 INFO             PET3 index=   3                         MPIR_CVAR_ALLGATHER_INTER_ALGORITHM : Variable to select allgather algorithmauto                      - Internal algorithm selectionlocal_gather_remote_bcast - Force local-gather-remote-bcast algorithmnb                        - Force nonblocking algorithm
20211015 115039.976 INFO             PET3 index=   4                       MPIR_CVAR_ALLGATHER_DEVICE_COLLECTIVE : If set to true, MPI_Allgather will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level allgather function will not be called.
20211015 115039.976 INFO             PET3 index=   5                      MPIR_CVAR_ALLGATHERV_PIPELINE_MSG_SIZE : The smallest message size that will be used for the pipelined, large-message, ring algorithm in the MPI_Allgatherv implementation.
20211015 115039.976 INFO             PET3 index=   6                        MPIR_CVAR_ALLGATHERV_INTRA_ALGORITHM : Variable to select allgatherv algorithmauto               - Internal algorithm selectionbrucks             - Force brucks algorithmnb                 - Force nonblocking algorithmrecursive_doubling - Force recursive doubling algorithmring               - Force ring algorithm
20211015 115039.976 INFO             PET3 index=   7                        MPIR_CVAR_ALLGATHERV_INTER_ALGORITHM : Variable to select allgatherv algorithmauto                      - Internal algorithm selectionnb                        - Force nonblocking algorithmremote_gather_local_bcast - Force remote-gather-local-bcast algorithm
20211015 115039.976 INFO             PET3 index=   8                      MPIR_CVAR_ALLGATHERV_DEVICE_COLLECTIVE : If set to true, MPI_Allgatherv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level allgatherv function will not be called.
20211015 115039.976 INFO             PET3 index=   9                          MPIR_CVAR_ALLREDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20211015 115039.976 INFO             PET3 index=  10                            MPIR_CVAR_ENABLE_SMP_COLLECTIVES : Enable SMP aware collective communication.
20211015 115039.976 INFO             PET3 index=  11                              MPIR_CVAR_ENABLE_SMP_ALLREDUCE : Enable SMP aware allreduce.
20211015 115039.976 INFO             PET3 index=  12                        MPIR_CVAR_MAX_SMP_ALLREDUCE_MSG_SIZE : Maximum message size for which SMP-aware allreduce is used.  A value of '0' uses SMP-aware allreduce for all message sizes.
20211015 115039.976 INFO             PET3 index=  13                         MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM : Variable to select allreduce algorithmauto                     - Internal algorithm selectionnb                       - Force nonblocking algorithmrecursive_doubling       - Force recursive doubling algorithmreduce_scatter_allgather - Force reduce scatter allgather algorithm
20211015 115039.976 INFO             PET3 index=  14                         MPIR_CVAR_ALLREDUCE_INTER_ALGORITHM : Variable to select allreduce algorithmauto                  - Internal algorithm selectionnb                    - Force nonblocking algorithmreduce_exchange_bcast - Force reduce-exchange-bcast algorithm
20211015 115039.976 INFO             PET3 index=  15                       MPIR_CVAR_ALLREDUCE_DEVICE_COLLECTIVE : If set to true, MPI_Allreduce will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level allreduce function will not be called.
20211015 115039.976 INFO             PET3 index=  16                           MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE : the short message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value (See also: MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE)
20211015 115039.976 INFO             PET3 index=  17                          MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE : the medium message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value and larger than MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE (See also: MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE)
20211015 115039.976 INFO             PET3 index=  18                                 MPIR_CVAR_ALLTOALL_THROTTLE : max no. of irecvs/isends posted at a time in some alltoall algorithms. Setting it to 0 causes all irecvs/isends to be posted at once
20211015 115039.976 INFO             PET3 index=  19                          MPIR_CVAR_ALLTOALL_INTRA_ALGORITHM : Variable to select alltoall algorithmauto                      - Internal algorithm selectionbrucks                    - Force brucks algorithmnb                        - Force nonblocking algorithmpairwise                  - Force pairwise algorithmpairwise_sendrecv_replace - Force pairwise sendrecv replace algorithmscattered                 - Force scattered algorithm
20211015 115039.976 INFO             PET3 index=  20                          MPIR_CVAR_ALLTOALL_INTER_ALGORITHM : Variable to select alltoall algorithmauto              - Internal algorithm selectionnb                - Force nonblocking algorithmpairwise_exchange - Force pairwise exchange algorithm
20211015 115039.976 INFO             PET3 index=  21                        MPIR_CVAR_ALLTOALL_DEVICE_COLLECTIVE : If set to true, MPI_Alltoall will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level alltoall function will not be called.
20211015 115039.976 INFO             PET3 index=  22                         MPIR_CVAR_ALLTOALLV_INTRA_ALGORITHM : Variable to select alltoallv algorithmauto                      - Internal algorithm selectionnb                        - Force nonblocking algorithmpairwise_sendrecv_replace - Force pairwise_sendrecv_replace algorithmscattered                 - Force scattered algorithm
20211015 115039.976 INFO             PET3 index=  23                         MPIR_CVAR_ALLTOALLV_INTER_ALGORITHM : Variable to select alltoallv algorithmauto              - Internal algorithm selectionpairwise_exchange - Force pairwise exchange algorithmnb                - Force nonblocking algorithm
20211015 115039.976 INFO             PET3 index=  24                       MPIR_CVAR_ALLTOALLV_DEVICE_COLLECTIVE : If set to true, MPI_Alltoallv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level alltoallv function will not be called.
20211015 115039.976 INFO             PET3 index=  25                         MPIR_CVAR_ALLTOALLW_INTRA_ALGORITHM : Variable to select alltoallw algorithmauto                      - Internal algorithm selectionnb                        - Force nonblocking algorithmpairwise_sendrecv_replace - Force pairwise sendrecv replace algorithmscattered                 - Force scattered algorithm
20211015 115039.976 INFO             PET3 index=  26                         MPIR_CVAR_ALLTOALLW_INTER_ALGORITHM : Variable to select alltoallw algorithmauto              - Internal algorithm selectionnb                - Force nonblocking algorithmpairwise_exchange - Force pairwise exchange algorithm
20211015 115039.976 INFO             PET3 index=  27                       MPIR_CVAR_ALLTOALLW_DEVICE_COLLECTIVE : If set to true, MPI_Alltoallw will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level alltoallw function will not be called.
20211015 115039.976 INFO             PET3 index=  28                                MPIR_CVAR_ENABLE_SMP_BARRIER : Enable SMP aware barrier.
20211015 115039.976 INFO             PET3 index=  29                           MPIR_CVAR_BARRIER_INTRA_ALGORITHM : Variable to select barrier algorithmauto               - Internal algorithm selectionnb                 - Force nonblocking algorithmrecursive_doubling - Force recursive doubling algorithm
20211015 115039.976 INFO             PET3 index=  30                           MPIR_CVAR_BARRIER_INTER_ALGORITHM : Variable to select barrier algorithmauto  - Internal algorithm selectionbcast - Force bcast algorithmnb    - Force nonblocking algorithm
20211015 115039.976 INFO             PET3 index=  31                         MPIR_CVAR_BARRIER_DEVICE_COLLECTIVE : If set to true, MPI_Barrier will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level barrier function will not be called.
20211015 115039.976 INFO             PET3 index=  32                                   MPIR_CVAR_BCAST_MIN_PROCS : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_SHORT_MSG_SIZE, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20211015 115039.976 INFO             PET3 index=  33                              MPIR_CVAR_BCAST_SHORT_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20211015 115039.976 INFO             PET3 index=  34                               MPIR_CVAR_BCAST_LONG_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_SHORT_MSG_SIZE)
20211015 115039.976 INFO             PET3 index=  35                                  MPIR_CVAR_ENABLE_SMP_BCAST : Enable SMP aware broadcast (See also: MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE)
20211015 115039.977 INFO             PET3 index=  36                            MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE : Maximum message size for which SMP-aware broadcast is used.  A value of '0' uses SMP-aware broadcast for all message sizes. (See also: MPIR_CVAR_ENABLE_SMP_BCAST)
20211015 115039.977 INFO             PET3 index=  37                             MPIR_CVAR_BCAST_INTRA_ALGORITHM : Variable to select bcast algorithmauto                                    - Internal algorithm selectionbinomial                                - Force Binomial Treenb                                      - Force nonblocking algorithmscatter_recursive_doubling_allgather    - Force Scatter Recursive-Doubling Allgatherscatter_ring_allgather                  - Force Scatter Ring
20211015 115039.977 INFO             PET3 index=  38                             MPIR_CVAR_BCAST_INTER_ALGORITHM : Variable to select bcast algorithmauto                    - Internal algorithm selectionnb                      - Force nonblocking algorithmremote_send_local_bcast - Force remote-send-local-bcast algorithm
20211015 115039.977 INFO             PET3 index=  39                           MPIR_CVAR_BCAST_DEVICE_COLLECTIVE : If set to true, MPI_Bcast will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually.  If set to false, the device-level bcast function will not be called.
20211015 115039.977 INFO             PET3 index=  40                            MPIR_CVAR_EXSCAN_INTRA_ALGORITHM : Variable to select allgather algorithmauto               - Internal algorithm selectionnb                 - Force nonblocking algorithmrecursive_doubling - Force recursive doubling algorithm
20211015 115039.977 INFO             PET3 index=  41                          MPIR_CVAR_EXSCAN_DEVICE_COLLECTIVE : If set to true, MPI_Exscan will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level exscan function will not be called.
20211015 115039.977 INFO             PET3 index=  42                       MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_VSMALL_MSG_SIZE)
20211015 115039.977 INFO             PET3 index=  43                            MPIR_CVAR_GATHER_INTRA_ALGORITHM : Variable to select gather algorithmauto     - Internal algorithm selectionbinomial - Force binomial algorithmnb       - Force nonblocking algorithm
20211015 115039.977 INFO             PET3 index=  44                            MPIR_CVAR_GATHER_INTER_ALGORITHM : Variable to select gather algorithmauto                     - Internal algorithm selectionlinear                   - Force linear algorithmlocal_gather_remote_send - Force local-gather-remote-send algorithmnb                       - Force nonblocking algorithm
20211015 115039.977 INFO             PET3 index=  45                          MPIR_CVAR_GATHER_DEVICE_COLLECTIVE : If set to true, MPI_Gather will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level gather function will not be called.
20211015 115039.977 INFO             PET3 index=  46                            MPIR_CVAR_GATHER_VSMALL_MSG_SIZE : use a temporary buffer for intracommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE)
20211015 115039.977 INFO             PET3 index=  47                           MPIR_CVAR_GATHERV_INTRA_ALGORITHM : Variable to select gatherv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithmnb     - Force nonblocking algorithm
20211015 115039.977 INFO             PET3 index=  48                           MPIR_CVAR_GATHERV_INTER_ALGORITHM : Variable to select gatherv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithmnb     - Force nonblocking algorithm
20211015 115039.977 INFO             PET3 index=  49                         MPIR_CVAR_GATHERV_DEVICE_COLLECTIVE : If set to true, MPI_Gatherv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level gatherv function will not be called.
20211015 115039.977 INFO             PET3 index=  50                     MPIR_CVAR_GATHERV_INTER_SSEND_MIN_PROCS : Use Ssend (synchronous send) for intercommunicator MPI_Gatherv if the "group B" size is >= this value.  Specifying "-1" always avoids using Ssend.  For backwards compatibility, specifying "0" uses the default value.
20211015 115039.977 INFO             PET3 index=  51                           MPIR_CVAR_IALLGATHER_RECEXCH_KVAL : k value for recursive exchange based iallgather
20211015 115039.977 INFO             PET3 index=  52                            MPIR_CVAR_IALLGATHER_BRUCKS_KVAL : k value for radix in brucks based iallgather
20211015 115039.977 INFO             PET3 index=  53                        MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM : Variable to select iallgather algorithmauto               - Internal algorithm selectionbrucks             - Force brucks algorithmrecursive_doubling - Force recursive doubling algorithmring               - Force ring algorithmrecexch_distance_doubling    - Force generic transport recursive exchange with neighbours doubling in distance in each phaserecexch_distance_halving  - Force generic transport recursive exchange with neighbours halving in distance in each phasegentran_brucks     - Force generic transport based brucks algorithm
20211015 115039.977 INFO             PET3 index=  54                        MPIR_CVAR_IALLGATHER_INTER_ALGORITHM : Variable to select iallgather algorithmauto                      - Internal algorithm selectionlocal_gather_remote_bcast - Force local-gather-remote-bcast algorithm
20211015 115039.977 INFO             PET3 index=  55                      MPIR_CVAR_IALLGATHER_DEVICE_COLLECTIVE : If set to true, MPI_Iallgather will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iallgather function will not be called.
20211015 115039.977 INFO             PET3 index=  56                          MPIR_CVAR_IALLGATHERV_RECEXCH_KVAL : k value for recursive exchange based iallgatherv
20211015 115039.977 INFO             PET3 index=  57                       MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM : Variable to select iallgatherv algorithmauto               - Internal algorithm selectionbrucks             - Force brucks algorithmrecursive_doubling - Force recursive doubling algorithmring               - Force ring algorithmrecexch_distance_doubling    - Force generic transport recursive exchange with neighbours doubling in distance in each phaserecexch_distance_halving     - Force generic transport recursive exchange with neighbours halving in distance in each phasegentran_ring              - Force generic transport ring algorithm
20211015 115039.977 INFO             PET3 index=  58                       MPIR_CVAR_IALLGATHERV_INTER_ALGORITHM : Variable to select iallgatherv algorithmauto                      - Internal algorithm selectionremote_gather_local_bcast - Force remote-gather-local-bcast algorithm
20211015 115039.977 INFO             PET3 index=  59                     MPIR_CVAR_IALLGATHERV_DEVICE_COLLECTIVE : If set to true, MPI_Iallgatherv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iallgatherv function will not be called.
20211015 115039.977 INFO             PET3 index=  60                              MPIR_CVAR_IALLREDUCE_TREE_KVAL : k value for tree based iallreduce (for tree_kary and tree_knomial)
20211015 115039.977 INFO             PET3 index=  61               MPIR_CVAR_IALLREDUCE_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based iallreduce (tree_kary and tree_knomial). Default value is 0, that is, no pipelining by default
20211015 115039.977 INFO             PET3 index=  62                  MPIR_CVAR_IALLREDUCE_TREE_BUFFER_PER_CHILD : If set to true, a rank in tree_kary and tree_knomial algorithms will allocate a dedicated buffer for every child it receives data from. This would mean more memory consumption but it would allow preposting of the receives and hence reduce the number of unexpected messages. If set to false, there is only one buffer that is used to receive the data from all the children. The receives are therefore serialized, that is, only one receive can be posted at a time.
20211015 115039.977 INFO             PET3 index=  63                           MPIR_CVAR_IALLREDUCE_RECEXCH_KVAL : k value for recursive exchange based iallreduce
20211015 115039.977 INFO             PET3 index=  64                        MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM : Variable to select iallreduce algorithmauto                     - Internal algorithm selectionnaive                    - Force naive algorithmrecursive_doubling       - Force recursive doubling algorithmreduce_scatter_allgather - Force reduce scatter allgather algorithmrecexch_single_buffer    - Force generic transport recursive exchange with single buffer for receivesrecexch_multiple_buffer  - Force generic transport recursive exchange with multiple buffers for receives
20211015 115039.977 INFO             PET3 index=  65                        MPIR_CVAR_IALLREDUCE_INTER_ALGORITHM : Variable to select iallreduce algorithmauto                      - Internal algorithm selectionremote_reduce_local_bcast - Force remote-reduce-local-bcast algorithm
20211015 115039.977 INFO             PET3 index=  66                      MPIR_CVAR_IALLREDUCE_DEVICE_COLLECTIVE : If set to true, MPI_Iallreduce will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iallreduce function will not be called.
20211015 115039.977 INFO             PET3 index=  67                         MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM : Variable to select ialltoall algorithmauto              - Internal algorithm selectionbrucks            - Force brucks algorithminplace           - Force inplace algorithmpairwise          - Force pairwise algorithmpermuted_sendrecv - Force permuted sendrecv algorithm
20211015 115039.977 INFO             PET3 index=  68                         MPIR_CVAR_IALLTOALL_INTER_ALGORITHM : Variable to select ialltoall algorithmauto              - Internal algorithm selectionpairwise_exchange - Force pairwise exchange algorithm
20211015 115039.977 INFO             PET3 index=  69                       MPIR_CVAR_IALLTOALL_DEVICE_COLLECTIVE : If set to true, MPI_Ialltoall will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ialltoall function will not be called.
20211015 115039.977 INFO             PET3 index=  70                        MPIR_CVAR_IALLTOALLV_INTRA_ALGORITHM : Variable to select ialltoallv algorithmauto              - Internal algorithm selectionblocked           - Force blocked algorithminplace           - Force inplace algorithmpairwise_exchange - Force pairwise exchange algorithm
20211015 115039.977 INFO             PET3 index=  71                        MPIR_CVAR_IALLTOALLV_INTER_ALGORITHM : Variable to select ialltoallv algorithmauto - Internal algorithm selection
20211015 115039.977 INFO             PET3 index=  72                      MPIR_CVAR_IALLTOALLV_DEVICE_COLLECTIVE : If set to true, MPI_Ialltoallv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ialltoallv function will not be called.
20211015 115039.977 INFO             PET3 index=  73                        MPIR_CVAR_IALLTOALLW_INTRA_ALGORITHM : Variable to select ialltoallw algorithmauto              - Internal algorithm selectionblocked           - Force blocked algorithminplace           - Force inplace algorithmpairwise_exchange - Force pairwise exchange algorithm
20211015 115039.977 INFO             PET3 index=  74                        MPIR_CVAR_IALLTOALLW_INTER_ALGORITHM : Variable to select ialltoallw algorithmauto - Internal algorithm selection
20211015 115039.977 INFO             PET3 index=  75                      MPIR_CVAR_IALLTOALLW_DEVICE_COLLECTIVE : If set to true, MPI_Ialltoallw will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ialltoallw function will not be called.
20211015 115039.977 INFO             PET3 index=  76                             MPIR_CVAR_IBARRIER_RECEXCH_KVAL : k value for recursive exchange based ibarrier
20211015 115039.977 INFO             PET3 index=  77                          MPIR_CVAR_IBARRIER_INTRA_ALGORITHM : Variable to select ibarrier algorithmauto               - Internal algorithm selectionrecursive_doubling - Force recursive doubling algorithmrecexch            - Force generic transport based recursive exchange algorithm
20211015 115039.977 INFO             PET3 index=  78                          MPIR_CVAR_IBARRIER_INTER_ALGORITHM : Variable to select ibarrier algorithmauto  - Internal algorithm selectionbcast - Force bcast algorithm
20211015 115039.977 INFO             PET3 index=  79                        MPIR_CVAR_IBARRIER_DEVICE_COLLECTIVE : If set to true, MPI_Ibarrier will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ibarrier function will not be called.
20211015 115039.977 INFO             PET3 index=  80                                  MPIR_CVAR_IBCAST_TREE_KVAL : k value for tree (kary, knomial, etc.) based ibcast
20211015 115039.977 INFO             PET3 index=  81                                  MPIR_CVAR_IBCAST_TREE_TYPE : Tree type for tree based ibcast kary      - kary tree type knomial_1 - knomial_1 tree type knomial_2 - knomial_2 tree type
20211015 115039.977 INFO             PET3 index=  82                   MPIR_CVAR_IBCAST_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based ibcast. Default value is 0, that is, no pipelining by default
20211015 115039.977 INFO             PET3 index=  83                            MPIR_CVAR_IBCAST_RING_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in ibcast ring algorithm. Default value is 0, that is, no pipelining by default
20211015 115039.977 INFO             PET3 index=  84                            MPIR_CVAR_IBCAST_INTRA_ALGORITHM : Variable to select ibcast algorithmauto                                 - Internal algorithm selectionbinomial                             - Force Binomial algorithmscatter_recursive_doubling_allgather - Force Scatter Recursive Doubling Allgather algorithmscatter_ring_allgather               - Force Scatter Ring Allgather algorithmtree                                 - Force Generic Transport Tree algorithmscatter_recexch_allgather            - Force Generic Transport Scatter followed by Recursive Exchange Allgather algorithmring                                 - Force Generic Transport Ring algorithm
20211015 115039.977 INFO             PET3 index=  85                               MPIR_CVAR_IBCAST_SCATTER_KVAL : k value for tree based scatter in scatter_recexch_allgather algorithm
20211015 115039.977 INFO             PET3 index=  86                     MPIR_CVAR_IBCAST_ALLGATHER_RECEXCH_KVAL : k value for recursive exchange based allgather in scatter_recexch_allgather algorithm
20211015 115039.977 INFO             PET3 index=  87                            MPIR_CVAR_IBCAST_INTER_ALGORITHM : Variable to select ibcast algorithmauto - Internal algorithm selectionflat - Force flat algorithm
20211015 115039.977 INFO             PET3 index=  88                          MPIR_CVAR_IBCAST_DEVICE_COLLECTIVE : If set to true, MPI_Ibcast will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually.  If set to false, the device-level ibcast function will not be called.
20211015 115039.977 INFO             PET3 index=  89                           MPIR_CVAR_IEXSCAN_INTRA_ALGORITHM : Variable to select iexscan algorithmauto               - Internal algorithm selectionrecursive_doubling - Force recursive doubling algorithm
20211015 115039.977 INFO             PET3 index=  90                         MPIR_CVAR_IEXSCAN_DEVICE_COLLECTIVE : If set to true, MPI_Iexscan will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iexscan function will not be called.
20211015 115039.977 INFO             PET3 index=  91                           MPIR_CVAR_IGATHER_INTRA_ALGORITHM : Variable to select igather algorithmauto     - Internal algorithm selectionbinomial - Force binomial algorithmtree     - Force genetric transport based tree algorithm
20211015 115039.977 INFO             PET3 index=  92                                 MPIR_CVAR_IGATHER_TREE_KVAL : k value for tree based igather
20211015 115039.977 INFO             PET3 index=  93                           MPIR_CVAR_IGATHER_INTER_ALGORITHM : Variable to select igather algorithmauto        - Internal algorithm selectionlong_inter  - Force long inter algorithmshort_inter - Force short inter algorithm
20211015 115039.977 INFO             PET3 index=  94                         MPIR_CVAR_IGATHER_DEVICE_COLLECTIVE : If set to true, MPI_Igather will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level igather function will not be called.
20211015 115039.977 INFO             PET3 index=  95                          MPIR_CVAR_IGATHERV_INTRA_ALGORITHM : Variable to select igatherv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET3 index=  96                          MPIR_CVAR_IGATHERV_INTER_ALGORITHM : Variable to select igatherv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET3 index=  97                        MPIR_CVAR_IGATHERV_DEVICE_COLLECTIVE : If set to true, MPI_Igatherv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level igatherv function will not be called.
20211015 115039.977 INFO             PET3 index=  98               MPIR_CVAR_INEIGHBOR_ALLGATHER_INTRA_ALGORITHM : Variable to select ineighbor_allgather algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET3 index=  99               MPIR_CVAR_INEIGHBOR_ALLGATHER_INTER_ALGORITHM : Variable to select ineighbor_allgather algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET3 index= 100             MPIR_CVAR_INEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE : If set to true, MPI_ineighbor_allgather will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ineighbor_allgather function will not be called.
20211015 115039.977 INFO             PET3 index= 101              MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTRA_ALGORITHM : Variable to select ineighbor_allgatherv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET3 index= 102              MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTER_ALGORITHM : Variable to select ineighbor_allgatherv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET3 index= 103            MPIR_CVAR_INEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE : If set to true, MPI_ineighbor_allgatherv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ineighbor_allgatherv function will not be called.
20211015 115039.977 INFO             PET3 index= 104                MPIR_CVAR_INEIGHBOR_ALLTOALL_INTRA_ALGORITHM : Variable to select ineighbor_alltoall algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET3 index= 105                MPIR_CVAR_INEIGHBOR_ALLTOALL_INTER_ALGORITHM : Variable to select ineighbor_alltoall algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET3 index= 106              MPIR_CVAR_INEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE : If set to true, MPI_ineighbor_alltoall will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ineighbor_alltoall function will not be called.
20211015 115039.977 INFO             PET3 index= 107               MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTRA_ALGORITHM : Variable to select ineighbor_alltoallv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET3 index= 108               MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTER_ALGORITHM : Variable to select ineighbor_alltoallv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET3 index= 109             MPIR_CVAR_INEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE : If set to true, MPI_ineighbor_alltoallv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ineighbor_alltoallv function will not be called.
20211015 115039.977 INFO             PET3 index= 110               MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTRA_ALGORITHM : Variable to select ineighbor_alltoallw algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET3 index= 111               MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTER_ALGORITHM : Variable to select ineighbor_alltoallw algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET3 index= 112             MPIR_CVAR_INEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE : If set to true, MPI_ineighbor_alltoallw will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ineighbor_alltoallw function will not be called.
20211015 115039.977 INFO             PET3 index= 113                                 MPIR_CVAR_IREDUCE_TREE_KVAL : k value for tree (kary, knomial, etc.) based ireduce
20211015 115039.977 INFO             PET3 index= 114                                 MPIR_CVAR_IREDUCE_TREE_TYPE : Tree type for tree based ireduce kary      - kary tree knomial_1 - knomial_1 tree knomial_2 - knomial_2 tree
20211015 115039.977 INFO             PET3 index= 115                  MPIR_CVAR_IREDUCE_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based ireduce. Default value is 0, that is, no pipelining by default
20211015 115039.977 INFO             PET3 index= 116                           MPIR_CVAR_IREDUCE_RING_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in ireduce ring algorithm. Default value is 0, that is, no pipelining by default
20211015 115039.977 INFO             PET3 index= 117                     MPIR_CVAR_IREDUCE_TREE_BUFFER_PER_CHILD : If set to true, a rank in tree algorithms will allocate a dedicated buffer for every child it receives data from. This would mean more memory consumption but it would allow preposting of the receives and hence reduce the number of unexpected messages. If set to false, there is only one buffer that is used to receive the data from all the children. The receives are therefore serialized, that is, only one receive can be posted at a time.
20211015 115039.977 INFO             PET3 index= 118                           MPIR_CVAR_IREDUCE_INTRA_ALGORITHM : Variable to select ireduce algorithmauto                  - Internal algorithm selectionbinomial              - Force binomial algorithmreduce_scatter_gather - Force reduce scatter gather algorithmtree                  - Force Generic Transport Treering                  - Force Generic Transport Ring
20211015 115039.978 INFO             PET3 index= 119                           MPIR_CVAR_IREDUCE_INTER_ALGORITHM : Variable to select ireduce algorithmauto                     - Internal algorithm selectionlocal_reduce_remote_send - Force local-reduce-remote-send algorithm
20211015 115039.978 INFO             PET3 index= 120                         MPIR_CVAR_IREDUCE_DEVICE_COLLECTIVE : If set to true, MPI_Ireduce will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ireduce function will not be called.
20211015 115039.978 INFO             PET3 index= 121                      MPIR_CVAR_IREDUCE_SCATTER_RECEXCH_KVAL : k value for recursive exchange based ireduce_scatter
20211015 115039.978 INFO             PET3 index= 122                   MPIR_CVAR_IREDUCE_SCATTER_INTRA_ALGORITHM : Variable to select ireduce_scatter algorithmauto               - Internal algorithm selectionnoncommutative     - Force noncommutative algorithmrecursive_doubling - Force recursive doubling algorithmpairwise           - Force pairwise algorithmrecursive_halving  - Force recursive halving algorithmrecexch  - Force generic transport recursive exchange algorithm
20211015 115039.978 INFO             PET3 index= 123                   MPIR_CVAR_IREDUCE_SCATTER_INTER_ALGORITHM : Variable to select ireduce_scatter algorithmauto                         - Internal algorithm selectionremote_reduce_local_scatterv - Force remote-reduce-local-scatterv algorithm
20211015 115039.978 INFO             PET3 index= 124                 MPIR_CVAR_IREDUCE_SCATTER_DEVICE_COLLECTIVE : If set to true, MPI_Ireduce_scatter will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ireduce_scatter function will not be called.
20211015 115039.978 INFO             PET3 index= 125                MPIR_CVAR_IREDUCE_SCATTER_BLOCK_RECEXCH_KVAL : k value for recursive exchange based ireduce_scatter_block
20211015 115039.978 INFO             PET3 index= 126             MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM : Variable to select ireduce_scatter_block algorithmauto               - Internal algorithm selectionnoncommutative     - Force noncommutative algorithmrecursive_doubling - Force recursive doubling algorithmpairwise           - Force pairwise algorithmrecursive_halving  - Force recursive halving algorithmrecexch  - Force generic transport recursive exchange algorithm
20211015 115039.978 INFO             PET3 index= 127             MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTER_ALGORITHM : Variable to select ireduce_scatter_block algorithmauto                         - Internal algorithm selectionremote_reduce_local_scatterv - Force remote-reduce-local-scatterv algorithm
20211015 115039.978 INFO             PET3 index= 128           MPIR_CVAR_IREDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE : If set to true, MPI_Ireduce_scatter_block will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ireduce_scatter_block function will not be called.
20211015 115039.978 INFO             PET3 index= 129                             MPIR_CVAR_ISCAN_INTRA_ALGORITHM : Variable to select allgather algorithmauto               - Internal algorithm selectionrecursive_doubling - Force recursive doubling algorithm
20211015 115039.978 INFO             PET3 index= 130                           MPIR_CVAR_ISCAN_DEVICE_COLLECTIVE : If set to true, MPI_Iscan will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iscan function will not be called.
20211015 115039.978 INFO             PET3 index= 131                          MPIR_CVAR_ISCATTER_INTRA_ALGORITHM : Variable to select iscatter algorithmauto     - Internal algorithm selectionbinomial - Force binomial algorithmtree     - Force genetric transport based tree algorithm
20211015 115039.978 INFO             PET3 index= 132                                MPIR_CVAR_ISCATTER_TREE_KVAL : k value for tree based iscatter
20211015 115039.978 INFO             PET3 index= 133                          MPIR_CVAR_ISCATTER_INTER_ALGORITHM : Variable to select iscatter algorithmauto                      - Internal algorithm selectionlinear                    - Force linear algorithmremote_send_local_scatter - Force remote-send-local-scatter algorithm
20211015 115039.978 INFO             PET3 index= 134                        MPIR_CVAR_ISCATTER_DEVICE_COLLECTIVE : If set to true, MPI_Iscatter will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iscatter function will not be called.
20211015 115039.978 INFO             PET3 index= 135                         MPIR_CVAR_ISCATTERV_INTRA_ALGORITHM : Variable to select iscatterv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET3 index= 136                         MPIR_CVAR_ISCATTERV_INTER_ALGORITHM : Variable to select iscatterv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET3 index= 137                       MPIR_CVAR_ISCATTERV_DEVICE_COLLECTIVE : If set to true, MPI_Iscatterv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iscatterv function will not be called.
20211015 115039.978 INFO             PET3 index= 138                MPIR_CVAR_NEIGHBOR_ALLGATHER_INTRA_ALGORITHM : Variable to select ineighbor_allgather algorithmauto - Internal algorithm selectionnb   - Force nonblocking algorithm
20211015 115039.978 INFO             PET3 index= 139                MPIR_CVAR_NEIGHBOR_ALLGATHER_INTER_ALGORITHM : Variable to select ineighbor_allgather algorithmauto - Internal algorithm selectionnb   - Force nonblocking algorithm
20211015 115039.978 INFO             PET3 index= 140              MPIR_CVAR_NEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE : If set to true, MPI_neighbor_allgather will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level neighbor_allgather function will not be called.
20211015 115039.978 INFO             PET3 index= 141               MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTRA_ALGORITHM : Variable to select neighbor_allgatherv algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET3 index= 142               MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTER_ALGORITHM : Variable to select neighbor_allgatherv algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET3 index= 143             MPIR_CVAR_NEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE : If set to true, MPI_neighbor_allgatherv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level neighbor_allgatherv function will not be called.
20211015 115039.978 INFO             PET3 index= 144                 MPIR_CVAR_NEIGHBOR_ALLTOALL_INTRA_ALGORITHM : Variable to select neighbor_alltoall algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET3 index= 145                 MPIR_CVAR_NEIGHBOR_ALLTOALL_INTER_ALGORITHM : Variable to select neighbor_alltoall algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET3 index= 146               MPIR_CVAR_NEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE : If set to true, MPI_neighbor_alltoall will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level neighbor_alltoall function will not be called.
20211015 115039.978 INFO             PET3 index= 147                MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTRA_ALGORITHM : Variable to select neighbor_alltoallv algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET3 index= 148                MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTER_ALGORITHM : Variable to select neighbor_alltoallv algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET3 index= 149              MPIR_CVAR_NEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE : If set to true, MPI_neighbor_alltoallv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level neighbor_alltoallv function will not be called.
20211015 115039.978 INFO             PET3 index= 150                MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTRA_ALGORITHM : Variable to select neighbor_alltoallw algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET3 index= 151                MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTER_ALGORITHM : Variable to select neighbor_alltoallw algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET3 index= 152              MPIR_CVAR_NEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE : If set to true, MPI_neighbor_alltoallw will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level neighbor_alltoallw function will not be called.
20211015 115039.978 INFO             PET3 index= 153                             MPIR_CVAR_REDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20211015 115039.978 INFO             PET3 index= 154                                 MPIR_CVAR_ENABLE_SMP_REDUCE : Enable SMP aware reduce.
20211015 115039.978 INFO             PET3 index= 155                           MPIR_CVAR_MAX_SMP_REDUCE_MSG_SIZE : Maximum message size for which SMP-aware reduce is used.  A value of '0' uses SMP-aware reduce for all message sizes.
20211015 115039.978 INFO             PET3 index= 156                            MPIR_CVAR_REDUCE_INTRA_ALGORITHM : Variable to select reduce algorithmauto                  - Internal algorithm selectionbinomial              - Force binomial algorithmnb                    - Force nonblocking algorithmreduce_scatter_gather - Force reduce scatter gather algorithm
20211015 115039.978 INFO             PET3 index= 157                            MPIR_CVAR_REDUCE_INTER_ALGORITHM : Variable to select reduce algorithmauto                     - Internal algorithm selectionlocal_reduce_remote_send - Force local-reduce-remote-send algorithmnb                       - Force nonblocking algorithm
20211015 115039.978 INFO             PET3 index= 158                          MPIR_CVAR_REDUCE_DEVICE_COLLECTIVE : If set to true, MPI_Reduce will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level reduce function will not be called.
20211015 115039.978 INFO             PET3 index= 159          MPIR_CVAR_REDUCE_SCATTER_COMMUTATIVE_LONG_MSG_SIZE : the long message algorithm will be used if the operation is commutative and the send buffer size is >= this value (in bytes)
20211015 115039.978 INFO             PET3 index= 160                    MPIR_CVAR_REDUCE_SCATTER_INTRA_ALGORITHM : Variable to select reduce_scatter algorithmauto               - Internal algorithm selectionnb                 - Force nonblocking algorithmnoncommutative     - Force noncommutative algorithmpairwise           - Force pairwise algorithmrecursive_doubling - Force recursive doubling algorithmrecursive_halving  - Force recursive halving algorithm
20211015 115039.978 INFO             PET3 index= 161                    MPIR_CVAR_REDUCE_SCATTER_INTER_ALGORITHM : Variable to select reduce_scatter algorithmauto                        - Internal algorithm selectionnb                          - Force nonblocking algorithmremote_reduce_local_scatter - Force remote-reduce-local-scatter algorithm
20211015 115039.978 INFO             PET3 index= 162                  MPIR_CVAR_REDUCE_SCATTER_DEVICE_COLLECTIVE : If set to true, MPI_Redscat will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level reduce_scatter function will not be called.
20211015 115039.978 INFO             PET3 index= 163              MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM : Variable to select reduce_scatter_block algorithmauto               - Internal algorithm selectionnoncommutative     - Force noncommutative algorithmrecursive_doubling - Force recursive doubling algorithmpairwise           - Force pairwise algorithmrecursive_halving  - Force recursive halving algorithmnb                 - Force nonblocking algorithm
20211015 115039.978 INFO             PET3 index= 164              MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTER_ALGORITHM : Variable to select reduce_scatter_block algorithmauto                        - Internal algorithm selectionnb                          - Force nonblocking algorithmremote_reduce_local_scatter - Force remote-reduce-local-scatter algorithm
20211015 115039.978 INFO             PET3 index= 165            MPIR_CVAR_REDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE : If set to true, MPI_Reduce_scatter_block will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level reduce_scatter_block function will not be called.
20211015 115039.978 INFO             PET3 index= 166                              MPIR_CVAR_SCAN_INTRA_ALGORITHM : Variable to select allgather algorithmauto               - Internal algorithm selectionnb                 - Force nonblocking algorithmrecursive_doubling - Force recursive doubling algorithm
20211015 115039.978 INFO             PET3 index= 167                            MPIR_CVAR_SCAN_DEVICE_COLLECTIVE : If set to true, MPI_Scan will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level scan function will not be called.
20211015 115039.978 INFO             PET3 index= 168                      MPIR_CVAR_SCATTER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Scatter if the send buffer size is < this value (in bytes)
20211015 115039.978 INFO             PET3 index= 169                           MPIR_CVAR_SCATTER_INTRA_ALGORITHM : Variable to select scatter algorithmauto     - Internal algorithm selectionbinomial - Force binomial algorithmnb       - Force nonblocking algorithm
20211015 115039.978 INFO             PET3 index= 170                           MPIR_CVAR_SCATTER_INTER_ALGORITHM : Variable to select scatter algorithmauto                      - Internal algorithm selectionlinear                    - Force linear algorithmnb                        - Force nonblocking algorithmremote_send_local_scatter - Force remote-send-local-scatter algorithm
20211015 115039.978 INFO             PET3 index= 171                         MPIR_CVAR_SCATTER_DEVICE_COLLECTIVE : If set to true, MPI_Scatter will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level scatter function will not be called.
20211015 115039.978 INFO             PET3 index= 172                          MPIR_CVAR_SCATTERV_INTRA_ALGORITHM : Variable to select scatterv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithmnb     - Force nonblocking algorithm
20211015 115039.978 INFO             PET3 index= 173                          MPIR_CVAR_SCATTERV_INTER_ALGORITHM : Variable to select scatterv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithmnb     - Force nonblocking algorithm
20211015 115039.978 INFO             PET3 index= 174                        MPIR_CVAR_SCATTERV_DEVICE_COLLECTIVE : If set to true, MPI_scatterv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level scatterv function will not be called.
20211015 115039.978 INFO             PET3 index= 175                                MPIR_CVAR_DEVICE_COLLECTIVES : If set to true, MPI collectives will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually.  If set to false, the device-level collective function will not be called.
20211015 115039.978 INFO             PET3 index= 176                                MPIR_CVAR_PROGRESS_MAX_COLLS : Maximum number of collective operations at a time that the progress engine should make progress on
20211015 115039.978 INFO             PET3 index= 177                              MPIR_CVAR_COMM_SPLIT_USE_QSORT : Use qsort(3) in the implementation of MPI_Comm_split instead of bubble sort.
20211015 115039.978 INFO             PET3 index= 178                                  MPIR_CVAR_CTXID_EAGER_SIZE : The MPIR_CVAR_CTXID_EAGER_SIZE environment variable allows you to specify how many words in the context ID mask will be set aside for the eager allocation protocol.  If the application is running out of context IDs, reducing this value may help.
20211015 115039.978 INFO             PET3 index= 179                                    MPIR_CVAR_PROCTABLE_SIZE : Size of the "MPIR" debugger interface proctable (process table).
20211015 115039.978 INFO             PET3 index= 180                                   MPIR_CVAR_PROCTABLE_PRINT : If true, dump the proctable entries at MPII_Wait_for_debugger-time.
20211015 115039.978 INFO             PET3 index= 181                                 MPIR_CVAR_PRINT_ERROR_STACK : If true, print an error stack trace at error handling time.
20211015 115039.978 INFO             PET3 index= 182                                  MPIR_CVAR_CHOP_ERROR_STACK : If >0, truncate error stack output lines this many characters wide.  If 0, do not truncate, and if <0 use a sensible default.
20211015 115039.978 INFO             PET3 index= 183                            MPIR_CVAR_SUPPRESS_ABORT_MESSAGE : Disable printing of abort error message.
20211015 115039.978 INFO             PET3 index= 184                                           MPIR_CVAR_MEMDUMP : If true, list any memory that was allocated by MPICH and that remains allocated when MPI_Finalize completes.
20211015 115039.978 INFO             PET3 index= 185                          MPIR_CVAR_MEM_CATEGORY_INFORMATION : If true, print a summary of memory allocation by category. The category definitions are found in mpl_trmem.h.
20211015 115039.978 INFO             PET3 index= 186                                    MPIR_CVAR_ASYNC_PROGRESS : If set to true, MPICH will initiate an additional thread to make asynchronous progress on all communication operations including point-to-point, collective, one-sided operations and I/O.  Setting this variable will automatically increase the thread-safety level to MPI_THREAD_MULTIPLE.  While this improves the progress semantics, it might cause a small amount of performance overhead for regular MPI operations.  The user is encouraged to leave one or more hardware threads vacant in order to prevent contention between the application threads and the progress thread(s).  The impact of oversubscription is highly system dependent but may be substantial in some cases, hence this recommendation.
20211015 115039.978 INFO             PET3 index= 187                              MPIR_CVAR_DEFAULT_THREAD_LEVEL : Sets the default thread level to use when using MPI_INIT. This variable is case-insensitive.
20211015 115039.978 INFO             PET3 index= 188                                        MPIR_CVAR_DEBUG_HOLD : If true, causes processes to wait in MPI_Init and MPI_Initthread for a debugger to be attached.  Once the debugger has attached, the variable 'hold' should be set to 0 in order to allow the process to continue (e.g., in gdb, "set hold=0").
20211015 115039.978 INFO             PET3 index= 189                                    MPIR_CVAR_ERROR_CHECKING : If true, perform checks for errors, typically to verify valid inputs to MPI routines.  Only effective when MPICH is configured with --enable-error-checking=runtime .
20211015 115039.978 INFO             PET3 index= 190                                  MPIR_CVAR_NETLOC_NODE_FILE : Subnet json file
20211015 115039.978 INFO             PET3 index= 191                                      MPIR_CVAR_DIMS_VERBOSE : If true, enable verbose output about the actions of the implementation of MPI_Dims_create.
20211015 115039.978 INFO             PET3 index= 192                              MPIR_CVAR_NAMESERV_FILE_PUBDIR : Sets the directory to use for MPI service publishing in the file nameserv implementation.  Allows the user to override where the publish and lookup information is placed for connect/accept based applications.
20211015 115039.978 INFO             PET3 index= 193                           MPIR_CVAR_ABORT_ON_LEAKED_HANDLES : If true, MPI will call MPI_Abort at MPI_Finalize if any MPI object handles have been leaked.  For example, if MPI_Comm_dup is called without calling a corresponding MPI_Comm_free.  For uninteresting reasons, enabling this option may prevent all known object leaks from being reported.  MPICH must have been configure with "--enable-g=handlealloc" or better in order for this functionality to work.
20211015 115039.978 INFO             PET3 index= 194                                           MPIR_CVAR_NOLOCAL : If true, force all processes to operate as though all processes are located on another node.  For example, this disables shared memory communication hierarchical collectives.
20211015 115039.978 INFO             PET3 index= 195                                  MPIR_CVAR_ODD_EVEN_CLIQUES : If true, odd procs on a node are seen as local to each other, and even procs on a node are seen as local to each other.  Used for debugging on a single machine. Deprecated in favor of MPIR_CVAR_NUM_CLIQUES.
20211015 115039.978 INFO             PET3 index= 196                                       MPIR_CVAR_NUM_CLIQUES : Specify the number of cliques that should be used to partition procs on a local node. Procs with the same clique number are seen as local to each other. Used for debugging on a single machine.
20211015 115039.978 INFO             PET3 index= 197                                  MPIR_CVAR_COLL_ALIAS_CHECK : Enable checking of aliasing in collective operations
20211015 115039.978 INFO             PET3 index= 198                                 MPIR_CVAR_REQUEST_POLL_FREQ : How frequent to poll during completion calls (wait/test) in terms of number of processed requests before polling.
20211015 115039.978 INFO             PET3 index= 199                                MPIR_CVAR_REQUEST_BATCH_SIZE : The number of requests to make completion as a batch in MPI_Waitall and MPI_Testall implementation. A large number is likely to cause more cache misses.
20211015 115039.978 INFO             PET3 index= 200                                MPIR_CVAR_POLLS_BEFORE_YIELD : When MPICH is in a busy waiting loop, it will periodically call a function to yield the processor.  This cvar sets the number of loops before the yield function is called.  A value of 0 disables yielding.
20211015 115039.978 INFO             PET3 index= 201                          MPIR_CVAR_NEMESIS_MXM_BULK_CONNECT : If true, force mxm to connect all processes at initialization time.
20211015 115039.978 INFO             PET3 index= 202                       MPIR_CVAR_NEMESIS_MXM_BULK_DISCONNECT : If true, force mxm to disconnect all processes at finalization time.
20211015 115039.978 INFO             PET3 index= 203                              MPIR_CVAR_NEMESIS_MXM_HUGEPAGE : If true, mxm tries detecting hugepage support.  On HPC-X 2.3 and earlier, this might cause problems on Ubuntu and other platforms even if the system provides hugepage support.
20211015 115039.978 INFO             PET3 index= 204                                  MPIR_CVAR_OFI_USE_PROVIDER : If non-null, choose an OFI provider by name. If using with the CH4 device and using a provider that supports an older version of the libfabric API then the default version of the installed library, specifying the OFI version via the appropriate CVARs is also recommended.
20211015 115039.978 INFO             PET3 index= 205                                MPIR_CVAR_OFI_DUMP_PROVIDERS : If true, dump provider information at init
20211015 115039.978 INFO             PET3 index= 206                            MPIR_CVAR_CH3_INTERFACE_HOSTNAME : If non-NULL, this cvar specifies the IP address that other processes should use when connecting to this process. This cvar is mutually exclusive with the MPIR_CVAR_CH3_NETWORK_IFACE cvar and it is an error to set them both.
20211015 115039.978 INFO             PET3 index= 207                                    MPIR_CVAR_CH3_PORT_RANGE : The MPIR_CVAR_CH3_PORT_RANGE environment variable allows you to specify the range of TCP ports to be used by the process manager and the MPICH library. The format of this variable is <low>:<high>.  To specify any available port, use 0:0.
20211015 115039.978 INFO             PET3 index= 208                         MPIR_CVAR_NEMESIS_TCP_NETWORK_IFACE : If non-NULL, this cvar specifies which pseudo-ethernet interface the tcp netmod should use (e.g., "eth1", "ib0"). Note, this is a Linux-specific cvar. This cvar is mutually exclusive with the MPIR_CVAR_CH3_INTERFACE_HOSTNAME cvar and it is an error to set them both.
20211015 115039.978 INFO             PET3 index= 209                   MPIR_CVAR_NEMESIS_TCP_HOST_LOOKUP_RETRIES : This cvar controls the number of times to retry the gethostbyname() function before giving up.
20211015 115039.978 INFO             PET3 index= 210                            MPIR_CVAR_NEMESIS_ENABLE_CKPOINT : If true, enables checkpointing support and returns an error if checkpointing library cannot be initialized.
20211015 115039.978 INFO             PET3 index= 211                          MPIR_CVAR_NEMESIS_SHM_EAGER_MAX_SZ : This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for shared memory. If this cvar is set to -1, then Nemesis will choose an appropriate value.
20211015 115039.978 INFO             PET3 index= 212                    MPIR_CVAR_NEMESIS_SHM_READY_EAGER_MAX_SZ : This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for ready-send messages.  If this cvar is set to -1, then ready messages will always be sent eagerly.  If this cvar is set to -2, then Nemesis will choose an appropriate value.
20211015 115039.979 INFO             PET3 index= 213                                         MPIR_CVAR_ENABLE_FT : Enable fault tolerance functions
20211015 115039.979 INFO             PET3 index= 214                         MPIR_CVAR_NEMESIS_LMT_DMA_THRESHOLD : Messages larger than this size will use the "dma" (knem) intranode LMT implementation, if it is enabled and available.
20211015 115039.979 INFO             PET3 index= 215                                    MPIR_CVAR_NEMESIS_NETMOD : If non-empty, this cvar specifies which network module should be used for communication. This variable is case-insensitive.
20211015 115039.979 INFO             PET3 index= 216                                  MPIR_CVAR_CH3_ENABLE_HCOLL : If true, enable HCOLL collectives.
20211015 115039.979 INFO             PET3 index= 217                          MPIR_CVAR_CH3_COMM_CONNECT_TIMEOUT : The default time out period in seconds for a connection attempt to the server communicator where the named port exists but no pending accept. User can change the value for a specified connection through its info argument.
20211015 115039.979 INFO             PET3 index= 218               MPIR_CVAR_CH3_RMA_OP_PIGGYBACK_LOCK_DATA_SIZE : Specify the threshold of data size of a RMA operation which can be piggybacked with a LOCK message. It is always a positive value and should not be smaller than MPIDI_RMA_IMMED_BYTES. If user sets it as a small value, for middle and large data size, we will lose performance because of always waiting for round-trip of LOCK synchronization; if user sets it as a large value, we need to consume more memory on target side to buffer this lock request when lock is not satisfied.
20211015 115039.979 INFO             PET3 index= 219                      MPIR_CVAR_CH3_RMA_ACTIVE_REQ_THRESHOLD : Threshold of number of active requests to trigger blocking waiting in operation routines. When the value is negative, we never blockingly wait in operation routines. When the value is zero, we always trigger blocking waiting in operation routines to wait until no. of active requests becomes zero. When the value is positive, we do blocking waiting in operation routines to wait until no. of active requests being reduced to this value.
20211015 115039.979 INFO             PET3 index= 220               MPIR_CVAR_CH3_RMA_POKE_PROGRESS_REQ_THRESHOLD : Threshold at which the RMA implementation attempts to complete requests while completing RMA operations and while using the lazy synchonization approach.  Change this value if programs fail because they run out of requests or other internal resources
20211015 115039.979 INFO             PET3 index= 221                MPIR_CVAR_CH3_RMA_SCALABLE_FENCE_PROCESS_NUM : Specify the threshold of switching the algorithm used in FENCE from the basic algorithm to the scalable algorithm. The value can be nagative, zero or positive. When the number of processes is larger than or equal to this value, FENCE will use a scalable algorithm which do not use O(P) data structure; when the number of processes is smaller than the value, FENCE will use a basic but fast algorithm which requires an O(P) data structure.
20211015 115039.979 INFO             PET3 index= 222            MPIR_CVAR_CH3_RMA_DELAY_ISSUING_FOR_PIGGYBACKING : Specify if delay issuing of RMA operations for piggybacking LOCK/UNLOCK/FLUSH is enabled. It can be either 0 or 1. When it is set to 1, the issuing of LOCK message is delayed until origin process see the first RMA operation and piggyback LOCK with that operation, and the origin process always keeps the current last operation until the ending synchronization call in order to piggyback UNLOCK/FLUSH with that operation. When it is set to 0, in WIN_LOCK/UNLOCK case, the LOCK message is sent out as early as possible, in WIN_LOCK_ALL/UNLOCK_ALL case, the origin process still tries to piggyback LOCK message with the first operation; for UNLOCK/FLUSH message, the origin process no longer keeps the current last operation but only piggyback UNLOCK/FLUSH if there is an operation avaliable in the ending synchronization call.
20211015 115039.979 INFO             PET3 index= 223                                MPIR_CVAR_CH3_RMA_SLOTS_SIZE : Number of RMA slots during window creation. Each slot contains a linked list of target elements. The distribution of ranks among slots follows a round-robin pattern. Requires a positive value.
20211015 115039.979 INFO             PET3 index= 224                    MPIR_CVAR_CH3_RMA_TARGET_LOCK_DATA_BYTES : Size (in bytes) of available lock data this window can provided. If current buffered lock data is more than this value, the process will drop the upcoming operation data. Requires a positive calue.
20211015 115039.979 INFO             PET3 index= 225                            MPIR_CVAR_CH3_EAGER_MAX_MSG_SIZE : This cvar controls the message size at which CH3 switches from eager to rendezvous mode.
20211015 115039.979 INFO             PET3 index= 226                          MPIR_CVAR_CH3_RMA_OP_WIN_POOL_SIZE : Size of the window-private RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediately.  Requires a positive value.
20211015 115039.979 INFO             PET3 index= 227                       MPIR_CVAR_CH3_RMA_OP_GLOBAL_POOL_SIZE : Size of the Global RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediatly.  Requires a positive value.
20211015 115039.979 INFO             PET3 index= 228                      MPIR_CVAR_CH3_RMA_TARGET_WIN_POOL_SIZE : Size of the window-private RMA target pool (in number of targets) that stores information about RMA targets that could not be issued immediately.  Requires a positive value.
20211015 115039.979 INFO             PET3 index= 229                   MPIR_CVAR_CH3_RMA_TARGET_GLOBAL_POOL_SIZE : Size of the Global RMA targets pool (in number of targets) that stores information about RMA targets that could not be issued immediatly.  Requires a positive value.
20211015 115039.979 INFO             PET3 index= 230           MPIR_CVAR_CH3_RMA_TARGET_LOCK_ENTRY_WIN_POOL_SIZE : Size of the window-private RMA lock entries pool (in number of lock entries) that stores information about RMA lock requests that could not be satisfied immediatly.  Requires a positive value.
20211015 115039.979 INFO             PET3 index= 231                     MPIR_CVAR_CH4_OFI_CAPABILITY_SETS_DEBUG : Prints out the configuration of each capability selected via the capability sets interface.
20211015 115039.979 INFO             PET3 index= 232                               MPIR_CVAR_CH4_OFI_ENABLE_DATA : Enable immediate data fields in OFI to transmit source rank outside of the match bits
20211015 115039.979 INFO             PET3 index= 233                           MPIR_CVAR_CH4_OFI_ENABLE_AV_TABLE : If true, the OFI addressing information will be stored with an FI_AV_TABLE. If false, an FI_AV_MAP will be used.
20211015 115039.979 INFO             PET3 index= 234                 MPIR_CVAR_CH4_OFI_ENABLE_SCALABLE_ENDPOINTS : If true, use OFI scalable endpoints.
20211015 115039.979 INFO             PET3 index= 235                    MPIR_CVAR_CH4_OFI_ENABLE_SHARED_CONTEXTS : If set to false (zero), MPICH does not use OFI shared contexts. If set to -1, it is determined by the OFI capability sets based on the provider. Otherwise, MPICH tries to use OFI shared contexts. If they are unavailable, it'll fall back to the mode without shared contexts.
20211015 115039.979 INFO             PET3 index= 236                        MPIR_CVAR_CH4_OFI_ENABLE_MR_SCALABLE : If true, MR_SCALABLE for OFI memory regions. If false, MR_BASIC for OFI memory regions.
20211015 115039.979 INFO             PET3 index= 237                             MPIR_CVAR_CH4_OFI_ENABLE_TAGGED : If true, use tagged message transmission functions in OFI.
20211015 115039.979 INFO             PET3 index= 238                                 MPIR_CVAR_CH4_OFI_ENABLE_AM : If true, enable OFI active message support.
20211015 115039.979 INFO             PET3 index= 239                                MPIR_CVAR_CH4_OFI_ENABLE_RMA : If true, enable OFI RMA support.
20211015 115039.979 INFO             PET3 index= 240                            MPIR_CVAR_CH4_OFI_ENABLE_ATOMICS : If true, enable OFI Atomics support.
20211015 115039.979 INFO             PET3 index= 241                       MPIR_CVAR_CH4_OFI_FETCH_ATOMIC_IOVECS : Specifies the maximum number of iovecs that can be used by the OFI provider for fetch_atomic operations. The default value is -1, indicating that no value is set.
20211015 115039.979 INFO             PET3 index= 242                 MPIR_CVAR_CH4_OFI_ENABLE_DATA_AUTO_PROGRESS : If true, enable MPI data auto progress.
20211015 115039.979 INFO             PET3 index= 243              MPIR_CVAR_CH4_OFI_ENABLE_CONTROL_AUTO_PROGRESS : If true, enable MPI control auto progress.
20211015 115039.979 INFO             PET3 index= 244                       MPIR_CVAR_CH4_OFI_ENABLE_PT2PT_NOPACK : If true, enable iovec for pt2pt.
20211015 115039.979 INFO             PET3 index= 245                           MPIR_CVAR_CH4_OFI_CONTEXT_ID_BITS : Specifies the number of bits that will be used for matching the context ID. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20211015 115039.979 INFO             PET3 index= 246                                 MPIR_CVAR_CH4_OFI_RANK_BITS : Specifies the number of bits that will be used for matching the MPI rank. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20211015 115039.979 INFO             PET3 index= 247                                  MPIR_CVAR_CH4_OFI_TAG_BITS : Specifies the number of bits that will be used for matching the user tag. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20211015 115039.979 INFO             PET3 index= 248                             MPIR_CVAR_CH4_OFI_MAJOR_VERSION : Specifies the major version of the OFI library. The default is the major version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
20211015 115039.979 INFO             PET3 index= 249                             MPIR_CVAR_CH4_OFI_MINOR_VERSION : Specifies the major version of the OFI library. The default is the minor version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
20211015 115039.979 INFO             PET3 index= 250                                  MPIR_CVAR_CH4_OFI_MAX_VNIS : If set to positive, this CVAR specifies the maximum number of CH4 VNIs that OFI netmod exposes.
20211015 115039.979 INFO             PET3 index= 251                           MPIR_CVAR_CH4_OFI_MAX_RMA_SEP_CTX : If set to positive, this CVAR specifies the maximum number of transmit contexts RMA can utilize in a scalable endpoint. This value is effective only when scalable endpoint is available, otherwise it will be ignored.
20211015 115039.979 INFO             PET3 index= 252                          MPIR_CVAR_CH4_OFI_MAX_EAGAIN_RETRY : If set to positive, this CVAR specifies the maximum number of retries of an ofi operations before returning MPIX_ERR_EAGAIN. This value is effective only when the communicator has the MPI_OFI_set_eagain info hint set to true.
20211015 115039.979 INFO             PET3 index= 253                            MPIR_CVAR_CH4_OFI_NUM_AM_BUFFERS : Specifies the number of buffers for receiving active messages.
20211015 115039.979 INFO             PET3 index= 254                                        MPIR_CVAR_CH4_NETMOD : If non-empty, this cvar specifies which network module to use
20211015 115039.979 INFO             PET3 index= 255                                           MPIR_CVAR_CH4_SHM : If non-empty, this cvar specifies which shm module to use
20211015 115039.979 INFO             PET3 index= 256                                MPIR_CVAR_CH4_ROOTS_ONLY_PMI : Enables an optimized business card exchange over PMI for node root processes only.
20211015 115039.979 INFO             PET3 index= 257                            MPIR_CVAR_CH4_RUNTIME_CONF_DEBUG : If enabled, CH4-level runtime configurations are printed out
20211015 115039.979 INFO             PET3 index= 258                                      MPIR_CVAR_CH4_MT_MODEL : Specifies the CH4 multi-threading model. Possible values are: direct (default) handoff trylock
20211015 115039.979 INFO             PET3 index= 259                          MPIR_CVAR_CH4_COMM_CONNECT_TIMEOUT : The default time out period in seconds for a connection attempt to the server communicator where the named port exists but no pending accept. User can change the value for a specified connection through its info argument.
20211015 115039.979 INFO             PET3 index= 260                             MPIR_CVAR_CH4_RANDOM_ADDR_RETRY : The default number of retries for generating a random address. A retrying involves only local operations.
20211015 115039.979 INFO             PET3 index= 261                             MPIR_CVAR_CH4_SHM_SYMHEAP_RETRY : The default number of retries for allocating a symmetric heap in shared memory. A retrying involves collective communication over the group in the shared memory.
20211015 115039.979 INFO             PET3 index= 262                             MPIR_CVAR_CH4_RMA_MEM_EFFICIENT : If true, memory-saving mode is on, per-target object is released at the epoch end call. If false, performance-efficient mode is on, all allocated target objects are cached and freed at win_finalize.
20211015 115039.979 INFO             PET3 index= 263                                      MPIR_CVAR_ENABLE_HCOLL : Enable hcoll collective support.
20211015 115039.979 INFO             PET3 index= 264                                   MPIR_CVAR_COLL_SCHED_DUMP : Print schedule data for nonblocking collective operations.
20211015 115039.979 INFO             PET3 --- VMK::logSystem() end ---------------------------------
20211015 115039.979 INFO             PET3 main: --- VMK::log() start -------------------------------------
20211015 115039.979 INFO             PET3 main: vm located at: 0x7fe0e840cf90
20211015 115039.979 INFO             PET3 main: petCount=6 localPet=3 mypthid=0 currentSsiPe=-1
20211015 115039.979 INFO             PET3 main: Current system level OMP_NUM_THREADS setting for local PET: 12
20211015 115039.979 INFO             PET3 main: ssiCount=1 localSsi=0
20211015 115039.979 INFO             PET3 main: mpionly=1 threadsflag=0
20211015 115039.979 INFO             PET3 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20211015 115039.979 INFO             PET3 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20211015 115039.979 INFO             PET3 main:  PE=0 SSI=0 SSIPE=0
20211015 115039.979 INFO             PET3 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20211015 115039.979 INFO             PET3 main:  PE=1 SSI=0 SSIPE=1
20211015 115039.979 INFO             PET3 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20211015 115039.979 INFO             PET3 main:  PE=2 SSI=0 SSIPE=2
20211015 115039.979 INFO             PET3 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20211015 115039.979 INFO             PET3 main:  PE=3 SSI=0 SSIPE=3
20211015 115039.979 INFO             PET3 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20211015 115039.979 INFO             PET3 main:  PE=4 SSI=0 SSIPE=4
20211015 115039.979 INFO             PET3 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20211015 115039.979 INFO             PET3 main:  PE=5 SSI=0 SSIPE=5
20211015 115039.979 INFO             PET3 main: --- VMK::log() end ---------------------------------------
20211015 115039.980 INFO             PET3 Executing 'userm1_setvm'
20211015 115039.980 INFO             PET3 Executing 'userm1_register'
20211015 115039.980 INFO             PET3 Executing 'userm2_setvm'
20211015 115039.980 INFO             PET3 Executing 'userm2_register'
20211015 115039.980 INFO             PET3 model1: --- VMK::log() start -------------------------------------
20211015 115039.980 INFO             PET3 model1: vm located at: 0x7fe0e9305180
20211015 115039.980 INFO             PET3 model1: petCount=6 localPet=3 mypthid=0 currentSsiPe=-1
20211015 115039.980 INFO             PET3 model1: Current system level OMP_NUM_THREADS setting for local PET: 12
20211015 115039.980 INFO             PET3 model1: ssiCount=1 localSsi=0
20211015 115039.980 INFO             PET3 model1: mpionly=1 threadsflag=0
20211015 115039.980 INFO             PET3 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20211015 115039.980 INFO             PET3 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20211015 115039.980 INFO             PET3 model1:  PE=0 SSI=0 SSIPE=0
20211015 115039.980 INFO             PET3 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20211015 115039.980 INFO             PET3 model1:  PE=1 SSI=0 SSIPE=1
20211015 115039.980 INFO             PET3 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20211015 115039.980 INFO             PET3 model1:  PE=2 SSI=0 SSIPE=2
20211015 115039.980 INFO             PET3 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20211015 115039.980 INFO             PET3 model1:  PE=3 SSI=0 SSIPE=3
20211015 115039.980 INFO             PET3 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20211015 115039.980 INFO             PET3 model1:  PE=4 SSI=0 SSIPE=4
20211015 115039.980 INFO             PET3 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20211015 115039.980 INFO             PET3 model1:  PE=5 SSI=0 SSIPE=5
20211015 115039.980 INFO             PET3 model1: --- VMK::log() end ---------------------------------------
20211015 115039.985 INFO             PET3 Entering 'user1_run'
20211015 115039.985 INFO             PET3  user1_run: on SSIPE:           -1  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20211015 115040.390 INFO             PET3  user1_run: on SSIPE:           -1  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20211015 115040.639 INFO             PET3  user1_run: on SSIPE:           -1  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20211015 115040.903 INFO             PET3  user1_run: on SSIPE:           -1  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20211015 115041.152 INFO             PET3  user1_run: on SSIPE:           -1  Filling data lbound:        5002           1           1  ubound:        6668        1200          10
20211015 115041.408 INFO             PET3 Exiting 'user1_run'
20211015 115041.412 INFO             PET3 Entering 'user2_run'
20211015 115041.412 INFO             PET3 model2: --- VMK::log() start -------------------------------------
20211015 115041.412 INFO             PET3 model2: vm located at: 0x7fe0e9305650
20211015 115041.412 INFO             PET3 model2: petCount=6 localPet=3 mypthid=0 currentSsiPe=-1
20211015 115041.412 INFO             PET3 model2: Current system level OMP_NUM_THREADS setting for local PET: 12
20211015 115041.412 INFO             PET3 model2: ssiCount=1 localSsi=0
20211015 115041.412 INFO             PET3 model2: mpionly=1 threadsflag=0
20211015 115041.412 INFO             PET3 model2: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20211015 115041.412 INFO             PET3 model2: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20211015 115041.412 INFO             PET3 model2:  PE=0 SSI=0 SSIPE=0
20211015 115041.412 INFO             PET3 model2: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20211015 115041.412 INFO             PET3 model2:  PE=1 SSI=0 SSIPE=1
20211015 115041.412 INFO             PET3 model2: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20211015 115041.412 INFO             PET3 model2:  PE=2 SSI=0 SSIPE=2
20211015 115041.412 INFO             PET3 model2: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20211015 115041.412 INFO             PET3 model2:  PE=3 SSI=0 SSIPE=3
20211015 115041.412 INFO             PET3 model2: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20211015 115041.412 INFO             PET3 model2:  PE=4 SSI=0 SSIPE=4
20211015 115041.412 INFO             PET3 model2: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20211015 115041.412 INFO             PET3 model2:  PE=5 SSI=0 SSIPE=5
20211015 115041.412 INFO             PET3 model2: --- VMK::log() end ---------------------------------------
20211015 115041.412 INFO             PET3  user2_run: ssiLocalDeCount=           6
20211015 115041.413 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:           -1  Testing data for localDe =           2  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20211015 115041.413 INFO             PET3  user2_run: OpenMP thread:           3  on SSIPE:           -1  Testing data for localDe =           3  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20211015 115041.423 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:           -1  Testing data for localDe =           0  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20211015 115041.433 INFO             PET3  user2_run: OpenMP thread:           5  on SSIPE:           -1  Testing data for localDe =           5  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20211015 115041.453 INFO             PET3  user2_run: OpenMP thread:           4  on SSIPE:           -1  Testing data for localDe =           4  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20211015 115041.472 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:           -1  Testing data for localDe =           1  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20211015 115042.896 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:           -1  Testing data for localDe =           0  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20211015 115042.898 INFO             PET3  user2_run: OpenMP thread:           3  on SSIPE:           -1  Testing data for localDe =           3  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20211015 115042.898 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:           -1  Testing data for localDe =           2  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20211015 115042.899 INFO             PET3  user2_run: OpenMP thread:           5  on SSIPE:           -1  Testing data for localDe =           5  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20211015 115042.901 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:           -1  Testing data for localDe =           1  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20211015 115042.904 INFO             PET3  user2_run: OpenMP thread:           4  on SSIPE:           -1  Testing data for localDe =           4  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20211015 115044.234 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:           -1  Testing data for localDe =           2  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20211015 115044.235 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:           -1  Testing data for localDe =           0  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20211015 115044.237 INFO             PET3  user2_run: OpenMP thread:           3  on SSIPE:           -1  Testing data for localDe =           3  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20211015 115044.238 INFO             PET3  user2_run: OpenMP thread:           5  on SSIPE:           -1  Testing data for localDe =           5  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20211015 115044.239 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:           -1  Testing data for localDe =           1  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20211015 115044.241 INFO             PET3  user2_run: OpenMP thread:           4  on SSIPE:           -1  Testing data for localDe =           4  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20211015 115045.573 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:           -1  Testing data for localDe =           0  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20211015 115045.574 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:           -1  Testing data for localDe =           2  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20211015 115045.575 INFO             PET3  user2_run: OpenMP thread:           3  on SSIPE:           -1  Testing data for localDe =           3  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20211015 115045.578 INFO             PET3  user2_run: OpenMP thread:           5  on SSIPE:           -1  Testing data for localDe =           5  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20211015 115045.578 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:           -1  Testing data for localDe =           1  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20211015 115045.582 INFO             PET3  user2_run: OpenMP thread:           4  on SSIPE:           -1  Testing data for localDe =           4  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20211015 115046.924 INFO             PET3  user2_run: OpenMP thread:           0  on SSIPE:           -1  Testing data for localDe =           0  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20211015 115046.925 INFO             PET3  user2_run: OpenMP thread:           2  on SSIPE:           -1  Testing data for localDe =           2  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20211015 115046.926 INFO             PET3  user2_run: OpenMP thread:           3  on SSIPE:           -1  Testing data for localDe =           3  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20211015 115046.934 INFO             PET3  user2_run: OpenMP thread:           4  on SSIPE:           -1  Testing data for localDe =           4  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20211015 115046.935 INFO             PET3  user2_run: OpenMP thread:           5  on SSIPE:           -1  Testing data for localDe =           5  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20211015 115046.939 INFO             PET3  user2_run: OpenMP thread:           1  on SSIPE:           -1  Testing data for localDe =           1  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20211015 115048.260 INFO             PET3  user2_run: All data correct.
20211015 115048.260 INFO             PET3 Exiting 'user2_run'
20211015 115048.325 INFO             PET3  NUMBER_OF_PROCESSORS           6
20211015 115048.325 INFO             PET3  PASS  System Test ESMF_FieldSharedDeSSISTest, ESMF_FieldSharedDeSSISTest.F90, line 276
20211015 115048.325 INFO             PET3 Finalizing ESMF
20211015 115039.974 INFO             PET4 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20211015 115039.975 INFO             PET4 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20211015 115039.975 INFO             PET4 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20211015 115039.975 INFO             PET4 !!! FOR PRODUCTION RUNS, USE:                      !!!
20211015 115039.975 INFO             PET4 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20211015 115039.975 INFO             PET4 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20211015 115039.975 INFO             PET4 Running with ESMF Version   : ESMF_8_2_0_beta_snapshot_22-10-g6974ca47a6
20211015 115039.975 INFO             PET4 ESMF library build date/time: "Oct 15 2021" "11:35:17"
20211015 115039.975 INFO             PET4 ESMF library build location : /Volumes/esmf/rocky/esmf-testing/gfortran_9.3.0_mpich3_O_release_8.2.0
20211015 115039.975 INFO             PET4 ESMF_COMM                   : mpich3
20211015 115039.975 INFO             PET4 ESMF_MOAB                   : enabled
20211015 115039.975 INFO             PET4 ESMF_LAPACK                 : enabled
20211015 115039.975 INFO             PET4 ESMF_NETCDF                 : enabled
20211015 115039.975 INFO             PET4 ESMF_PNETCDF                : disabled
20211015 115039.975 INFO             PET4 ESMF_PIO                    : enabled
20211015 115039.975 INFO             PET4 ESMF_YAMLCPP                : enabled
20211015 115039.976 INFO             PET4 --- VMK::logSystem() start -------------------------------
20211015 115039.976 INFO             PET4 esmfComm=mpich3
20211015 115039.976 INFO             PET4 isPthreadsEnabled=0
20211015 115039.976 INFO             PET4 isOpenMPEnabled=1
20211015 115039.976 INFO             PET4 isOpenACCEnabled=0
20211015 115039.976 INFO             PET4 isSsiSharedMemoryEnabled=1
20211015 115039.976 INFO             PET4 ssiCount=1 peCount=6
20211015 115039.976 INFO             PET4 PE=0 SSI=0 SSIPE=0
20211015 115039.976 INFO             PET4 PE=1 SSI=0 SSIPE=1
20211015 115039.976 INFO             PET4 PE=2 SSI=0 SSIPE=2
20211015 115039.976 INFO             PET4 PE=3 SSI=0 SSIPE=3
20211015 115039.976 INFO             PET4 PE=4 SSI=0 SSIPE=4
20211015 115039.976 INFO             PET4 PE=5 SSI=0 SSIPE=5
20211015 115039.976 INFO             PET4 --- VMK::logSystem() MPI Control Variables ---------------
20211015 115039.976 INFO             PET4 index=   0                          MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the short message algorithm will be used if the send buffer size is < this value (in bytes). (See also: MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE)
20211015 115039.976 INFO             PET4 index=   1                           MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the long message algorithm will be used if the send buffer size is >= this value (in bytes) (See also: MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE)
20211015 115039.976 INFO             PET4 index=   2                         MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM : Variable to select allgather algorithmauto               - Internal algorithm selectionbrucks             - Force brucks algorithmnb                 - Force nonblocking algorithmrecursive_doubling - Force recursive doubling algorithmring               - Force ring algorithm
20211015 115039.976 INFO             PET4 index=   3                         MPIR_CVAR_ALLGATHER_INTER_ALGORITHM : Variable to select allgather algorithmauto                      - Internal algorithm selectionlocal_gather_remote_bcast - Force local-gather-remote-bcast algorithmnb                        - Force nonblocking algorithm
20211015 115039.976 INFO             PET4 index=   4                       MPIR_CVAR_ALLGATHER_DEVICE_COLLECTIVE : If set to true, MPI_Allgather will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level allgather function will not be called.
20211015 115039.976 INFO             PET4 index=   5                      MPIR_CVAR_ALLGATHERV_PIPELINE_MSG_SIZE : The smallest message size that will be used for the pipelined, large-message, ring algorithm in the MPI_Allgatherv implementation.
20211015 115039.976 INFO             PET4 index=   6                        MPIR_CVAR_ALLGATHERV_INTRA_ALGORITHM : Variable to select allgatherv algorithmauto               - Internal algorithm selectionbrucks             - Force brucks algorithmnb                 - Force nonblocking algorithmrecursive_doubling - Force recursive doubling algorithmring               - Force ring algorithm
20211015 115039.976 INFO             PET4 index=   7                        MPIR_CVAR_ALLGATHERV_INTER_ALGORITHM : Variable to select allgatherv algorithmauto                      - Internal algorithm selectionnb                        - Force nonblocking algorithmremote_gather_local_bcast - Force remote-gather-local-bcast algorithm
20211015 115039.976 INFO             PET4 index=   8                      MPIR_CVAR_ALLGATHERV_DEVICE_COLLECTIVE : If set to true, MPI_Allgatherv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level allgatherv function will not be called.
20211015 115039.976 INFO             PET4 index=   9                          MPIR_CVAR_ALLREDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20211015 115039.976 INFO             PET4 index=  10                            MPIR_CVAR_ENABLE_SMP_COLLECTIVES : Enable SMP aware collective communication.
20211015 115039.976 INFO             PET4 index=  11                              MPIR_CVAR_ENABLE_SMP_ALLREDUCE : Enable SMP aware allreduce.
20211015 115039.976 INFO             PET4 index=  12                        MPIR_CVAR_MAX_SMP_ALLREDUCE_MSG_SIZE : Maximum message size for which SMP-aware allreduce is used.  A value of '0' uses SMP-aware allreduce for all message sizes.
20211015 115039.976 INFO             PET4 index=  13                         MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM : Variable to select allreduce algorithmauto                     - Internal algorithm selectionnb                       - Force nonblocking algorithmrecursive_doubling       - Force recursive doubling algorithmreduce_scatter_allgather - Force reduce scatter allgather algorithm
20211015 115039.976 INFO             PET4 index=  14                         MPIR_CVAR_ALLREDUCE_INTER_ALGORITHM : Variable to select allreduce algorithmauto                  - Internal algorithm selectionnb                    - Force nonblocking algorithmreduce_exchange_bcast - Force reduce-exchange-bcast algorithm
20211015 115039.976 INFO             PET4 index=  15                       MPIR_CVAR_ALLREDUCE_DEVICE_COLLECTIVE : If set to true, MPI_Allreduce will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level allreduce function will not be called.
20211015 115039.976 INFO             PET4 index=  16                           MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE : the short message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value (See also: MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE)
20211015 115039.976 INFO             PET4 index=  17                          MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE : the medium message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value and larger than MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE (See also: MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE)
20211015 115039.976 INFO             PET4 index=  18                                 MPIR_CVAR_ALLTOALL_THROTTLE : max no. of irecvs/isends posted at a time in some alltoall algorithms. Setting it to 0 causes all irecvs/isends to be posted at once
20211015 115039.976 INFO             PET4 index=  19                          MPIR_CVAR_ALLTOALL_INTRA_ALGORITHM : Variable to select alltoall algorithmauto                      - Internal algorithm selectionbrucks                    - Force brucks algorithmnb                        - Force nonblocking algorithmpairwise                  - Force pairwise algorithmpairwise_sendrecv_replace - Force pairwise sendrecv replace algorithmscattered                 - Force scattered algorithm
20211015 115039.977 INFO             PET4 index=  20                          MPIR_CVAR_ALLTOALL_INTER_ALGORITHM : Variable to select alltoall algorithmauto              - Internal algorithm selectionnb                - Force nonblocking algorithmpairwise_exchange - Force pairwise exchange algorithm
20211015 115039.977 INFO             PET4 index=  21                        MPIR_CVAR_ALLTOALL_DEVICE_COLLECTIVE : If set to true, MPI_Alltoall will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level alltoall function will not be called.
20211015 115039.977 INFO             PET4 index=  22                         MPIR_CVAR_ALLTOALLV_INTRA_ALGORITHM : Variable to select alltoallv algorithmauto                      - Internal algorithm selectionnb                        - Force nonblocking algorithmpairwise_sendrecv_replace - Force pairwise_sendrecv_replace algorithmscattered                 - Force scattered algorithm
20211015 115039.977 INFO             PET4 index=  23                         MPIR_CVAR_ALLTOALLV_INTER_ALGORITHM : Variable to select alltoallv algorithmauto              - Internal algorithm selectionpairwise_exchange - Force pairwise exchange algorithmnb                - Force nonblocking algorithm
20211015 115039.977 INFO             PET4 index=  24                       MPIR_CVAR_ALLTOALLV_DEVICE_COLLECTIVE : If set to true, MPI_Alltoallv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level alltoallv function will not be called.
20211015 115039.977 INFO             PET4 index=  25                         MPIR_CVAR_ALLTOALLW_INTRA_ALGORITHM : Variable to select alltoallw algorithmauto                      - Internal algorithm selectionnb                        - Force nonblocking algorithmpairwise_sendrecv_replace - Force pairwise sendrecv replace algorithmscattered                 - Force scattered algorithm
20211015 115039.977 INFO             PET4 index=  26                         MPIR_CVAR_ALLTOALLW_INTER_ALGORITHM : Variable to select alltoallw algorithmauto              - Internal algorithm selectionnb                - Force nonblocking algorithmpairwise_exchange - Force pairwise exchange algorithm
20211015 115039.977 INFO             PET4 index=  27                       MPIR_CVAR_ALLTOALLW_DEVICE_COLLECTIVE : If set to true, MPI_Alltoallw will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level alltoallw function will not be called.
20211015 115039.977 INFO             PET4 index=  28                                MPIR_CVAR_ENABLE_SMP_BARRIER : Enable SMP aware barrier.
20211015 115039.977 INFO             PET4 index=  29                           MPIR_CVAR_BARRIER_INTRA_ALGORITHM : Variable to select barrier algorithmauto               - Internal algorithm selectionnb                 - Force nonblocking algorithmrecursive_doubling - Force recursive doubling algorithm
20211015 115039.977 INFO             PET4 index=  30                           MPIR_CVAR_BARRIER_INTER_ALGORITHM : Variable to select barrier algorithmauto  - Internal algorithm selectionbcast - Force bcast algorithmnb    - Force nonblocking algorithm
20211015 115039.977 INFO             PET4 index=  31                         MPIR_CVAR_BARRIER_DEVICE_COLLECTIVE : If set to true, MPI_Barrier will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level barrier function will not be called.
20211015 115039.977 INFO             PET4 index=  32                                   MPIR_CVAR_BCAST_MIN_PROCS : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_SHORT_MSG_SIZE, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20211015 115039.977 INFO             PET4 index=  33                              MPIR_CVAR_BCAST_SHORT_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20211015 115039.977 INFO             PET4 index=  34                               MPIR_CVAR_BCAST_LONG_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_SHORT_MSG_SIZE)
20211015 115039.977 INFO             PET4 index=  35                                  MPIR_CVAR_ENABLE_SMP_BCAST : Enable SMP aware broadcast (See also: MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE)
20211015 115039.977 INFO             PET4 index=  36                            MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE : Maximum message size for which SMP-aware broadcast is used.  A value of '0' uses SMP-aware broadcast for all message sizes. (See also: MPIR_CVAR_ENABLE_SMP_BCAST)
20211015 115039.977 INFO             PET4 index=  37                             MPIR_CVAR_BCAST_INTRA_ALGORITHM : Variable to select bcast algorithmauto                                    - Internal algorithm selectionbinomial                                - Force Binomial Treenb                                      - Force nonblocking algorithmscatter_recursive_doubling_allgather    - Force Scatter Recursive-Doubling Allgatherscatter_ring_allgather                  - Force Scatter Ring
20211015 115039.977 INFO             PET4 index=  38                             MPIR_CVAR_BCAST_INTER_ALGORITHM : Variable to select bcast algorithmauto                    - Internal algorithm selectionnb                      - Force nonblocking algorithmremote_send_local_bcast - Force remote-send-local-bcast algorithm
20211015 115039.977 INFO             PET4 index=  39                           MPIR_CVAR_BCAST_DEVICE_COLLECTIVE : If set to true, MPI_Bcast will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually.  If set to false, the device-level bcast function will not be called.
20211015 115039.977 INFO             PET4 index=  40                            MPIR_CVAR_EXSCAN_INTRA_ALGORITHM : Variable to select allgather algorithmauto               - Internal algorithm selectionnb                 - Force nonblocking algorithmrecursive_doubling - Force recursive doubling algorithm
20211015 115039.977 INFO             PET4 index=  41                          MPIR_CVAR_EXSCAN_DEVICE_COLLECTIVE : If set to true, MPI_Exscan will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level exscan function will not be called.
20211015 115039.977 INFO             PET4 index=  42                       MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_VSMALL_MSG_SIZE)
20211015 115039.977 INFO             PET4 index=  43                            MPIR_CVAR_GATHER_INTRA_ALGORITHM : Variable to select gather algorithmauto     - Internal algorithm selectionbinomial - Force binomial algorithmnb       - Force nonblocking algorithm
20211015 115039.977 INFO             PET4 index=  44                            MPIR_CVAR_GATHER_INTER_ALGORITHM : Variable to select gather algorithmauto                     - Internal algorithm selectionlinear                   - Force linear algorithmlocal_gather_remote_send - Force local-gather-remote-send algorithmnb                       - Force nonblocking algorithm
20211015 115039.977 INFO             PET4 index=  45                          MPIR_CVAR_GATHER_DEVICE_COLLECTIVE : If set to true, MPI_Gather will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level gather function will not be called.
20211015 115039.977 INFO             PET4 index=  46                            MPIR_CVAR_GATHER_VSMALL_MSG_SIZE : use a temporary buffer for intracommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE)
20211015 115039.977 INFO             PET4 index=  47                           MPIR_CVAR_GATHERV_INTRA_ALGORITHM : Variable to select gatherv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithmnb     - Force nonblocking algorithm
20211015 115039.977 INFO             PET4 index=  48                           MPIR_CVAR_GATHERV_INTER_ALGORITHM : Variable to select gatherv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithmnb     - Force nonblocking algorithm
20211015 115039.977 INFO             PET4 index=  49                         MPIR_CVAR_GATHERV_DEVICE_COLLECTIVE : If set to true, MPI_Gatherv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level gatherv function will not be called.
20211015 115039.977 INFO             PET4 index=  50                     MPIR_CVAR_GATHERV_INTER_SSEND_MIN_PROCS : Use Ssend (synchronous send) for intercommunicator MPI_Gatherv if the "group B" size is >= this value.  Specifying "-1" always avoids using Ssend.  For backwards compatibility, specifying "0" uses the default value.
20211015 115039.977 INFO             PET4 index=  51                           MPIR_CVAR_IALLGATHER_RECEXCH_KVAL : k value for recursive exchange based iallgather
20211015 115039.977 INFO             PET4 index=  52                            MPIR_CVAR_IALLGATHER_BRUCKS_KVAL : k value for radix in brucks based iallgather
20211015 115039.977 INFO             PET4 index=  53                        MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM : Variable to select iallgather algorithmauto               - Internal algorithm selectionbrucks             - Force brucks algorithmrecursive_doubling - Force recursive doubling algorithmring               - Force ring algorithmrecexch_distance_doubling    - Force generic transport recursive exchange with neighbours doubling in distance in each phaserecexch_distance_halving  - Force generic transport recursive exchange with neighbours halving in distance in each phasegentran_brucks     - Force generic transport based brucks algorithm
20211015 115039.977 INFO             PET4 index=  54                        MPIR_CVAR_IALLGATHER_INTER_ALGORITHM : Variable to select iallgather algorithmauto                      - Internal algorithm selectionlocal_gather_remote_bcast - Force local-gather-remote-bcast algorithm
20211015 115039.977 INFO             PET4 index=  55                      MPIR_CVAR_IALLGATHER_DEVICE_COLLECTIVE : If set to true, MPI_Iallgather will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iallgather function will not be called.
20211015 115039.977 INFO             PET4 index=  56                          MPIR_CVAR_IALLGATHERV_RECEXCH_KVAL : k value for recursive exchange based iallgatherv
20211015 115039.977 INFO             PET4 index=  57                       MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM : Variable to select iallgatherv algorithmauto               - Internal algorithm selectionbrucks             - Force brucks algorithmrecursive_doubling - Force recursive doubling algorithmring               - Force ring algorithmrecexch_distance_doubling    - Force generic transport recursive exchange with neighbours doubling in distance in each phaserecexch_distance_halving     - Force generic transport recursive exchange with neighbours halving in distance in each phasegentran_ring              - Force generic transport ring algorithm
20211015 115039.977 INFO             PET4 index=  58                       MPIR_CVAR_IALLGATHERV_INTER_ALGORITHM : Variable to select iallgatherv algorithmauto                      - Internal algorithm selectionremote_gather_local_bcast - Force remote-gather-local-bcast algorithm
20211015 115039.977 INFO             PET4 index=  59                     MPIR_CVAR_IALLGATHERV_DEVICE_COLLECTIVE : If set to true, MPI_Iallgatherv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iallgatherv function will not be called.
20211015 115039.977 INFO             PET4 index=  60                              MPIR_CVAR_IALLREDUCE_TREE_KVAL : k value for tree based iallreduce (for tree_kary and tree_knomial)
20211015 115039.977 INFO             PET4 index=  61               MPIR_CVAR_IALLREDUCE_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based iallreduce (tree_kary and tree_knomial). Default value is 0, that is, no pipelining by default
20211015 115039.977 INFO             PET4 index=  62                  MPIR_CVAR_IALLREDUCE_TREE_BUFFER_PER_CHILD : If set to true, a rank in tree_kary and tree_knomial algorithms will allocate a dedicated buffer for every child it receives data from. This would mean more memory consumption but it would allow preposting of the receives and hence reduce the number of unexpected messages. If set to false, there is only one buffer that is used to receive the data from all the children. The receives are therefore serialized, that is, only one receive can be posted at a time.
20211015 115039.977 INFO             PET4 index=  63                           MPIR_CVAR_IALLREDUCE_RECEXCH_KVAL : k value for recursive exchange based iallreduce
20211015 115039.977 INFO             PET4 index=  64                        MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM : Variable to select iallreduce algorithmauto                     - Internal algorithm selectionnaive                    - Force naive algorithmrecursive_doubling       - Force recursive doubling algorithmreduce_scatter_allgather - Force reduce scatter allgather algorithmrecexch_single_buffer    - Force generic transport recursive exchange with single buffer for receivesrecexch_multiple_buffer  - Force generic transport recursive exchange with multiple buffers for receives
20211015 115039.977 INFO             PET4 index=  65                        MPIR_CVAR_IALLREDUCE_INTER_ALGORITHM : Variable to select iallreduce algorithmauto                      - Internal algorithm selectionremote_reduce_local_bcast - Force remote-reduce-local-bcast algorithm
20211015 115039.977 INFO             PET4 index=  66                      MPIR_CVAR_IALLREDUCE_DEVICE_COLLECTIVE : If set to true, MPI_Iallreduce will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iallreduce function will not be called.
20211015 115039.977 INFO             PET4 index=  67                         MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM : Variable to select ialltoall algorithmauto              - Internal algorithm selectionbrucks            - Force brucks algorithminplace           - Force inplace algorithmpairwise          - Force pairwise algorithmpermuted_sendrecv - Force permuted sendrecv algorithm
20211015 115039.977 INFO             PET4 index=  68                         MPIR_CVAR_IALLTOALL_INTER_ALGORITHM : Variable to select ialltoall algorithmauto              - Internal algorithm selectionpairwise_exchange - Force pairwise exchange algorithm
20211015 115039.977 INFO             PET4 index=  69                       MPIR_CVAR_IALLTOALL_DEVICE_COLLECTIVE : If set to true, MPI_Ialltoall will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ialltoall function will not be called.
20211015 115039.977 INFO             PET4 index=  70                        MPIR_CVAR_IALLTOALLV_INTRA_ALGORITHM : Variable to select ialltoallv algorithmauto              - Internal algorithm selectionblocked           - Force blocked algorithminplace           - Force inplace algorithmpairwise_exchange - Force pairwise exchange algorithm
20211015 115039.977 INFO             PET4 index=  71                        MPIR_CVAR_IALLTOALLV_INTER_ALGORITHM : Variable to select ialltoallv algorithmauto - Internal algorithm selection
20211015 115039.977 INFO             PET4 index=  72                      MPIR_CVAR_IALLTOALLV_DEVICE_COLLECTIVE : If set to true, MPI_Ialltoallv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ialltoallv function will not be called.
20211015 115039.977 INFO             PET4 index=  73                        MPIR_CVAR_IALLTOALLW_INTRA_ALGORITHM : Variable to select ialltoallw algorithmauto              - Internal algorithm selectionblocked           - Force blocked algorithminplace           - Force inplace algorithmpairwise_exchange - Force pairwise exchange algorithm
20211015 115039.977 INFO             PET4 index=  74                        MPIR_CVAR_IALLTOALLW_INTER_ALGORITHM : Variable to select ialltoallw algorithmauto - Internal algorithm selection
20211015 115039.977 INFO             PET4 index=  75                      MPIR_CVAR_IALLTOALLW_DEVICE_COLLECTIVE : If set to true, MPI_Ialltoallw will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ialltoallw function will not be called.
20211015 115039.977 INFO             PET4 index=  76                             MPIR_CVAR_IBARRIER_RECEXCH_KVAL : k value for recursive exchange based ibarrier
20211015 115039.977 INFO             PET4 index=  77                          MPIR_CVAR_IBARRIER_INTRA_ALGORITHM : Variable to select ibarrier algorithmauto               - Internal algorithm selectionrecursive_doubling - Force recursive doubling algorithmrecexch            - Force generic transport based recursive exchange algorithm
20211015 115039.977 INFO             PET4 index=  78                          MPIR_CVAR_IBARRIER_INTER_ALGORITHM : Variable to select ibarrier algorithmauto  - Internal algorithm selectionbcast - Force bcast algorithm
20211015 115039.977 INFO             PET4 index=  79                        MPIR_CVAR_IBARRIER_DEVICE_COLLECTIVE : If set to true, MPI_Ibarrier will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ibarrier function will not be called.
20211015 115039.977 INFO             PET4 index=  80                                  MPIR_CVAR_IBCAST_TREE_KVAL : k value for tree (kary, knomial, etc.) based ibcast
20211015 115039.977 INFO             PET4 index=  81                                  MPIR_CVAR_IBCAST_TREE_TYPE : Tree type for tree based ibcast kary      - kary tree type knomial_1 - knomial_1 tree type knomial_2 - knomial_2 tree type
20211015 115039.977 INFO             PET4 index=  82                   MPIR_CVAR_IBCAST_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based ibcast. Default value is 0, that is, no pipelining by default
20211015 115039.977 INFO             PET4 index=  83                            MPIR_CVAR_IBCAST_RING_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in ibcast ring algorithm. Default value is 0, that is, no pipelining by default
20211015 115039.977 INFO             PET4 index=  84                            MPIR_CVAR_IBCAST_INTRA_ALGORITHM : Variable to select ibcast algorithmauto                                 - Internal algorithm selectionbinomial                             - Force Binomial algorithmscatter_recursive_doubling_allgather - Force Scatter Recursive Doubling Allgather algorithmscatter_ring_allgather               - Force Scatter Ring Allgather algorithmtree                                 - Force Generic Transport Tree algorithmscatter_recexch_allgather            - Force Generic Transport Scatter followed by Recursive Exchange Allgather algorithmring                                 - Force Generic Transport Ring algorithm
20211015 115039.977 INFO             PET4 index=  85                               MPIR_CVAR_IBCAST_SCATTER_KVAL : k value for tree based scatter in scatter_recexch_allgather algorithm
20211015 115039.977 INFO             PET4 index=  86                     MPIR_CVAR_IBCAST_ALLGATHER_RECEXCH_KVAL : k value for recursive exchange based allgather in scatter_recexch_allgather algorithm
20211015 115039.977 INFO             PET4 index=  87                            MPIR_CVAR_IBCAST_INTER_ALGORITHM : Variable to select ibcast algorithmauto - Internal algorithm selectionflat - Force flat algorithm
20211015 115039.977 INFO             PET4 index=  88                          MPIR_CVAR_IBCAST_DEVICE_COLLECTIVE : If set to true, MPI_Ibcast will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually.  If set to false, the device-level ibcast function will not be called.
20211015 115039.978 INFO             PET4 index=  89                           MPIR_CVAR_IEXSCAN_INTRA_ALGORITHM : Variable to select iexscan algorithmauto               - Internal algorithm selectionrecursive_doubling - Force recursive doubling algorithm
20211015 115039.978 INFO             PET4 index=  90                         MPIR_CVAR_IEXSCAN_DEVICE_COLLECTIVE : If set to true, MPI_Iexscan will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iexscan function will not be called.
20211015 115039.978 INFO             PET4 index=  91                           MPIR_CVAR_IGATHER_INTRA_ALGORITHM : Variable to select igather algorithmauto     - Internal algorithm selectionbinomial - Force binomial algorithmtree     - Force genetric transport based tree algorithm
20211015 115039.978 INFO             PET4 index=  92                                 MPIR_CVAR_IGATHER_TREE_KVAL : k value for tree based igather
20211015 115039.978 INFO             PET4 index=  93                           MPIR_CVAR_IGATHER_INTER_ALGORITHM : Variable to select igather algorithmauto        - Internal algorithm selectionlong_inter  - Force long inter algorithmshort_inter - Force short inter algorithm
20211015 115039.978 INFO             PET4 index=  94                         MPIR_CVAR_IGATHER_DEVICE_COLLECTIVE : If set to true, MPI_Igather will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level igather function will not be called.
20211015 115039.978 INFO             PET4 index=  95                          MPIR_CVAR_IGATHERV_INTRA_ALGORITHM : Variable to select igatherv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET4 index=  96                          MPIR_CVAR_IGATHERV_INTER_ALGORITHM : Variable to select igatherv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET4 index=  97                        MPIR_CVAR_IGATHERV_DEVICE_COLLECTIVE : If set to true, MPI_Igatherv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level igatherv function will not be called.
20211015 115039.978 INFO             PET4 index=  98               MPIR_CVAR_INEIGHBOR_ALLGATHER_INTRA_ALGORITHM : Variable to select ineighbor_allgather algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET4 index=  99               MPIR_CVAR_INEIGHBOR_ALLGATHER_INTER_ALGORITHM : Variable to select ineighbor_allgather algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET4 index= 100             MPIR_CVAR_INEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE : If set to true, MPI_ineighbor_allgather will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ineighbor_allgather function will not be called.
20211015 115039.978 INFO             PET4 index= 101              MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTRA_ALGORITHM : Variable to select ineighbor_allgatherv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET4 index= 102              MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTER_ALGORITHM : Variable to select ineighbor_allgatherv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET4 index= 103            MPIR_CVAR_INEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE : If set to true, MPI_ineighbor_allgatherv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ineighbor_allgatherv function will not be called.
20211015 115039.978 INFO             PET4 index= 104                MPIR_CVAR_INEIGHBOR_ALLTOALL_INTRA_ALGORITHM : Variable to select ineighbor_alltoall algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET4 index= 105                MPIR_CVAR_INEIGHBOR_ALLTOALL_INTER_ALGORITHM : Variable to select ineighbor_alltoall algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET4 index= 106              MPIR_CVAR_INEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE : If set to true, MPI_ineighbor_alltoall will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ineighbor_alltoall function will not be called.
20211015 115039.978 INFO             PET4 index= 107               MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTRA_ALGORITHM : Variable to select ineighbor_alltoallv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET4 index= 108               MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTER_ALGORITHM : Variable to select ineighbor_alltoallv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET4 index= 109             MPIR_CVAR_INEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE : If set to true, MPI_ineighbor_alltoallv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ineighbor_alltoallv function will not be called.
20211015 115039.978 INFO             PET4 index= 110               MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTRA_ALGORITHM : Variable to select ineighbor_alltoallw algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET4 index= 111               MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTER_ALGORITHM : Variable to select ineighbor_alltoallw algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET4 index= 112             MPIR_CVAR_INEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE : If set to true, MPI_ineighbor_alltoallw will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ineighbor_alltoallw function will not be called.
20211015 115039.978 INFO             PET4 index= 113                                 MPIR_CVAR_IREDUCE_TREE_KVAL : k value for tree (kary, knomial, etc.) based ireduce
20211015 115039.978 INFO             PET4 index= 114                                 MPIR_CVAR_IREDUCE_TREE_TYPE : Tree type for tree based ireduce kary      - kary tree knomial_1 - knomial_1 tree knomial_2 - knomial_2 tree
20211015 115039.978 INFO             PET4 index= 115                  MPIR_CVAR_IREDUCE_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based ireduce. Default value is 0, that is, no pipelining by default
20211015 115039.978 INFO             PET4 index= 116                           MPIR_CVAR_IREDUCE_RING_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in ireduce ring algorithm. Default value is 0, that is, no pipelining by default
20211015 115039.978 INFO             PET4 index= 117                     MPIR_CVAR_IREDUCE_TREE_BUFFER_PER_CHILD : If set to true, a rank in tree algorithms will allocate a dedicated buffer for every child it receives data from. This would mean more memory consumption but it would allow preposting of the receives and hence reduce the number of unexpected messages. If set to false, there is only one buffer that is used to receive the data from all the children. The receives are therefore serialized, that is, only one receive can be posted at a time.
20211015 115039.978 INFO             PET4 index= 118                           MPIR_CVAR_IREDUCE_INTRA_ALGORITHM : Variable to select ireduce algorithmauto                  - Internal algorithm selectionbinomial              - Force binomial algorithmreduce_scatter_gather - Force reduce scatter gather algorithmtree                  - Force Generic Transport Treering                  - Force Generic Transport Ring
20211015 115039.978 INFO             PET4 index= 119                           MPIR_CVAR_IREDUCE_INTER_ALGORITHM : Variable to select ireduce algorithmauto                     - Internal algorithm selectionlocal_reduce_remote_send - Force local-reduce-remote-send algorithm
20211015 115039.978 INFO             PET4 index= 120                         MPIR_CVAR_IREDUCE_DEVICE_COLLECTIVE : If set to true, MPI_Ireduce will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ireduce function will not be called.
20211015 115039.978 INFO             PET4 index= 121                      MPIR_CVAR_IREDUCE_SCATTER_RECEXCH_KVAL : k value for recursive exchange based ireduce_scatter
20211015 115039.978 INFO             PET4 index= 122                   MPIR_CVAR_IREDUCE_SCATTER_INTRA_ALGORITHM : Variable to select ireduce_scatter algorithmauto               - Internal algorithm selectionnoncommutative     - Force noncommutative algorithmrecursive_doubling - Force recursive doubling algorithmpairwise           - Force pairwise algorithmrecursive_halving  - Force recursive halving algorithmrecexch  - Force generic transport recursive exchange algorithm
20211015 115039.978 INFO             PET4 index= 123                   MPIR_CVAR_IREDUCE_SCATTER_INTER_ALGORITHM : Variable to select ireduce_scatter algorithmauto                         - Internal algorithm selectionremote_reduce_local_scatterv - Force remote-reduce-local-scatterv algorithm
20211015 115039.978 INFO             PET4 index= 124                 MPIR_CVAR_IREDUCE_SCATTER_DEVICE_COLLECTIVE : If set to true, MPI_Ireduce_scatter will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ireduce_scatter function will not be called.
20211015 115039.978 INFO             PET4 index= 125                MPIR_CVAR_IREDUCE_SCATTER_BLOCK_RECEXCH_KVAL : k value for recursive exchange based ireduce_scatter_block
20211015 115039.978 INFO             PET4 index= 126             MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM : Variable to select ireduce_scatter_block algorithmauto               - Internal algorithm selectionnoncommutative     - Force noncommutative algorithmrecursive_doubling - Force recursive doubling algorithmpairwise           - Force pairwise algorithmrecursive_halving  - Force recursive halving algorithmrecexch  - Force generic transport recursive exchange algorithm
20211015 115039.978 INFO             PET4 index= 127             MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTER_ALGORITHM : Variable to select ireduce_scatter_block algorithmauto                         - Internal algorithm selectionremote_reduce_local_scatterv - Force remote-reduce-local-scatterv algorithm
20211015 115039.978 INFO             PET4 index= 128           MPIR_CVAR_IREDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE : If set to true, MPI_Ireduce_scatter_block will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ireduce_scatter_block function will not be called.
20211015 115039.978 INFO             PET4 index= 129                             MPIR_CVAR_ISCAN_INTRA_ALGORITHM : Variable to select allgather algorithmauto               - Internal algorithm selectionrecursive_doubling - Force recursive doubling algorithm
20211015 115039.978 INFO             PET4 index= 130                           MPIR_CVAR_ISCAN_DEVICE_COLLECTIVE : If set to true, MPI_Iscan will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iscan function will not be called.
20211015 115039.978 INFO             PET4 index= 131                          MPIR_CVAR_ISCATTER_INTRA_ALGORITHM : Variable to select iscatter algorithmauto     - Internal algorithm selectionbinomial - Force binomial algorithmtree     - Force genetric transport based tree algorithm
20211015 115039.978 INFO             PET4 index= 132                                MPIR_CVAR_ISCATTER_TREE_KVAL : k value for tree based iscatter
20211015 115039.978 INFO             PET4 index= 133                          MPIR_CVAR_ISCATTER_INTER_ALGORITHM : Variable to select iscatter algorithmauto                      - Internal algorithm selectionlinear                    - Force linear algorithmremote_send_local_scatter - Force remote-send-local-scatter algorithm
20211015 115039.978 INFO             PET4 index= 134                        MPIR_CVAR_ISCATTER_DEVICE_COLLECTIVE : If set to true, MPI_Iscatter will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iscatter function will not be called.
20211015 115039.978 INFO             PET4 index= 135                         MPIR_CVAR_ISCATTERV_INTRA_ALGORITHM : Variable to select iscatterv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET4 index= 136                         MPIR_CVAR_ISCATTERV_INTER_ALGORITHM : Variable to select iscatterv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET4 index= 137                       MPIR_CVAR_ISCATTERV_DEVICE_COLLECTIVE : If set to true, MPI_Iscatterv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iscatterv function will not be called.
20211015 115039.978 INFO             PET4 index= 138                MPIR_CVAR_NEIGHBOR_ALLGATHER_INTRA_ALGORITHM : Variable to select ineighbor_allgather algorithmauto - Internal algorithm selectionnb   - Force nonblocking algorithm
20211015 115039.978 INFO             PET4 index= 139                MPIR_CVAR_NEIGHBOR_ALLGATHER_INTER_ALGORITHM : Variable to select ineighbor_allgather algorithmauto - Internal algorithm selectionnb   - Force nonblocking algorithm
20211015 115039.978 INFO             PET4 index= 140              MPIR_CVAR_NEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE : If set to true, MPI_neighbor_allgather will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level neighbor_allgather function will not be called.
20211015 115039.978 INFO             PET4 index= 141               MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTRA_ALGORITHM : Variable to select neighbor_allgatherv algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET4 index= 142               MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTER_ALGORITHM : Variable to select neighbor_allgatherv algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET4 index= 143             MPIR_CVAR_NEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE : If set to true, MPI_neighbor_allgatherv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level neighbor_allgatherv function will not be called.
20211015 115039.978 INFO             PET4 index= 144                 MPIR_CVAR_NEIGHBOR_ALLTOALL_INTRA_ALGORITHM : Variable to select neighbor_alltoall algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET4 index= 145                 MPIR_CVAR_NEIGHBOR_ALLTOALL_INTER_ALGORITHM : Variable to select neighbor_alltoall algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET4 index= 146               MPIR_CVAR_NEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE : If set to true, MPI_neighbor_alltoall will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level neighbor_alltoall function will not be called.
20211015 115039.978 INFO             PET4 index= 147                MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTRA_ALGORITHM : Variable to select neighbor_alltoallv algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET4 index= 148                MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTER_ALGORITHM : Variable to select neighbor_alltoallv algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET4 index= 149              MPIR_CVAR_NEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE : If set to true, MPI_neighbor_alltoallv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level neighbor_alltoallv function will not be called.
20211015 115039.978 INFO             PET4 index= 150                MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTRA_ALGORITHM : Variable to select neighbor_alltoallw algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET4 index= 151                MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTER_ALGORITHM : Variable to select neighbor_alltoallw algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET4 index= 152              MPIR_CVAR_NEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE : If set to true, MPI_neighbor_alltoallw will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level neighbor_alltoallw function will not be called.
20211015 115039.978 INFO             PET4 index= 153                             MPIR_CVAR_REDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20211015 115039.978 INFO             PET4 index= 154                                 MPIR_CVAR_ENABLE_SMP_REDUCE : Enable SMP aware reduce.
20211015 115039.978 INFO             PET4 index= 155                           MPIR_CVAR_MAX_SMP_REDUCE_MSG_SIZE : Maximum message size for which SMP-aware reduce is used.  A value of '0' uses SMP-aware reduce for all message sizes.
20211015 115039.978 INFO             PET4 index= 156                            MPIR_CVAR_REDUCE_INTRA_ALGORITHM : Variable to select reduce algorithmauto                  - Internal algorithm selectionbinomial              - Force binomial algorithmnb                    - Force nonblocking algorithmreduce_scatter_gather - Force reduce scatter gather algorithm
20211015 115039.978 INFO             PET4 index= 157                            MPIR_CVAR_REDUCE_INTER_ALGORITHM : Variable to select reduce algorithmauto                     - Internal algorithm selectionlocal_reduce_remote_send - Force local-reduce-remote-send algorithmnb                       - Force nonblocking algorithm
20211015 115039.978 INFO             PET4 index= 158                          MPIR_CVAR_REDUCE_DEVICE_COLLECTIVE : If set to true, MPI_Reduce will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level reduce function will not be called.
20211015 115039.978 INFO             PET4 index= 159          MPIR_CVAR_REDUCE_SCATTER_COMMUTATIVE_LONG_MSG_SIZE : the long message algorithm will be used if the operation is commutative and the send buffer size is >= this value (in bytes)
20211015 115039.978 INFO             PET4 index= 160                    MPIR_CVAR_REDUCE_SCATTER_INTRA_ALGORITHM : Variable to select reduce_scatter algorithmauto               - Internal algorithm selectionnb                 - Force nonblocking algorithmnoncommutative     - Force noncommutative algorithmpairwise           - Force pairwise algorithmrecursive_doubling - Force recursive doubling algorithmrecursive_halving  - Force recursive halving algorithm
20211015 115039.978 INFO             PET4 index= 161                    MPIR_CVAR_REDUCE_SCATTER_INTER_ALGORITHM : Variable to select reduce_scatter algorithmauto                        - Internal algorithm selectionnb                          - Force nonblocking algorithmremote_reduce_local_scatter - Force remote-reduce-local-scatter algorithm
20211015 115039.978 INFO             PET4 index= 162                  MPIR_CVAR_REDUCE_SCATTER_DEVICE_COLLECTIVE : If set to true, MPI_Redscat will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level reduce_scatter function will not be called.
20211015 115039.978 INFO             PET4 index= 163              MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM : Variable to select reduce_scatter_block algorithmauto               - Internal algorithm selectionnoncommutative     - Force noncommutative algorithmrecursive_doubling - Force recursive doubling algorithmpairwise           - Force pairwise algorithmrecursive_halving  - Force recursive halving algorithmnb                 - Force nonblocking algorithm
20211015 115039.978 INFO             PET4 index= 164              MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTER_ALGORITHM : Variable to select reduce_scatter_block algorithmauto                        - Internal algorithm selectionnb                          - Force nonblocking algorithmremote_reduce_local_scatter - Force remote-reduce-local-scatter algorithm
20211015 115039.978 INFO             PET4 index= 165            MPIR_CVAR_REDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE : If set to true, MPI_Reduce_scatter_block will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level reduce_scatter_block function will not be called.
20211015 115039.978 INFO             PET4 index= 166                              MPIR_CVAR_SCAN_INTRA_ALGORITHM : Variable to select allgather algorithmauto               - Internal algorithm selectionnb                 - Force nonblocking algorithmrecursive_doubling - Force recursive doubling algorithm
20211015 115039.978 INFO             PET4 index= 167                            MPIR_CVAR_SCAN_DEVICE_COLLECTIVE : If set to true, MPI_Scan will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level scan function will not be called.
20211015 115039.978 INFO             PET4 index= 168                      MPIR_CVAR_SCATTER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Scatter if the send buffer size is < this value (in bytes)
20211015 115039.978 INFO             PET4 index= 169                           MPIR_CVAR_SCATTER_INTRA_ALGORITHM : Variable to select scatter algorithmauto     - Internal algorithm selectionbinomial - Force binomial algorithmnb       - Force nonblocking algorithm
20211015 115039.978 INFO             PET4 index= 170                           MPIR_CVAR_SCATTER_INTER_ALGORITHM : Variable to select scatter algorithmauto                      - Internal algorithm selectionlinear                    - Force linear algorithmnb                        - Force nonblocking algorithmremote_send_local_scatter - Force remote-send-local-scatter algorithm
20211015 115039.978 INFO             PET4 index= 171                         MPIR_CVAR_SCATTER_DEVICE_COLLECTIVE : If set to true, MPI_Scatter will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level scatter function will not be called.
20211015 115039.978 INFO             PET4 index= 172                          MPIR_CVAR_SCATTERV_INTRA_ALGORITHM : Variable to select scatterv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithmnb     - Force nonblocking algorithm
20211015 115039.978 INFO             PET4 index= 173                          MPIR_CVAR_SCATTERV_INTER_ALGORITHM : Variable to select scatterv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithmnb     - Force nonblocking algorithm
20211015 115039.978 INFO             PET4 index= 174                        MPIR_CVAR_SCATTERV_DEVICE_COLLECTIVE : If set to true, MPI_scatterv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level scatterv function will not be called.
20211015 115039.978 INFO             PET4 index= 175                                MPIR_CVAR_DEVICE_COLLECTIVES : If set to true, MPI collectives will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually.  If set to false, the device-level collective function will not be called.
20211015 115039.978 INFO             PET4 index= 176                                MPIR_CVAR_PROGRESS_MAX_COLLS : Maximum number of collective operations at a time that the progress engine should make progress on
20211015 115039.978 INFO             PET4 index= 177                              MPIR_CVAR_COMM_SPLIT_USE_QSORT : Use qsort(3) in the implementation of MPI_Comm_split instead of bubble sort.
20211015 115039.978 INFO             PET4 index= 178                                  MPIR_CVAR_CTXID_EAGER_SIZE : The MPIR_CVAR_CTXID_EAGER_SIZE environment variable allows you to specify how many words in the context ID mask will be set aside for the eager allocation protocol.  If the application is running out of context IDs, reducing this value may help.
20211015 115039.978 INFO             PET4 index= 179                                    MPIR_CVAR_PROCTABLE_SIZE : Size of the "MPIR" debugger interface proctable (process table).
20211015 115039.978 INFO             PET4 index= 180                                   MPIR_CVAR_PROCTABLE_PRINT : If true, dump the proctable entries at MPII_Wait_for_debugger-time.
20211015 115039.978 INFO             PET4 index= 181                                 MPIR_CVAR_PRINT_ERROR_STACK : If true, print an error stack trace at error handling time.
20211015 115039.978 INFO             PET4 index= 182                                  MPIR_CVAR_CHOP_ERROR_STACK : If >0, truncate error stack output lines this many characters wide.  If 0, do not truncate, and if <0 use a sensible default.
20211015 115039.978 INFO             PET4 index= 183                            MPIR_CVAR_SUPPRESS_ABORT_MESSAGE : Disable printing of abort error message.
20211015 115039.978 INFO             PET4 index= 184                                           MPIR_CVAR_MEMDUMP : If true, list any memory that was allocated by MPICH and that remains allocated when MPI_Finalize completes.
20211015 115039.978 INFO             PET4 index= 185                          MPIR_CVAR_MEM_CATEGORY_INFORMATION : If true, print a summary of memory allocation by category. The category definitions are found in mpl_trmem.h.
20211015 115039.979 INFO             PET4 index= 186                                    MPIR_CVAR_ASYNC_PROGRESS : If set to true, MPICH will initiate an additional thread to make asynchronous progress on all communication operations including point-to-point, collective, one-sided operations and I/O.  Setting this variable will automatically increase the thread-safety level to MPI_THREAD_MULTIPLE.  While this improves the progress semantics, it might cause a small amount of performance overhead for regular MPI operations.  The user is encouraged to leave one or more hardware threads vacant in order to prevent contention between the application threads and the progress thread(s).  The impact of oversubscription is highly system dependent but may be substantial in some cases, hence this recommendation.
20211015 115039.979 INFO             PET4 index= 187                              MPIR_CVAR_DEFAULT_THREAD_LEVEL : Sets the default thread level to use when using MPI_INIT. This variable is case-insensitive.
20211015 115039.979 INFO             PET4 index= 188                                        MPIR_CVAR_DEBUG_HOLD : If true, causes processes to wait in MPI_Init and MPI_Initthread for a debugger to be attached.  Once the debugger has attached, the variable 'hold' should be set to 0 in order to allow the process to continue (e.g., in gdb, "set hold=0").
20211015 115039.979 INFO             PET4 index= 189                                    MPIR_CVAR_ERROR_CHECKING : If true, perform checks for errors, typically to verify valid inputs to MPI routines.  Only effective when MPICH is configured with --enable-error-checking=runtime .
20211015 115039.979 INFO             PET4 index= 190                                  MPIR_CVAR_NETLOC_NODE_FILE : Subnet json file
20211015 115039.979 INFO             PET4 index= 191                                      MPIR_CVAR_DIMS_VERBOSE : If true, enable verbose output about the actions of the implementation of MPI_Dims_create.
20211015 115039.979 INFO             PET4 index= 192                              MPIR_CVAR_NAMESERV_FILE_PUBDIR : Sets the directory to use for MPI service publishing in the file nameserv implementation.  Allows the user to override where the publish and lookup information is placed for connect/accept based applications.
20211015 115039.979 INFO             PET4 index= 193                           MPIR_CVAR_ABORT_ON_LEAKED_HANDLES : If true, MPI will call MPI_Abort at MPI_Finalize if any MPI object handles have been leaked.  For example, if MPI_Comm_dup is called without calling a corresponding MPI_Comm_free.  For uninteresting reasons, enabling this option may prevent all known object leaks from being reported.  MPICH must have been configure with "--enable-g=handlealloc" or better in order for this functionality to work.
20211015 115039.979 INFO             PET4 index= 194                                           MPIR_CVAR_NOLOCAL : If true, force all processes to operate as though all processes are located on another node.  For example, this disables shared memory communication hierarchical collectives.
20211015 115039.979 INFO             PET4 index= 195                                  MPIR_CVAR_ODD_EVEN_CLIQUES : If true, odd procs on a node are seen as local to each other, and even procs on a node are seen as local to each other.  Used for debugging on a single machine. Deprecated in favor of MPIR_CVAR_NUM_CLIQUES.
20211015 115039.979 INFO             PET4 index= 196                                       MPIR_CVAR_NUM_CLIQUES : Specify the number of cliques that should be used to partition procs on a local node. Procs with the same clique number are seen as local to each other. Used for debugging on a single machine.
20211015 115039.979 INFO             PET4 index= 197                                  MPIR_CVAR_COLL_ALIAS_CHECK : Enable checking of aliasing in collective operations
20211015 115039.979 INFO             PET4 index= 198                                 MPIR_CVAR_REQUEST_POLL_FREQ : How frequent to poll during completion calls (wait/test) in terms of number of processed requests before polling.
20211015 115039.979 INFO             PET4 index= 199                                MPIR_CVAR_REQUEST_BATCH_SIZE : The number of requests to make completion as a batch in MPI_Waitall and MPI_Testall implementation. A large number is likely to cause more cache misses.
20211015 115039.979 INFO             PET4 index= 200                                MPIR_CVAR_POLLS_BEFORE_YIELD : When MPICH is in a busy waiting loop, it will periodically call a function to yield the processor.  This cvar sets the number of loops before the yield function is called.  A value of 0 disables yielding.
20211015 115039.979 INFO             PET4 index= 201                          MPIR_CVAR_NEMESIS_MXM_BULK_CONNECT : If true, force mxm to connect all processes at initialization time.
20211015 115039.979 INFO             PET4 index= 202                       MPIR_CVAR_NEMESIS_MXM_BULK_DISCONNECT : If true, force mxm to disconnect all processes at finalization time.
20211015 115039.979 INFO             PET4 index= 203                              MPIR_CVAR_NEMESIS_MXM_HUGEPAGE : If true, mxm tries detecting hugepage support.  On HPC-X 2.3 and earlier, this might cause problems on Ubuntu and other platforms even if the system provides hugepage support.
20211015 115039.979 INFO             PET4 index= 204                                  MPIR_CVAR_OFI_USE_PROVIDER : If non-null, choose an OFI provider by name. If using with the CH4 device and using a provider that supports an older version of the libfabric API then the default version of the installed library, specifying the OFI version via the appropriate CVARs is also recommended.
20211015 115039.979 INFO             PET4 index= 205                                MPIR_CVAR_OFI_DUMP_PROVIDERS : If true, dump provider information at init
20211015 115039.979 INFO             PET4 index= 206                            MPIR_CVAR_CH3_INTERFACE_HOSTNAME : If non-NULL, this cvar specifies the IP address that other processes should use when connecting to this process. This cvar is mutually exclusive with the MPIR_CVAR_CH3_NETWORK_IFACE cvar and it is an error to set them both.
20211015 115039.979 INFO             PET4 index= 207                                    MPIR_CVAR_CH3_PORT_RANGE : The MPIR_CVAR_CH3_PORT_RANGE environment variable allows you to specify the range of TCP ports to be used by the process manager and the MPICH library. The format of this variable is <low>:<high>.  To specify any available port, use 0:0.
20211015 115039.979 INFO             PET4 index= 208                         MPIR_CVAR_NEMESIS_TCP_NETWORK_IFACE : If non-NULL, this cvar specifies which pseudo-ethernet interface the tcp netmod should use (e.g., "eth1", "ib0"). Note, this is a Linux-specific cvar. This cvar is mutually exclusive with the MPIR_CVAR_CH3_INTERFACE_HOSTNAME cvar and it is an error to set them both.
20211015 115039.979 INFO             PET4 index= 209                   MPIR_CVAR_NEMESIS_TCP_HOST_LOOKUP_RETRIES : This cvar controls the number of times to retry the gethostbyname() function before giving up.
20211015 115039.979 INFO             PET4 index= 210                            MPIR_CVAR_NEMESIS_ENABLE_CKPOINT : If true, enables checkpointing support and returns an error if checkpointing library cannot be initialized.
20211015 115039.979 INFO             PET4 index= 211                          MPIR_CVAR_NEMESIS_SHM_EAGER_MAX_SZ : This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for shared memory. If this cvar is set to -1, then Nemesis will choose an appropriate value.
20211015 115039.979 INFO             PET4 index= 212                    MPIR_CVAR_NEMESIS_SHM_READY_EAGER_MAX_SZ : This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for ready-send messages.  If this cvar is set to -1, then ready messages will always be sent eagerly.  If this cvar is set to -2, then Nemesis will choose an appropriate value.
20211015 115039.979 INFO             PET4 index= 213                                         MPIR_CVAR_ENABLE_FT : Enable fault tolerance functions
20211015 115039.979 INFO             PET4 index= 214                         MPIR_CVAR_NEMESIS_LMT_DMA_THRESHOLD : Messages larger than this size will use the "dma" (knem) intranode LMT implementation, if it is enabled and available.
20211015 115039.979 INFO             PET4 index= 215                                    MPIR_CVAR_NEMESIS_NETMOD : If non-empty, this cvar specifies which network module should be used for communication. This variable is case-insensitive.
20211015 115039.979 INFO             PET4 index= 216                                  MPIR_CVAR_CH3_ENABLE_HCOLL : If true, enable HCOLL collectives.
20211015 115039.979 INFO             PET4 index= 217                          MPIR_CVAR_CH3_COMM_CONNECT_TIMEOUT : The default time out period in seconds for a connection attempt to the server communicator where the named port exists but no pending accept. User can change the value for a specified connection through its info argument.
20211015 115039.979 INFO             PET4 index= 218               MPIR_CVAR_CH3_RMA_OP_PIGGYBACK_LOCK_DATA_SIZE : Specify the threshold of data size of a RMA operation which can be piggybacked with a LOCK message. It is always a positive value and should not be smaller than MPIDI_RMA_IMMED_BYTES. If user sets it as a small value, for middle and large data size, we will lose performance because of always waiting for round-trip of LOCK synchronization; if user sets it as a large value, we need to consume more memory on target side to buffer this lock request when lock is not satisfied.
20211015 115039.979 INFO             PET4 index= 219                      MPIR_CVAR_CH3_RMA_ACTIVE_REQ_THRESHOLD : Threshold of number of active requests to trigger blocking waiting in operation routines. When the value is negative, we never blockingly wait in operation routines. When the value is zero, we always trigger blocking waiting in operation routines to wait until no. of active requests becomes zero. When the value is positive, we do blocking waiting in operation routines to wait until no. of active requests being reduced to this value.
20211015 115039.979 INFO             PET4 index= 220               MPIR_CVAR_CH3_RMA_POKE_PROGRESS_REQ_THRESHOLD : Threshold at which the RMA implementation attempts to complete requests while completing RMA operations and while using the lazy synchonization approach.  Change this value if programs fail because they run out of requests or other internal resources
20211015 115039.979 INFO             PET4 index= 221                MPIR_CVAR_CH3_RMA_SCALABLE_FENCE_PROCESS_NUM : Specify the threshold of switching the algorithm used in FENCE from the basic algorithm to the scalable algorithm. The value can be nagative, zero or positive. When the number of processes is larger than or equal to this value, FENCE will use a scalable algorithm which do not use O(P) data structure; when the number of processes is smaller than the value, FENCE will use a basic but fast algorithm which requires an O(P) data structure.
20211015 115039.979 INFO             PET4 index= 222            MPIR_CVAR_CH3_RMA_DELAY_ISSUING_FOR_PIGGYBACKING : Specify if delay issuing of RMA operations for piggybacking LOCK/UNLOCK/FLUSH is enabled. It can be either 0 or 1. When it is set to 1, the issuing of LOCK message is delayed until origin process see the first RMA operation and piggyback LOCK with that operation, and the origin process always keeps the current last operation until the ending synchronization call in order to piggyback UNLOCK/FLUSH with that operation. When it is set to 0, in WIN_LOCK/UNLOCK case, the LOCK message is sent out as early as possible, in WIN_LOCK_ALL/UNLOCK_ALL case, the origin process still tries to piggyback LOCK message with the first operation; for UNLOCK/FLUSH message, the origin process no longer keeps the current last operation but only piggyback UNLOCK/FLUSH if there is an operation avaliable in the ending synchronization call.
20211015 115039.979 INFO             PET4 index= 223                                MPIR_CVAR_CH3_RMA_SLOTS_SIZE : Number of RMA slots during window creation. Each slot contains a linked list of target elements. The distribution of ranks among slots follows a round-robin pattern. Requires a positive value.
20211015 115039.979 INFO             PET4 index= 224                    MPIR_CVAR_CH3_RMA_TARGET_LOCK_DATA_BYTES : Size (in bytes) of available lock data this window can provided. If current buffered lock data is more than this value, the process will drop the upcoming operation data. Requires a positive calue.
20211015 115039.979 INFO             PET4 index= 225                            MPIR_CVAR_CH3_EAGER_MAX_MSG_SIZE : This cvar controls the message size at which CH3 switches from eager to rendezvous mode.
20211015 115039.979 INFO             PET4 index= 226                          MPIR_CVAR_CH3_RMA_OP_WIN_POOL_SIZE : Size of the window-private RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediately.  Requires a positive value.
20211015 115039.979 INFO             PET4 index= 227                       MPIR_CVAR_CH3_RMA_OP_GLOBAL_POOL_SIZE : Size of the Global RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediatly.  Requires a positive value.
20211015 115039.979 INFO             PET4 index= 228                      MPIR_CVAR_CH3_RMA_TARGET_WIN_POOL_SIZE : Size of the window-private RMA target pool (in number of targets) that stores information about RMA targets that could not be issued immediately.  Requires a positive value.
20211015 115039.979 INFO             PET4 index= 229                   MPIR_CVAR_CH3_RMA_TARGET_GLOBAL_POOL_SIZE : Size of the Global RMA targets pool (in number of targets) that stores information about RMA targets that could not be issued immediatly.  Requires a positive value.
20211015 115039.979 INFO             PET4 index= 230           MPIR_CVAR_CH3_RMA_TARGET_LOCK_ENTRY_WIN_POOL_SIZE : Size of the window-private RMA lock entries pool (in number of lock entries) that stores information about RMA lock requests that could not be satisfied immediatly.  Requires a positive value.
20211015 115039.979 INFO             PET4 index= 231                     MPIR_CVAR_CH4_OFI_CAPABILITY_SETS_DEBUG : Prints out the configuration of each capability selected via the capability sets interface.
20211015 115039.979 INFO             PET4 index= 232                               MPIR_CVAR_CH4_OFI_ENABLE_DATA : Enable immediate data fields in OFI to transmit source rank outside of the match bits
20211015 115039.979 INFO             PET4 index= 233                           MPIR_CVAR_CH4_OFI_ENABLE_AV_TABLE : If true, the OFI addressing information will be stored with an FI_AV_TABLE. If false, an FI_AV_MAP will be used.
20211015 115039.979 INFO             PET4 index= 234                 MPIR_CVAR_CH4_OFI_ENABLE_SCALABLE_ENDPOINTS : If true, use OFI scalable endpoints.
20211015 115039.979 INFO             PET4 index= 235                    MPIR_CVAR_CH4_OFI_ENABLE_SHARED_CONTEXTS : If set to false (zero), MPICH does not use OFI shared contexts. If set to -1, it is determined by the OFI capability sets based on the provider. Otherwise, MPICH tries to use OFI shared contexts. If they are unavailable, it'll fall back to the mode without shared contexts.
20211015 115039.979 INFO             PET4 index= 236                        MPIR_CVAR_CH4_OFI_ENABLE_MR_SCALABLE : If true, MR_SCALABLE for OFI memory regions. If false, MR_BASIC for OFI memory regions.
20211015 115039.979 INFO             PET4 index= 237                             MPIR_CVAR_CH4_OFI_ENABLE_TAGGED : If true, use tagged message transmission functions in OFI.
20211015 115039.979 INFO             PET4 index= 238                                 MPIR_CVAR_CH4_OFI_ENABLE_AM : If true, enable OFI active message support.
20211015 115039.979 INFO             PET4 index= 239                                MPIR_CVAR_CH4_OFI_ENABLE_RMA : If true, enable OFI RMA support.
20211015 115039.979 INFO             PET4 index= 240                            MPIR_CVAR_CH4_OFI_ENABLE_ATOMICS : If true, enable OFI Atomics support.
20211015 115039.979 INFO             PET4 index= 241                       MPIR_CVAR_CH4_OFI_FETCH_ATOMIC_IOVECS : Specifies the maximum number of iovecs that can be used by the OFI provider for fetch_atomic operations. The default value is -1, indicating that no value is set.
20211015 115039.979 INFO             PET4 index= 242                 MPIR_CVAR_CH4_OFI_ENABLE_DATA_AUTO_PROGRESS : If true, enable MPI data auto progress.
20211015 115039.979 INFO             PET4 index= 243              MPIR_CVAR_CH4_OFI_ENABLE_CONTROL_AUTO_PROGRESS : If true, enable MPI control auto progress.
20211015 115039.979 INFO             PET4 index= 244                       MPIR_CVAR_CH4_OFI_ENABLE_PT2PT_NOPACK : If true, enable iovec for pt2pt.
20211015 115039.979 INFO             PET4 index= 245                           MPIR_CVAR_CH4_OFI_CONTEXT_ID_BITS : Specifies the number of bits that will be used for matching the context ID. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20211015 115039.979 INFO             PET4 index= 246                                 MPIR_CVAR_CH4_OFI_RANK_BITS : Specifies the number of bits that will be used for matching the MPI rank. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20211015 115039.979 INFO             PET4 index= 247                                  MPIR_CVAR_CH4_OFI_TAG_BITS : Specifies the number of bits that will be used for matching the user tag. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20211015 115039.979 INFO             PET4 index= 248                             MPIR_CVAR_CH4_OFI_MAJOR_VERSION : Specifies the major version of the OFI library. The default is the major version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
20211015 115039.979 INFO             PET4 index= 249                             MPIR_CVAR_CH4_OFI_MINOR_VERSION : Specifies the major version of the OFI library. The default is the minor version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
20211015 115039.979 INFO             PET4 index= 250                                  MPIR_CVAR_CH4_OFI_MAX_VNIS : If set to positive, this CVAR specifies the maximum number of CH4 VNIs that OFI netmod exposes.
20211015 115039.979 INFO             PET4 index= 251                           MPIR_CVAR_CH4_OFI_MAX_RMA_SEP_CTX : If set to positive, this CVAR specifies the maximum number of transmit contexts RMA can utilize in a scalable endpoint. This value is effective only when scalable endpoint is available, otherwise it will be ignored.
20211015 115039.979 INFO             PET4 index= 252                          MPIR_CVAR_CH4_OFI_MAX_EAGAIN_RETRY : If set to positive, this CVAR specifies the maximum number of retries of an ofi operations before returning MPIX_ERR_EAGAIN. This value is effective only when the communicator has the MPI_OFI_set_eagain info hint set to true.
20211015 115039.979 INFO             PET4 index= 253                            MPIR_CVAR_CH4_OFI_NUM_AM_BUFFERS : Specifies the number of buffers for receiving active messages.
20211015 115039.979 INFO             PET4 index= 254                                        MPIR_CVAR_CH4_NETMOD : If non-empty, this cvar specifies which network module to use
20211015 115039.979 INFO             PET4 index= 255                                           MPIR_CVAR_CH4_SHM : If non-empty, this cvar specifies which shm module to use
20211015 115039.979 INFO             PET4 index= 256                                MPIR_CVAR_CH4_ROOTS_ONLY_PMI : Enables an optimized business card exchange over PMI for node root processes only.
20211015 115039.979 INFO             PET4 index= 257                            MPIR_CVAR_CH4_RUNTIME_CONF_DEBUG : If enabled, CH4-level runtime configurations are printed out
20211015 115039.979 INFO             PET4 index= 258                                      MPIR_CVAR_CH4_MT_MODEL : Specifies the CH4 multi-threading model. Possible values are: direct (default) handoff trylock
20211015 115039.979 INFO             PET4 index= 259                          MPIR_CVAR_CH4_COMM_CONNECT_TIMEOUT : The default time out period in seconds for a connection attempt to the server communicator where the named port exists but no pending accept. User can change the value for a specified connection through its info argument.
20211015 115039.979 INFO             PET4 index= 260                             MPIR_CVAR_CH4_RANDOM_ADDR_RETRY : The default number of retries for generating a random address. A retrying involves only local operations.
20211015 115039.979 INFO             PET4 index= 261                             MPIR_CVAR_CH4_SHM_SYMHEAP_RETRY : The default number of retries for allocating a symmetric heap in shared memory. A retrying involves collective communication over the group in the shared memory.
20211015 115039.979 INFO             PET4 index= 262                             MPIR_CVAR_CH4_RMA_MEM_EFFICIENT : If true, memory-saving mode is on, per-target object is released at the epoch end call. If false, performance-efficient mode is on, all allocated target objects are cached and freed at win_finalize.
20211015 115039.979 INFO             PET4 index= 263                                      MPIR_CVAR_ENABLE_HCOLL : Enable hcoll collective support.
20211015 115039.979 INFO             PET4 index= 264                                   MPIR_CVAR_COLL_SCHED_DUMP : Print schedule data for nonblocking collective operations.
20211015 115039.979 INFO             PET4 --- VMK::logSystem() end ---------------------------------
20211015 115039.979 INFO             PET4 main: --- VMK::log() start -------------------------------------
20211015 115039.979 INFO             PET4 main: vm located at: 0x7fcbc740cf90
20211015 115039.979 INFO             PET4 main: petCount=6 localPet=4 mypthid=0 currentSsiPe=-1
20211015 115039.979 INFO             PET4 main: Current system level OMP_NUM_THREADS setting for local PET: 12
20211015 115039.979 INFO             PET4 main: ssiCount=1 localSsi=0
20211015 115039.979 INFO             PET4 main: mpionly=1 threadsflag=0
20211015 115039.979 INFO             PET4 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20211015 115039.979 INFO             PET4 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20211015 115039.979 INFO             PET4 main:  PE=0 SSI=0 SSIPE=0
20211015 115039.979 INFO             PET4 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20211015 115039.979 INFO             PET4 main:  PE=1 SSI=0 SSIPE=1
20211015 115039.979 INFO             PET4 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20211015 115039.979 INFO             PET4 main:  PE=2 SSI=0 SSIPE=2
20211015 115039.979 INFO             PET4 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20211015 115039.979 INFO             PET4 main:  PE=3 SSI=0 SSIPE=3
20211015 115039.979 INFO             PET4 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20211015 115039.979 INFO             PET4 main:  PE=4 SSI=0 SSIPE=4
20211015 115039.980 INFO             PET4 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20211015 115039.980 INFO             PET4 main:  PE=5 SSI=0 SSIPE=5
20211015 115039.980 INFO             PET4 main: --- VMK::log() end ---------------------------------------
20211015 115039.980 INFO             PET4 Executing 'userm1_setvm'
20211015 115039.980 INFO             PET4 Executing 'userm1_register'
20211015 115039.980 INFO             PET4 Executing 'userm2_setvm'
20211015 115039.980 INFO             PET4 Executing 'userm2_register'
20211015 115039.980 INFO             PET4 model1: --- VMK::log() start -------------------------------------
20211015 115039.980 INFO             PET4 model1: vm located at: 0x7fcbb7513fd0
20211015 115039.980 INFO             PET4 model1: petCount=6 localPet=4 mypthid=0 currentSsiPe=-1
20211015 115039.980 INFO             PET4 model1: Current system level OMP_NUM_THREADS setting for local PET: 12
20211015 115039.980 INFO             PET4 model1: ssiCount=1 localSsi=0
20211015 115039.980 INFO             PET4 model1: mpionly=1 threadsflag=0
20211015 115039.980 INFO             PET4 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20211015 115039.980 INFO             PET4 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20211015 115039.980 INFO             PET4 model1:  PE=0 SSI=0 SSIPE=0
20211015 115039.980 INFO             PET4 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20211015 115039.980 INFO             PET4 model1:  PE=1 SSI=0 SSIPE=1
20211015 115039.980 INFO             PET4 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20211015 115039.980 INFO             PET4 model1:  PE=2 SSI=0 SSIPE=2
20211015 115039.980 INFO             PET4 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20211015 115039.980 INFO             PET4 model1:  PE=3 SSI=0 SSIPE=3
20211015 115039.980 INFO             PET4 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20211015 115039.980 INFO             PET4 model1:  PE=4 SSI=0 SSIPE=4
20211015 115039.980 INFO             PET4 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20211015 115039.980 INFO             PET4 model1:  PE=5 SSI=0 SSIPE=5
20211015 115039.980 INFO             PET4 model1: --- VMK::log() end ---------------------------------------
20211015 115039.985 INFO             PET4 Entering 'user1_run'
20211015 115039.985 INFO             PET4  user1_run: on SSIPE:           -1  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20211015 115040.388 INFO             PET4  user1_run: on SSIPE:           -1  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20211015 115040.635 INFO             PET4  user1_run: on SSIPE:           -1  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20211015 115040.896 INFO             PET4  user1_run: on SSIPE:           -1  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20211015 115041.146 INFO             PET4  user1_run: on SSIPE:           -1  Filling data lbound:        6669           1           1  ubound:        8334        1200          10
20211015 115041.400 INFO             PET4 Exiting 'user1_run'
20211015 115041.412 INFO             PET4 Entering 'user2_run'
20211015 115041.412 INFO             PET4 model2: --- VMK::log() start -------------------------------------
20211015 115041.412 INFO             PET4 model2: vm located at: 0x7fcbb75144a0
20211015 115041.412 INFO             PET4 model2: petCount=6 localPet=4 mypthid=0 currentSsiPe=-1
20211015 115041.412 INFO             PET4 model2: Current system level OMP_NUM_THREADS setting for local PET: 12
20211015 115041.412 INFO             PET4 model2: ssiCount=1 localSsi=0
20211015 115041.412 INFO             PET4 model2: mpionly=1 threadsflag=0
20211015 115041.412 INFO             PET4 model2: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20211015 115041.412 INFO             PET4 model2: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20211015 115041.412 INFO             PET4 model2:  PE=0 SSI=0 SSIPE=0
20211015 115041.412 INFO             PET4 model2: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20211015 115041.412 INFO             PET4 model2:  PE=1 SSI=0 SSIPE=1
20211015 115041.412 INFO             PET4 model2: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20211015 115041.412 INFO             PET4 model2:  PE=2 SSI=0 SSIPE=2
20211015 115041.412 INFO             PET4 model2: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20211015 115041.412 INFO             PET4 model2:  PE=3 SSI=0 SSIPE=3
20211015 115041.412 INFO             PET4 model2: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20211015 115041.412 INFO             PET4 model2:  PE=4 SSI=0 SSIPE=4
20211015 115041.412 INFO             PET4 model2: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20211015 115041.412 INFO             PET4 model2:  PE=5 SSI=0 SSIPE=5
20211015 115041.412 INFO             PET4 model2: --- VMK::log() end ---------------------------------------
20211015 115041.412 INFO             PET4  user2_run: ssiLocalDeCount=           6
20211015 115041.413 INFO             PET4  user2_run: OpenMP thread:           3  on SSIPE:           -1  Testing data for localDe =           3  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20211015 115041.413 INFO             PET4  user2_run: OpenMP thread:           2  on SSIPE:           -1  Testing data for localDe =           2  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20211015 115041.423 INFO             PET4  user2_run: OpenMP thread:           1  on SSIPE:           -1  Testing data for localDe =           1  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20211015 115041.433 INFO             PET4  user2_run: OpenMP thread:           4  on SSIPE:           -1  Testing data for localDe =           4  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20211015 115041.453 INFO             PET4  user2_run: OpenMP thread:           0  on SSIPE:           -1  Testing data for localDe =           0  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20211015 115041.473 INFO             PET4  user2_run: OpenMP thread:           5  on SSIPE:           -1  Testing data for localDe =           5  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20211015 115042.938 INFO             PET4  user2_run: OpenMP thread:           0  on SSIPE:           -1  Testing data for localDe =           0  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20211015 115042.939 INFO             PET4  user2_run: OpenMP thread:           3  on SSIPE:           -1  Testing data for localDe =           3  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20211015 115042.942 INFO             PET4  user2_run: OpenMP thread:           2  on SSIPE:           -1  Testing data for localDe =           2  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20211015 115042.942 INFO             PET4  user2_run: OpenMP thread:           1  on SSIPE:           -1  Testing data for localDe =           1  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20211015 115042.944 INFO             PET4  user2_run: OpenMP thread:           4  on SSIPE:           -1  Testing data for localDe =           4  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20211015 115042.947 INFO             PET4  user2_run: OpenMP thread:           5  on SSIPE:           -1  Testing data for localDe =           5  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20211015 115044.277 INFO             PET4  user2_run: OpenMP thread:           0  on SSIPE:           -1  Testing data for localDe =           0  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20211015 115044.279 INFO             PET4  user2_run: OpenMP thread:           2  on SSIPE:           -1  Testing data for localDe =           2  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20211015 115044.279 INFO             PET4  user2_run: OpenMP thread:           3  on SSIPE:           -1  Testing data for localDe =           3  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20211015 115044.280 INFO             PET4  user2_run: OpenMP thread:           1  on SSIPE:           -1  Testing data for localDe =           1  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20211015 115044.281 INFO             PET4  user2_run: OpenMP thread:           4  on SSIPE:           -1  Testing data for localDe =           4  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20211015 115044.286 INFO             PET4  user2_run: OpenMP thread:           5  on SSIPE:           -1  Testing data for localDe =           5  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20211015 115045.615 INFO             PET4  user2_run: OpenMP thread:           0  on SSIPE:           -1  Testing data for localDe =           0  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20211015 115045.615 INFO             PET4  user2_run: OpenMP thread:           2  on SSIPE:           -1  Testing data for localDe =           2  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20211015 115045.617 INFO             PET4  user2_run: OpenMP thread:           1  on SSIPE:           -1  Testing data for localDe =           1  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20211015 115045.618 INFO             PET4  user2_run: OpenMP thread:           3  on SSIPE:           -1  Testing data for localDe =           3  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20211015 115045.619 INFO             PET4  user2_run: OpenMP thread:           4  on SSIPE:           -1  Testing data for localDe =           4  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20211015 115045.622 INFO             PET4  user2_run: OpenMP thread:           5  on SSIPE:           -1  Testing data for localDe =           5  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20211015 115046.961 INFO             PET4  user2_run: OpenMP thread:           0  on SSIPE:           -1  Testing data for localDe =           0  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20211015 115046.961 INFO             PET4  user2_run: OpenMP thread:           1  on SSIPE:           -1  Testing data for localDe =           1  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20211015 115046.961 INFO             PET4  user2_run: OpenMP thread:           3  on SSIPE:           -1  Testing data for localDe =           3  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20211015 115046.962 INFO             PET4  user2_run: OpenMP thread:           4  on SSIPE:           -1  Testing data for localDe =           4  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20211015 115046.965 INFO             PET4  user2_run: OpenMP thread:           2  on SSIPE:           -1  Testing data for localDe =           2  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20211015 115046.971 INFO             PET4  user2_run: OpenMP thread:           5  on SSIPE:           -1  Testing data for localDe =           5  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20211015 115048.264 INFO             PET4  user2_run: All data correct.
20211015 115048.264 INFO             PET4 Exiting 'user2_run'
20211015 115048.318 INFO             PET4  NUMBER_OF_PROCESSORS           6
20211015 115048.318 INFO             PET4  PASS  System Test ESMF_FieldSharedDeSSISTest, ESMF_FieldSharedDeSSISTest.F90, line 276
20211015 115048.318 INFO             PET4 Finalizing ESMF
20211015 115039.974 INFO             PET5 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20211015 115039.975 INFO             PET5 !!! THE ESMF_LOG IS SET TO OUTPUT ALL LOG MESSAGES !!!
20211015 115039.975 INFO             PET5 !!!     THIS MAY CAUSE SLOWDOWN IN PERFORMANCE     !!!
20211015 115039.975 INFO             PET5 !!! FOR PRODUCTION RUNS, USE:                      !!!
20211015 115039.975 INFO             PET5 !!!                   ESMF_LOGKIND_Multi_On_Error  !!!
20211015 115039.975 INFO             PET5 !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
20211015 115039.975 INFO             PET5 Running with ESMF Version   : ESMF_8_2_0_beta_snapshot_22-10-g6974ca47a6
20211015 115039.975 INFO             PET5 ESMF library build date/time: "Oct 15 2021" "11:35:17"
20211015 115039.975 INFO             PET5 ESMF library build location : /Volumes/esmf/rocky/esmf-testing/gfortran_9.3.0_mpich3_O_release_8.2.0
20211015 115039.975 INFO             PET5 ESMF_COMM                   : mpich3
20211015 115039.975 INFO             PET5 ESMF_MOAB                   : enabled
20211015 115039.975 INFO             PET5 ESMF_LAPACK                 : enabled
20211015 115039.975 INFO             PET5 ESMF_NETCDF                 : enabled
20211015 115039.975 INFO             PET5 ESMF_PNETCDF                : disabled
20211015 115039.975 INFO             PET5 ESMF_PIO                    : enabled
20211015 115039.975 INFO             PET5 ESMF_YAMLCPP                : enabled
20211015 115039.976 INFO             PET5 --- VMK::logSystem() start -------------------------------
20211015 115039.976 INFO             PET5 esmfComm=mpich3
20211015 115039.976 INFO             PET5 isPthreadsEnabled=0
20211015 115039.976 INFO             PET5 isOpenMPEnabled=1
20211015 115039.976 INFO             PET5 isOpenACCEnabled=0
20211015 115039.976 INFO             PET5 isSsiSharedMemoryEnabled=1
20211015 115039.976 INFO             PET5 ssiCount=1 peCount=6
20211015 115039.976 INFO             PET5 PE=0 SSI=0 SSIPE=0
20211015 115039.976 INFO             PET5 PE=1 SSI=0 SSIPE=1
20211015 115039.976 INFO             PET5 PE=2 SSI=0 SSIPE=2
20211015 115039.976 INFO             PET5 PE=3 SSI=0 SSIPE=3
20211015 115039.976 INFO             PET5 PE=4 SSI=0 SSIPE=4
20211015 115039.976 INFO             PET5 PE=5 SSI=0 SSIPE=5
20211015 115039.976 INFO             PET5 --- VMK::logSystem() MPI Control Variables ---------------
20211015 115039.976 INFO             PET5 index=   0                          MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the short message algorithm will be used if the send buffer size is < this value (in bytes). (See also: MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE)
20211015 115039.976 INFO             PET5 index=   1                           MPIR_CVAR_ALLGATHER_LONG_MSG_SIZE : For MPI_Allgather and MPI_Allgatherv, the long message algorithm will be used if the send buffer size is >= this value (in bytes) (See also: MPIR_CVAR_ALLGATHER_SHORT_MSG_SIZE)
20211015 115039.976 INFO             PET5 index=   2                         MPIR_CVAR_ALLGATHER_INTRA_ALGORITHM : Variable to select allgather algorithmauto               - Internal algorithm selectionbrucks             - Force brucks algorithmnb                 - Force nonblocking algorithmrecursive_doubling - Force recursive doubling algorithmring               - Force ring algorithm
20211015 115039.976 INFO             PET5 index=   3                         MPIR_CVAR_ALLGATHER_INTER_ALGORITHM : Variable to select allgather algorithmauto                      - Internal algorithm selectionlocal_gather_remote_bcast - Force local-gather-remote-bcast algorithmnb                        - Force nonblocking algorithm
20211015 115039.976 INFO             PET5 index=   4                       MPIR_CVAR_ALLGATHER_DEVICE_COLLECTIVE : If set to true, MPI_Allgather will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level allgather function will not be called.
20211015 115039.976 INFO             PET5 index=   5                      MPIR_CVAR_ALLGATHERV_PIPELINE_MSG_SIZE : The smallest message size that will be used for the pipelined, large-message, ring algorithm in the MPI_Allgatherv implementation.
20211015 115039.976 INFO             PET5 index=   6                        MPIR_CVAR_ALLGATHERV_INTRA_ALGORITHM : Variable to select allgatherv algorithmauto               - Internal algorithm selectionbrucks             - Force brucks algorithmnb                 - Force nonblocking algorithmrecursive_doubling - Force recursive doubling algorithmring               - Force ring algorithm
20211015 115039.976 INFO             PET5 index=   7                        MPIR_CVAR_ALLGATHERV_INTER_ALGORITHM : Variable to select allgatherv algorithmauto                      - Internal algorithm selectionnb                        - Force nonblocking algorithmremote_gather_local_bcast - Force remote-gather-local-bcast algorithm
20211015 115039.976 INFO             PET5 index=   8                      MPIR_CVAR_ALLGATHERV_DEVICE_COLLECTIVE : If set to true, MPI_Allgatherv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level allgatherv function will not be called.
20211015 115039.976 INFO             PET5 index=   9                          MPIR_CVAR_ALLREDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20211015 115039.976 INFO             PET5 index=  10                            MPIR_CVAR_ENABLE_SMP_COLLECTIVES : Enable SMP aware collective communication.
20211015 115039.976 INFO             PET5 index=  11                              MPIR_CVAR_ENABLE_SMP_ALLREDUCE : Enable SMP aware allreduce.
20211015 115039.976 INFO             PET5 index=  12                        MPIR_CVAR_MAX_SMP_ALLREDUCE_MSG_SIZE : Maximum message size for which SMP-aware allreduce is used.  A value of '0' uses SMP-aware allreduce for all message sizes.
20211015 115039.976 INFO             PET5 index=  13                         MPIR_CVAR_ALLREDUCE_INTRA_ALGORITHM : Variable to select allreduce algorithmauto                     - Internal algorithm selectionnb                       - Force nonblocking algorithmrecursive_doubling       - Force recursive doubling algorithmreduce_scatter_allgather - Force reduce scatter allgather algorithm
20211015 115039.976 INFO             PET5 index=  14                         MPIR_CVAR_ALLREDUCE_INTER_ALGORITHM : Variable to select allreduce algorithmauto                  - Internal algorithm selectionnb                    - Force nonblocking algorithmreduce_exchange_bcast - Force reduce-exchange-bcast algorithm
20211015 115039.976 INFO             PET5 index=  15                       MPIR_CVAR_ALLREDUCE_DEVICE_COLLECTIVE : If set to true, MPI_Allreduce will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level allreduce function will not be called.
20211015 115039.976 INFO             PET5 index=  16                           MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE : the short message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value (See also: MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE)
20211015 115039.976 INFO             PET5 index=  17                          MPIR_CVAR_ALLTOALL_MEDIUM_MSG_SIZE : the medium message algorithm will be used if the per-destination message size (sendcount*size(sendtype)) is <= this value and larger than MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE (See also: MPIR_CVAR_ALLTOALL_SHORT_MSG_SIZE)
20211015 115039.976 INFO             PET5 index=  18                                 MPIR_CVAR_ALLTOALL_THROTTLE : max no. of irecvs/isends posted at a time in some alltoall algorithms. Setting it to 0 causes all irecvs/isends to be posted at once
20211015 115039.976 INFO             PET5 index=  19                          MPIR_CVAR_ALLTOALL_INTRA_ALGORITHM : Variable to select alltoall algorithmauto                      - Internal algorithm selectionbrucks                    - Force brucks algorithmnb                        - Force nonblocking algorithmpairwise                  - Force pairwise algorithmpairwise_sendrecv_replace - Force pairwise sendrecv replace algorithmscattered                 - Force scattered algorithm
20211015 115039.976 INFO             PET5 index=  20                          MPIR_CVAR_ALLTOALL_INTER_ALGORITHM : Variable to select alltoall algorithmauto              - Internal algorithm selectionnb                - Force nonblocking algorithmpairwise_exchange - Force pairwise exchange algorithm
20211015 115039.976 INFO             PET5 index=  21                        MPIR_CVAR_ALLTOALL_DEVICE_COLLECTIVE : If set to true, MPI_Alltoall will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level alltoall function will not be called.
20211015 115039.976 INFO             PET5 index=  22                         MPIR_CVAR_ALLTOALLV_INTRA_ALGORITHM : Variable to select alltoallv algorithmauto                      - Internal algorithm selectionnb                        - Force nonblocking algorithmpairwise_sendrecv_replace - Force pairwise_sendrecv_replace algorithmscattered                 - Force scattered algorithm
20211015 115039.976 INFO             PET5 index=  23                         MPIR_CVAR_ALLTOALLV_INTER_ALGORITHM : Variable to select alltoallv algorithmauto              - Internal algorithm selectionpairwise_exchange - Force pairwise exchange algorithmnb                - Force nonblocking algorithm
20211015 115039.976 INFO             PET5 index=  24                       MPIR_CVAR_ALLTOALLV_DEVICE_COLLECTIVE : If set to true, MPI_Alltoallv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level alltoallv function will not be called.
20211015 115039.976 INFO             PET5 index=  25                         MPIR_CVAR_ALLTOALLW_INTRA_ALGORITHM : Variable to select alltoallw algorithmauto                      - Internal algorithm selectionnb                        - Force nonblocking algorithmpairwise_sendrecv_replace - Force pairwise sendrecv replace algorithmscattered                 - Force scattered algorithm
20211015 115039.976 INFO             PET5 index=  26                         MPIR_CVAR_ALLTOALLW_INTER_ALGORITHM : Variable to select alltoallw algorithmauto              - Internal algorithm selectionnb                - Force nonblocking algorithmpairwise_exchange - Force pairwise exchange algorithm
20211015 115039.976 INFO             PET5 index=  27                       MPIR_CVAR_ALLTOALLW_DEVICE_COLLECTIVE : If set to true, MPI_Alltoallw will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level alltoallw function will not be called.
20211015 115039.976 INFO             PET5 index=  28                                MPIR_CVAR_ENABLE_SMP_BARRIER : Enable SMP aware barrier.
20211015 115039.976 INFO             PET5 index=  29                           MPIR_CVAR_BARRIER_INTRA_ALGORITHM : Variable to select barrier algorithmauto               - Internal algorithm selectionnb                 - Force nonblocking algorithmrecursive_doubling - Force recursive doubling algorithm
20211015 115039.976 INFO             PET5 index=  30                           MPIR_CVAR_BARRIER_INTER_ALGORITHM : Variable to select barrier algorithmauto  - Internal algorithm selectionbcast - Force bcast algorithmnb    - Force nonblocking algorithm
20211015 115039.976 INFO             PET5 index=  31                         MPIR_CVAR_BARRIER_DEVICE_COLLECTIVE : If set to true, MPI_Barrier will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level barrier function will not be called.
20211015 115039.976 INFO             PET5 index=  32                                   MPIR_CVAR_BCAST_MIN_PROCS : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_SHORT_MSG_SIZE, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20211015 115039.976 INFO             PET5 index=  33                              MPIR_CVAR_BCAST_SHORT_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_LONG_MSG_SIZE)
20211015 115039.976 INFO             PET5 index=  34                               MPIR_CVAR_BCAST_LONG_MSG_SIZE : Let's define short messages as messages with size < MPIR_CVAR_BCAST_SHORT_MSG_SIZE, and medium messages as messages with size >= MPIR_CVAR_BCAST_SHORT_MSG_SIZE but < MPIR_CVAR_BCAST_LONG_MSG_SIZE, and long messages as messages with size >= MPIR_CVAR_BCAST_LONG_MSG_SIZE. The broadcast algorithms selection procedure is as follows. For short messages or when the number of processes is < MPIR_CVAR_BCAST_MIN_PROCS, we do broadcast using the binomial tree algorithm. Otherwise, for medium messages and with a power-of-two number of processes, we do broadcast based on a scatter followed by a recursive doubling allgather algorithm. Otherwise, for long messages or with non power-of-two number of processes, we do broadcast based on a scatter followed by a ring allgather algorithm. (See also: MPIR_CVAR_BCAST_MIN_PROCS, MPIR_CVAR_BCAST_SHORT_MSG_SIZE)
20211015 115039.976 INFO             PET5 index=  35                                  MPIR_CVAR_ENABLE_SMP_BCAST : Enable SMP aware broadcast (See also: MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE)
20211015 115039.976 INFO             PET5 index=  36                            MPIR_CVAR_MAX_SMP_BCAST_MSG_SIZE : Maximum message size for which SMP-aware broadcast is used.  A value of '0' uses SMP-aware broadcast for all message sizes. (See also: MPIR_CVAR_ENABLE_SMP_BCAST)
20211015 115039.976 INFO             PET5 index=  37                             MPIR_CVAR_BCAST_INTRA_ALGORITHM : Variable to select bcast algorithmauto                                    - Internal algorithm selectionbinomial                                - Force Binomial Treenb                                      - Force nonblocking algorithmscatter_recursive_doubling_allgather    - Force Scatter Recursive-Doubling Allgatherscatter_ring_allgather                  - Force Scatter Ring
20211015 115039.976 INFO             PET5 index=  38                             MPIR_CVAR_BCAST_INTER_ALGORITHM : Variable to select bcast algorithmauto                    - Internal algorithm selectionnb                      - Force nonblocking algorithmremote_send_local_bcast - Force remote-send-local-bcast algorithm
20211015 115039.977 INFO             PET5 index=  39                           MPIR_CVAR_BCAST_DEVICE_COLLECTIVE : If set to true, MPI_Bcast will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually.  If set to false, the device-level bcast function will not be called.
20211015 115039.977 INFO             PET5 index=  40                            MPIR_CVAR_EXSCAN_INTRA_ALGORITHM : Variable to select allgather algorithmauto               - Internal algorithm selectionnb                 - Force nonblocking algorithmrecursive_doubling - Force recursive doubling algorithm
20211015 115039.977 INFO             PET5 index=  41                          MPIR_CVAR_EXSCAN_DEVICE_COLLECTIVE : If set to true, MPI_Exscan will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level exscan function will not be called.
20211015 115039.977 INFO             PET5 index=  42                       MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_VSMALL_MSG_SIZE)
20211015 115039.977 INFO             PET5 index=  43                            MPIR_CVAR_GATHER_INTRA_ALGORITHM : Variable to select gather algorithmauto     - Internal algorithm selectionbinomial - Force binomial algorithmnb       - Force nonblocking algorithm
20211015 115039.977 INFO             PET5 index=  44                            MPIR_CVAR_GATHER_INTER_ALGORITHM : Variable to select gather algorithmauto                     - Internal algorithm selectionlinear                   - Force linear algorithmlocal_gather_remote_send - Force local-gather-remote-send algorithmnb                       - Force nonblocking algorithm
20211015 115039.977 INFO             PET5 index=  45                          MPIR_CVAR_GATHER_DEVICE_COLLECTIVE : If set to true, MPI_Gather will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level gather function will not be called.
20211015 115039.977 INFO             PET5 index=  46                            MPIR_CVAR_GATHER_VSMALL_MSG_SIZE : use a temporary buffer for intracommunicator MPI_Gather if the send buffer size is < this value (in bytes) (See also: MPIR_CVAR_GATHER_INTER_SHORT_MSG_SIZE)
20211015 115039.977 INFO             PET5 index=  47                           MPIR_CVAR_GATHERV_INTRA_ALGORITHM : Variable to select gatherv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithmnb     - Force nonblocking algorithm
20211015 115039.977 INFO             PET5 index=  48                           MPIR_CVAR_GATHERV_INTER_ALGORITHM : Variable to select gatherv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithmnb     - Force nonblocking algorithm
20211015 115039.977 INFO             PET5 index=  49                         MPIR_CVAR_GATHERV_DEVICE_COLLECTIVE : If set to true, MPI_Gatherv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level gatherv function will not be called.
20211015 115039.977 INFO             PET5 index=  50                     MPIR_CVAR_GATHERV_INTER_SSEND_MIN_PROCS : Use Ssend (synchronous send) for intercommunicator MPI_Gatherv if the "group B" size is >= this value.  Specifying "-1" always avoids using Ssend.  For backwards compatibility, specifying "0" uses the default value.
20211015 115039.977 INFO             PET5 index=  51                           MPIR_CVAR_IALLGATHER_RECEXCH_KVAL : k value for recursive exchange based iallgather
20211015 115039.977 INFO             PET5 index=  52                            MPIR_CVAR_IALLGATHER_BRUCKS_KVAL : k value for radix in brucks based iallgather
20211015 115039.977 INFO             PET5 index=  53                        MPIR_CVAR_IALLGATHER_INTRA_ALGORITHM : Variable to select iallgather algorithmauto               - Internal algorithm selectionbrucks             - Force brucks algorithmrecursive_doubling - Force recursive doubling algorithmring               - Force ring algorithmrecexch_distance_doubling    - Force generic transport recursive exchange with neighbours doubling in distance in each phaserecexch_distance_halving  - Force generic transport recursive exchange with neighbours halving in distance in each phasegentran_brucks     - Force generic transport based brucks algorithm
20211015 115039.977 INFO             PET5 index=  54                        MPIR_CVAR_IALLGATHER_INTER_ALGORITHM : Variable to select iallgather algorithmauto                      - Internal algorithm selectionlocal_gather_remote_bcast - Force local-gather-remote-bcast algorithm
20211015 115039.977 INFO             PET5 index=  55                      MPIR_CVAR_IALLGATHER_DEVICE_COLLECTIVE : If set to true, MPI_Iallgather will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iallgather function will not be called.
20211015 115039.977 INFO             PET5 index=  56                          MPIR_CVAR_IALLGATHERV_RECEXCH_KVAL : k value for recursive exchange based iallgatherv
20211015 115039.977 INFO             PET5 index=  57                       MPIR_CVAR_IALLGATHERV_INTRA_ALGORITHM : Variable to select iallgatherv algorithmauto               - Internal algorithm selectionbrucks             - Force brucks algorithmrecursive_doubling - Force recursive doubling algorithmring               - Force ring algorithmrecexch_distance_doubling    - Force generic transport recursive exchange with neighbours doubling in distance in each phaserecexch_distance_halving     - Force generic transport recursive exchange with neighbours halving in distance in each phasegentran_ring              - Force generic transport ring algorithm
20211015 115039.977 INFO             PET5 index=  58                       MPIR_CVAR_IALLGATHERV_INTER_ALGORITHM : Variable to select iallgatherv algorithmauto                      - Internal algorithm selectionremote_gather_local_bcast - Force remote-gather-local-bcast algorithm
20211015 115039.977 INFO             PET5 index=  59                     MPIR_CVAR_IALLGATHERV_DEVICE_COLLECTIVE : If set to true, MPI_Iallgatherv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iallgatherv function will not be called.
20211015 115039.977 INFO             PET5 index=  60                              MPIR_CVAR_IALLREDUCE_TREE_KVAL : k value for tree based iallreduce (for tree_kary and tree_knomial)
20211015 115039.977 INFO             PET5 index=  61               MPIR_CVAR_IALLREDUCE_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based iallreduce (tree_kary and tree_knomial). Default value is 0, that is, no pipelining by default
20211015 115039.977 INFO             PET5 index=  62                  MPIR_CVAR_IALLREDUCE_TREE_BUFFER_PER_CHILD : If set to true, a rank in tree_kary and tree_knomial algorithms will allocate a dedicated buffer for every child it receives data from. This would mean more memory consumption but it would allow preposting of the receives and hence reduce the number of unexpected messages. If set to false, there is only one buffer that is used to receive the data from all the children. The receives are therefore serialized, that is, only one receive can be posted at a time.
20211015 115039.977 INFO             PET5 index=  63                           MPIR_CVAR_IALLREDUCE_RECEXCH_KVAL : k value for recursive exchange based iallreduce
20211015 115039.977 INFO             PET5 index=  64                        MPIR_CVAR_IALLREDUCE_INTRA_ALGORITHM : Variable to select iallreduce algorithmauto                     - Internal algorithm selectionnaive                    - Force naive algorithmrecursive_doubling       - Force recursive doubling algorithmreduce_scatter_allgather - Force reduce scatter allgather algorithmrecexch_single_buffer    - Force generic transport recursive exchange with single buffer for receivesrecexch_multiple_buffer  - Force generic transport recursive exchange with multiple buffers for receives
20211015 115039.977 INFO             PET5 index=  65                        MPIR_CVAR_IALLREDUCE_INTER_ALGORITHM : Variable to select iallreduce algorithmauto                      - Internal algorithm selectionremote_reduce_local_bcast - Force remote-reduce-local-bcast algorithm
20211015 115039.977 INFO             PET5 index=  66                      MPIR_CVAR_IALLREDUCE_DEVICE_COLLECTIVE : If set to true, MPI_Iallreduce will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iallreduce function will not be called.
20211015 115039.977 INFO             PET5 index=  67                         MPIR_CVAR_IALLTOALL_INTRA_ALGORITHM : Variable to select ialltoall algorithmauto              - Internal algorithm selectionbrucks            - Force brucks algorithminplace           - Force inplace algorithmpairwise          - Force pairwise algorithmpermuted_sendrecv - Force permuted sendrecv algorithm
20211015 115039.977 INFO             PET5 index=  68                         MPIR_CVAR_IALLTOALL_INTER_ALGORITHM : Variable to select ialltoall algorithmauto              - Internal algorithm selectionpairwise_exchange - Force pairwise exchange algorithm
20211015 115039.977 INFO             PET5 index=  69                       MPIR_CVAR_IALLTOALL_DEVICE_COLLECTIVE : If set to true, MPI_Ialltoall will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ialltoall function will not be called.
20211015 115039.977 INFO             PET5 index=  70                        MPIR_CVAR_IALLTOALLV_INTRA_ALGORITHM : Variable to select ialltoallv algorithmauto              - Internal algorithm selectionblocked           - Force blocked algorithminplace           - Force inplace algorithmpairwise_exchange - Force pairwise exchange algorithm
20211015 115039.977 INFO             PET5 index=  71                        MPIR_CVAR_IALLTOALLV_INTER_ALGORITHM : Variable to select ialltoallv algorithmauto - Internal algorithm selection
20211015 115039.977 INFO             PET5 index=  72                      MPIR_CVAR_IALLTOALLV_DEVICE_COLLECTIVE : If set to true, MPI_Ialltoallv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ialltoallv function will not be called.
20211015 115039.977 INFO             PET5 index=  73                        MPIR_CVAR_IALLTOALLW_INTRA_ALGORITHM : Variable to select ialltoallw algorithmauto              - Internal algorithm selectionblocked           - Force blocked algorithminplace           - Force inplace algorithmpairwise_exchange - Force pairwise exchange algorithm
20211015 115039.977 INFO             PET5 index=  74                        MPIR_CVAR_IALLTOALLW_INTER_ALGORITHM : Variable to select ialltoallw algorithmauto - Internal algorithm selection
20211015 115039.977 INFO             PET5 index=  75                      MPIR_CVAR_IALLTOALLW_DEVICE_COLLECTIVE : If set to true, MPI_Ialltoallw will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ialltoallw function will not be called.
20211015 115039.977 INFO             PET5 index=  76                             MPIR_CVAR_IBARRIER_RECEXCH_KVAL : k value for recursive exchange based ibarrier
20211015 115039.977 INFO             PET5 index=  77                          MPIR_CVAR_IBARRIER_INTRA_ALGORITHM : Variable to select ibarrier algorithmauto               - Internal algorithm selectionrecursive_doubling - Force recursive doubling algorithmrecexch            - Force generic transport based recursive exchange algorithm
20211015 115039.977 INFO             PET5 index=  78                          MPIR_CVAR_IBARRIER_INTER_ALGORITHM : Variable to select ibarrier algorithmauto  - Internal algorithm selectionbcast - Force bcast algorithm
20211015 115039.977 INFO             PET5 index=  79                        MPIR_CVAR_IBARRIER_DEVICE_COLLECTIVE : If set to true, MPI_Ibarrier will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ibarrier function will not be called.
20211015 115039.977 INFO             PET5 index=  80                                  MPIR_CVAR_IBCAST_TREE_KVAL : k value for tree (kary, knomial, etc.) based ibcast
20211015 115039.977 INFO             PET5 index=  81                                  MPIR_CVAR_IBCAST_TREE_TYPE : Tree type for tree based ibcast kary      - kary tree type knomial_1 - knomial_1 tree type knomial_2 - knomial_2 tree type
20211015 115039.977 INFO             PET5 index=  82                   MPIR_CVAR_IBCAST_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based ibcast. Default value is 0, that is, no pipelining by default
20211015 115039.977 INFO             PET5 index=  83                            MPIR_CVAR_IBCAST_RING_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in ibcast ring algorithm. Default value is 0, that is, no pipelining by default
20211015 115039.977 INFO             PET5 index=  84                            MPIR_CVAR_IBCAST_INTRA_ALGORITHM : Variable to select ibcast algorithmauto                                 - Internal algorithm selectionbinomial                             - Force Binomial algorithmscatter_recursive_doubling_allgather - Force Scatter Recursive Doubling Allgather algorithmscatter_ring_allgather               - Force Scatter Ring Allgather algorithmtree                                 - Force Generic Transport Tree algorithmscatter_recexch_allgather            - Force Generic Transport Scatter followed by Recursive Exchange Allgather algorithmring                                 - Force Generic Transport Ring algorithm
20211015 115039.977 INFO             PET5 index=  85                               MPIR_CVAR_IBCAST_SCATTER_KVAL : k value for tree based scatter in scatter_recexch_allgather algorithm
20211015 115039.977 INFO             PET5 index=  86                     MPIR_CVAR_IBCAST_ALLGATHER_RECEXCH_KVAL : k value for recursive exchange based allgather in scatter_recexch_allgather algorithm
20211015 115039.977 INFO             PET5 index=  87                            MPIR_CVAR_IBCAST_INTER_ALGORITHM : Variable to select ibcast algorithmauto - Internal algorithm selectionflat - Force flat algorithm
20211015 115039.977 INFO             PET5 index=  88                          MPIR_CVAR_IBCAST_DEVICE_COLLECTIVE : If set to true, MPI_Ibcast will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually.  If set to false, the device-level ibcast function will not be called.
20211015 115039.977 INFO             PET5 index=  89                           MPIR_CVAR_IEXSCAN_INTRA_ALGORITHM : Variable to select iexscan algorithmauto               - Internal algorithm selectionrecursive_doubling - Force recursive doubling algorithm
20211015 115039.977 INFO             PET5 index=  90                         MPIR_CVAR_IEXSCAN_DEVICE_COLLECTIVE : If set to true, MPI_Iexscan will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iexscan function will not be called.
20211015 115039.977 INFO             PET5 index=  91                           MPIR_CVAR_IGATHER_INTRA_ALGORITHM : Variable to select igather algorithmauto     - Internal algorithm selectionbinomial - Force binomial algorithmtree     - Force genetric transport based tree algorithm
20211015 115039.977 INFO             PET5 index=  92                                 MPIR_CVAR_IGATHER_TREE_KVAL : k value for tree based igather
20211015 115039.977 INFO             PET5 index=  93                           MPIR_CVAR_IGATHER_INTER_ALGORITHM : Variable to select igather algorithmauto        - Internal algorithm selectionlong_inter  - Force long inter algorithmshort_inter - Force short inter algorithm
20211015 115039.977 INFO             PET5 index=  94                         MPIR_CVAR_IGATHER_DEVICE_COLLECTIVE : If set to true, MPI_Igather will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level igather function will not be called.
20211015 115039.977 INFO             PET5 index=  95                          MPIR_CVAR_IGATHERV_INTRA_ALGORITHM : Variable to select igatherv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET5 index=  96                          MPIR_CVAR_IGATHERV_INTER_ALGORITHM : Variable to select igatherv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET5 index=  97                        MPIR_CVAR_IGATHERV_DEVICE_COLLECTIVE : If set to true, MPI_Igatherv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level igatherv function will not be called.
20211015 115039.977 INFO             PET5 index=  98               MPIR_CVAR_INEIGHBOR_ALLGATHER_INTRA_ALGORITHM : Variable to select ineighbor_allgather algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET5 index=  99               MPIR_CVAR_INEIGHBOR_ALLGATHER_INTER_ALGORITHM : Variable to select ineighbor_allgather algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET5 index= 100             MPIR_CVAR_INEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE : If set to true, MPI_ineighbor_allgather will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ineighbor_allgather function will not be called.
20211015 115039.977 INFO             PET5 index= 101              MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTRA_ALGORITHM : Variable to select ineighbor_allgatherv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET5 index= 102              MPIR_CVAR_INEIGHBOR_ALLGATHERV_INTER_ALGORITHM : Variable to select ineighbor_allgatherv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET5 index= 103            MPIR_CVAR_INEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE : If set to true, MPI_ineighbor_allgatherv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ineighbor_allgatherv function will not be called.
20211015 115039.977 INFO             PET5 index= 104                MPIR_CVAR_INEIGHBOR_ALLTOALL_INTRA_ALGORITHM : Variable to select ineighbor_alltoall algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET5 index= 105                MPIR_CVAR_INEIGHBOR_ALLTOALL_INTER_ALGORITHM : Variable to select ineighbor_alltoall algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET5 index= 106              MPIR_CVAR_INEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE : If set to true, MPI_ineighbor_alltoall will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ineighbor_alltoall function will not be called.
20211015 115039.977 INFO             PET5 index= 107               MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTRA_ALGORITHM : Variable to select ineighbor_alltoallv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET5 index= 108               MPIR_CVAR_INEIGHBOR_ALLTOALLV_INTER_ALGORITHM : Variable to select ineighbor_alltoallv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET5 index= 109             MPIR_CVAR_INEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE : If set to true, MPI_ineighbor_alltoallv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ineighbor_alltoallv function will not be called.
20211015 115039.977 INFO             PET5 index= 110               MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTRA_ALGORITHM : Variable to select ineighbor_alltoallw algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET5 index= 111               MPIR_CVAR_INEIGHBOR_ALLTOALLW_INTER_ALGORITHM : Variable to select ineighbor_alltoallw algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.977 INFO             PET5 index= 112             MPIR_CVAR_INEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE : If set to true, MPI_ineighbor_alltoallw will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ineighbor_alltoallw function will not be called.
20211015 115039.977 INFO             PET5 index= 113                                 MPIR_CVAR_IREDUCE_TREE_KVAL : k value for tree (kary, knomial, etc.) based ireduce
20211015 115039.977 INFO             PET5 index= 114                                 MPIR_CVAR_IREDUCE_TREE_TYPE : Tree type for tree based ireduce kary      - kary tree knomial_1 - knomial_1 tree knomial_2 - knomial_2 tree
20211015 115039.977 INFO             PET5 index= 115                  MPIR_CVAR_IREDUCE_TREE_PIPELINE_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in tree based ireduce. Default value is 0, that is, no pipelining by default
20211015 115039.977 INFO             PET5 index= 116                           MPIR_CVAR_IREDUCE_RING_CHUNK_SIZE : Maximum chunk size (in bytes) for pipelining in ireduce ring algorithm. Default value is 0, that is, no pipelining by default
20211015 115039.977 INFO             PET5 index= 117                     MPIR_CVAR_IREDUCE_TREE_BUFFER_PER_CHILD : If set to true, a rank in tree algorithms will allocate a dedicated buffer for every child it receives data from. This would mean more memory consumption but it would allow preposting of the receives and hence reduce the number of unexpected messages. If set to false, there is only one buffer that is used to receive the data from all the children. The receives are therefore serialized, that is, only one receive can be posted at a time.
20211015 115039.977 INFO             PET5 index= 118                           MPIR_CVAR_IREDUCE_INTRA_ALGORITHM : Variable to select ireduce algorithmauto                  - Internal algorithm selectionbinomial              - Force binomial algorithmreduce_scatter_gather - Force reduce scatter gather algorithmtree                  - Force Generic Transport Treering                  - Force Generic Transport Ring
20211015 115039.977 INFO             PET5 index= 119                           MPIR_CVAR_IREDUCE_INTER_ALGORITHM : Variable to select ireduce algorithmauto                     - Internal algorithm selectionlocal_reduce_remote_send - Force local-reduce-remote-send algorithm
20211015 115039.977 INFO             PET5 index= 120                         MPIR_CVAR_IREDUCE_DEVICE_COLLECTIVE : If set to true, MPI_Ireduce will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ireduce function will not be called.
20211015 115039.977 INFO             PET5 index= 121                      MPIR_CVAR_IREDUCE_SCATTER_RECEXCH_KVAL : k value for recursive exchange based ireduce_scatter
20211015 115039.977 INFO             PET5 index= 122                   MPIR_CVAR_IREDUCE_SCATTER_INTRA_ALGORITHM : Variable to select ireduce_scatter algorithmauto               - Internal algorithm selectionnoncommutative     - Force noncommutative algorithmrecursive_doubling - Force recursive doubling algorithmpairwise           - Force pairwise algorithmrecursive_halving  - Force recursive halving algorithmrecexch  - Force generic transport recursive exchange algorithm
20211015 115039.977 INFO             PET5 index= 123                   MPIR_CVAR_IREDUCE_SCATTER_INTER_ALGORITHM : Variable to select ireduce_scatter algorithmauto                         - Internal algorithm selectionremote_reduce_local_scatterv - Force remote-reduce-local-scatterv algorithm
20211015 115039.977 INFO             PET5 index= 124                 MPIR_CVAR_IREDUCE_SCATTER_DEVICE_COLLECTIVE : If set to true, MPI_Ireduce_scatter will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ireduce_scatter function will not be called.
20211015 115039.977 INFO             PET5 index= 125                MPIR_CVAR_IREDUCE_SCATTER_BLOCK_RECEXCH_KVAL : k value for recursive exchange based ireduce_scatter_block
20211015 115039.977 INFO             PET5 index= 126             MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTRA_ALGORITHM : Variable to select ireduce_scatter_block algorithmauto               - Internal algorithm selectionnoncommutative     - Force noncommutative algorithmrecursive_doubling - Force recursive doubling algorithmpairwise           - Force pairwise algorithmrecursive_halving  - Force recursive halving algorithmrecexch  - Force generic transport recursive exchange algorithm
20211015 115039.977 INFO             PET5 index= 127             MPIR_CVAR_IREDUCE_SCATTER_BLOCK_INTER_ALGORITHM : Variable to select ireduce_scatter_block algorithmauto                         - Internal algorithm selectionremote_reduce_local_scatterv - Force remote-reduce-local-scatterv algorithm
20211015 115039.977 INFO             PET5 index= 128           MPIR_CVAR_IREDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE : If set to true, MPI_Ireduce_scatter_block will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level ireduce_scatter_block function will not be called.
20211015 115039.977 INFO             PET5 index= 129                             MPIR_CVAR_ISCAN_INTRA_ALGORITHM : Variable to select allgather algorithmauto               - Internal algorithm selectionrecursive_doubling - Force recursive doubling algorithm
20211015 115039.977 INFO             PET5 index= 130                           MPIR_CVAR_ISCAN_DEVICE_COLLECTIVE : If set to true, MPI_Iscan will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iscan function will not be called.
20211015 115039.977 INFO             PET5 index= 131                          MPIR_CVAR_ISCATTER_INTRA_ALGORITHM : Variable to select iscatter algorithmauto     - Internal algorithm selectionbinomial - Force binomial algorithmtree     - Force genetric transport based tree algorithm
20211015 115039.977 INFO             PET5 index= 132                                MPIR_CVAR_ISCATTER_TREE_KVAL : k value for tree based iscatter
20211015 115039.978 INFO             PET5 index= 133                          MPIR_CVAR_ISCATTER_INTER_ALGORITHM : Variable to select iscatter algorithmauto                      - Internal algorithm selectionlinear                    - Force linear algorithmremote_send_local_scatter - Force remote-send-local-scatter algorithm
20211015 115039.978 INFO             PET5 index= 134                        MPIR_CVAR_ISCATTER_DEVICE_COLLECTIVE : If set to true, MPI_Iscatter will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iscatter function will not be called.
20211015 115039.978 INFO             PET5 index= 135                         MPIR_CVAR_ISCATTERV_INTRA_ALGORITHM : Variable to select iscatterv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET5 index= 136                         MPIR_CVAR_ISCATTERV_INTER_ALGORITHM : Variable to select iscatterv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithm
20211015 115039.978 INFO             PET5 index= 137                       MPIR_CVAR_ISCATTERV_DEVICE_COLLECTIVE : If set to true, MPI_Iscatterv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level iscatterv function will not be called.
20211015 115039.978 INFO             PET5 index= 138                MPIR_CVAR_NEIGHBOR_ALLGATHER_INTRA_ALGORITHM : Variable to select ineighbor_allgather algorithmauto - Internal algorithm selectionnb   - Force nonblocking algorithm
20211015 115039.978 INFO             PET5 index= 139                MPIR_CVAR_NEIGHBOR_ALLGATHER_INTER_ALGORITHM : Variable to select ineighbor_allgather algorithmauto - Internal algorithm selectionnb   - Force nonblocking algorithm
20211015 115039.978 INFO             PET5 index= 140              MPIR_CVAR_NEIGHBOR_ALLGATHER_DEVICE_COLLECTIVE : If set to true, MPI_neighbor_allgather will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level neighbor_allgather function will not be called.
20211015 115039.978 INFO             PET5 index= 141               MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTRA_ALGORITHM : Variable to select neighbor_allgatherv algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET5 index= 142               MPIR_CVAR_NEIGHBOR_ALLGATHERV_INTER_ALGORITHM : Variable to select neighbor_allgatherv algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET5 index= 143             MPIR_CVAR_NEIGHBOR_ALLGATHERV_DEVICE_COLLECTIVE : If set to true, MPI_neighbor_allgatherv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level neighbor_allgatherv function will not be called.
20211015 115039.978 INFO             PET5 index= 144                 MPIR_CVAR_NEIGHBOR_ALLTOALL_INTRA_ALGORITHM : Variable to select neighbor_alltoall algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET5 index= 145                 MPIR_CVAR_NEIGHBOR_ALLTOALL_INTER_ALGORITHM : Variable to select neighbor_alltoall algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET5 index= 146               MPIR_CVAR_NEIGHBOR_ALLTOALL_DEVICE_COLLECTIVE : If set to true, MPI_neighbor_alltoall will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level neighbor_alltoall function will not be called.
20211015 115039.978 INFO             PET5 index= 147                MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTRA_ALGORITHM : Variable to select neighbor_alltoallv algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET5 index= 148                MPIR_CVAR_NEIGHBOR_ALLTOALLV_INTER_ALGORITHM : Variable to select neighbor_alltoallv algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET5 index= 149              MPIR_CVAR_NEIGHBOR_ALLTOALLV_DEVICE_COLLECTIVE : If set to true, MPI_neighbor_alltoallv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level neighbor_alltoallv function will not be called.
20211015 115039.978 INFO             PET5 index= 150                MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTRA_ALGORITHM : Variable to select neighbor_alltoallw algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET5 index= 151                MPIR_CVAR_NEIGHBOR_ALLTOALLW_INTER_ALGORITHM : Variable to select neighbor_alltoallw algorithmauto - Internal algorithm selectionnb   - Force nb algorithm
20211015 115039.978 INFO             PET5 index= 152              MPIR_CVAR_NEIGHBOR_ALLTOALLW_DEVICE_COLLECTIVE : If set to true, MPI_neighbor_alltoallw will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level neighbor_alltoallw function will not be called.
20211015 115039.978 INFO             PET5 index= 153                             MPIR_CVAR_REDUCE_SHORT_MSG_SIZE : the short message algorithm will be used if the send buffer size is <= this value (in bytes)
20211015 115039.978 INFO             PET5 index= 154                                 MPIR_CVAR_ENABLE_SMP_REDUCE : Enable SMP aware reduce.
20211015 115039.978 INFO             PET5 index= 155                           MPIR_CVAR_MAX_SMP_REDUCE_MSG_SIZE : Maximum message size for which SMP-aware reduce is used.  A value of '0' uses SMP-aware reduce for all message sizes.
20211015 115039.978 INFO             PET5 index= 156                            MPIR_CVAR_REDUCE_INTRA_ALGORITHM : Variable to select reduce algorithmauto                  - Internal algorithm selectionbinomial              - Force binomial algorithmnb                    - Force nonblocking algorithmreduce_scatter_gather - Force reduce scatter gather algorithm
20211015 115039.978 INFO             PET5 index= 157                            MPIR_CVAR_REDUCE_INTER_ALGORITHM : Variable to select reduce algorithmauto                     - Internal algorithm selectionlocal_reduce_remote_send - Force local-reduce-remote-send algorithmnb                       - Force nonblocking algorithm
20211015 115039.978 INFO             PET5 index= 158                          MPIR_CVAR_REDUCE_DEVICE_COLLECTIVE : If set to true, MPI_Reduce will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level reduce function will not be called.
20211015 115039.978 INFO             PET5 index= 159          MPIR_CVAR_REDUCE_SCATTER_COMMUTATIVE_LONG_MSG_SIZE : the long message algorithm will be used if the operation is commutative and the send buffer size is >= this value (in bytes)
20211015 115039.978 INFO             PET5 index= 160                    MPIR_CVAR_REDUCE_SCATTER_INTRA_ALGORITHM : Variable to select reduce_scatter algorithmauto               - Internal algorithm selectionnb                 - Force nonblocking algorithmnoncommutative     - Force noncommutative algorithmpairwise           - Force pairwise algorithmrecursive_doubling - Force recursive doubling algorithmrecursive_halving  - Force recursive halving algorithm
20211015 115039.978 INFO             PET5 index= 161                    MPIR_CVAR_REDUCE_SCATTER_INTER_ALGORITHM : Variable to select reduce_scatter algorithmauto                        - Internal algorithm selectionnb                          - Force nonblocking algorithmremote_reduce_local_scatter - Force remote-reduce-local-scatter algorithm
20211015 115039.978 INFO             PET5 index= 162                  MPIR_CVAR_REDUCE_SCATTER_DEVICE_COLLECTIVE : If set to true, MPI_Redscat will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level reduce_scatter function will not be called.
20211015 115039.978 INFO             PET5 index= 163              MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTRA_ALGORITHM : Variable to select reduce_scatter_block algorithmauto               - Internal algorithm selectionnoncommutative     - Force noncommutative algorithmrecursive_doubling - Force recursive doubling algorithmpairwise           - Force pairwise algorithmrecursive_halving  - Force recursive halving algorithmnb                 - Force nonblocking algorithm
20211015 115039.978 INFO             PET5 index= 164              MPIR_CVAR_REDUCE_SCATTER_BLOCK_INTER_ALGORITHM : Variable to select reduce_scatter_block algorithmauto                        - Internal algorithm selectionnb                          - Force nonblocking algorithmremote_reduce_local_scatter - Force remote-reduce-local-scatter algorithm
20211015 115039.978 INFO             PET5 index= 165            MPIR_CVAR_REDUCE_SCATTER_BLOCK_DEVICE_COLLECTIVE : If set to true, MPI_Reduce_scatter_block will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level reduce_scatter_block function will not be called.
20211015 115039.978 INFO             PET5 index= 166                              MPIR_CVAR_SCAN_INTRA_ALGORITHM : Variable to select allgather algorithmauto               - Internal algorithm selectionnb                 - Force nonblocking algorithmrecursive_doubling - Force recursive doubling algorithm
20211015 115039.978 INFO             PET5 index= 167                            MPIR_CVAR_SCAN_DEVICE_COLLECTIVE : If set to true, MPI_Scan will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level scan function will not be called.
20211015 115039.978 INFO             PET5 index= 168                      MPIR_CVAR_SCATTER_INTER_SHORT_MSG_SIZE : use the short message algorithm for intercommunicator MPI_Scatter if the send buffer size is < this value (in bytes)
20211015 115039.978 INFO             PET5 index= 169                           MPIR_CVAR_SCATTER_INTRA_ALGORITHM : Variable to select scatter algorithmauto     - Internal algorithm selectionbinomial - Force binomial algorithmnb       - Force nonblocking algorithm
20211015 115039.978 INFO             PET5 index= 170                           MPIR_CVAR_SCATTER_INTER_ALGORITHM : Variable to select scatter algorithmauto                      - Internal algorithm selectionlinear                    - Force linear algorithmnb                        - Force nonblocking algorithmremote_send_local_scatter - Force remote-send-local-scatter algorithm
20211015 115039.978 INFO             PET5 index= 171                         MPIR_CVAR_SCATTER_DEVICE_COLLECTIVE : If set to true, MPI_Scatter will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level scatter function will not be called.
20211015 115039.978 INFO             PET5 index= 172                          MPIR_CVAR_SCATTERV_INTRA_ALGORITHM : Variable to select scatterv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithmnb     - Force nonblocking algorithm
20211015 115039.978 INFO             PET5 index= 173                          MPIR_CVAR_SCATTERV_INTER_ALGORITHM : Variable to select scatterv algorithmauto   - Internal algorithm selectionlinear - Force linear algorithmnb     - Force nonblocking algorithm
20211015 115039.978 INFO             PET5 index= 174                        MPIR_CVAR_SCATTERV_DEVICE_COLLECTIVE : If set to true, MPI_scatterv will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually. If set to false, the device-level scatterv function will not be called.
20211015 115039.978 INFO             PET5 index= 175                                MPIR_CVAR_DEVICE_COLLECTIVES : If set to true, MPI collectives will allow the device to override the MPIR-level collective algorithms. The device still has the option to call the MPIR-level algorithms manually.  If set to false, the device-level collective function will not be called.
20211015 115039.978 INFO             PET5 index= 176                                MPIR_CVAR_PROGRESS_MAX_COLLS : Maximum number of collective operations at a time that the progress engine should make progress on
20211015 115039.978 INFO             PET5 index= 177                              MPIR_CVAR_COMM_SPLIT_USE_QSORT : Use qsort(3) in the implementation of MPI_Comm_split instead of bubble sort.
20211015 115039.978 INFO             PET5 index= 178                                  MPIR_CVAR_CTXID_EAGER_SIZE : The MPIR_CVAR_CTXID_EAGER_SIZE environment variable allows you to specify how many words in the context ID mask will be set aside for the eager allocation protocol.  If the application is running out of context IDs, reducing this value may help.
20211015 115039.978 INFO             PET5 index= 179                                    MPIR_CVAR_PROCTABLE_SIZE : Size of the "MPIR" debugger interface proctable (process table).
20211015 115039.978 INFO             PET5 index= 180                                   MPIR_CVAR_PROCTABLE_PRINT : If true, dump the proctable entries at MPII_Wait_for_debugger-time.
20211015 115039.978 INFO             PET5 index= 181                                 MPIR_CVAR_PRINT_ERROR_STACK : If true, print an error stack trace at error handling time.
20211015 115039.978 INFO             PET5 index= 182                                  MPIR_CVAR_CHOP_ERROR_STACK : If >0, truncate error stack output lines this many characters wide.  If 0, do not truncate, and if <0 use a sensible default.
20211015 115039.978 INFO             PET5 index= 183                            MPIR_CVAR_SUPPRESS_ABORT_MESSAGE : Disable printing of abort error message.
20211015 115039.978 INFO             PET5 index= 184                                           MPIR_CVAR_MEMDUMP : If true, list any memory that was allocated by MPICH and that remains allocated when MPI_Finalize completes.
20211015 115039.978 INFO             PET5 index= 185                          MPIR_CVAR_MEM_CATEGORY_INFORMATION : If true, print a summary of memory allocation by category. The category definitions are found in mpl_trmem.h.
20211015 115039.978 INFO             PET5 index= 186                                    MPIR_CVAR_ASYNC_PROGRESS : If set to true, MPICH will initiate an additional thread to make asynchronous progress on all communication operations including point-to-point, collective, one-sided operations and I/O.  Setting this variable will automatically increase the thread-safety level to MPI_THREAD_MULTIPLE.  While this improves the progress semantics, it might cause a small amount of performance overhead for regular MPI operations.  The user is encouraged to leave one or more hardware threads vacant in order to prevent contention between the application threads and the progress thread(s).  The impact of oversubscription is highly system dependent but may be substantial in some cases, hence this recommendation.
20211015 115039.978 INFO             PET5 index= 187                              MPIR_CVAR_DEFAULT_THREAD_LEVEL : Sets the default thread level to use when using MPI_INIT. This variable is case-insensitive.
20211015 115039.978 INFO             PET5 index= 188                                        MPIR_CVAR_DEBUG_HOLD : If true, causes processes to wait in MPI_Init and MPI_Initthread for a debugger to be attached.  Once the debugger has attached, the variable 'hold' should be set to 0 in order to allow the process to continue (e.g., in gdb, "set hold=0").
20211015 115039.978 INFO             PET5 index= 189                                    MPIR_CVAR_ERROR_CHECKING : If true, perform checks for errors, typically to verify valid inputs to MPI routines.  Only effective when MPICH is configured with --enable-error-checking=runtime .
20211015 115039.978 INFO             PET5 index= 190                                  MPIR_CVAR_NETLOC_NODE_FILE : Subnet json file
20211015 115039.978 INFO             PET5 index= 191                                      MPIR_CVAR_DIMS_VERBOSE : If true, enable verbose output about the actions of the implementation of MPI_Dims_create.
20211015 115039.978 INFO             PET5 index= 192                              MPIR_CVAR_NAMESERV_FILE_PUBDIR : Sets the directory to use for MPI service publishing in the file nameserv implementation.  Allows the user to override where the publish and lookup information is placed for connect/accept based applications.
20211015 115039.978 INFO             PET5 index= 193                           MPIR_CVAR_ABORT_ON_LEAKED_HANDLES : If true, MPI will call MPI_Abort at MPI_Finalize if any MPI object handles have been leaked.  For example, if MPI_Comm_dup is called without calling a corresponding MPI_Comm_free.  For uninteresting reasons, enabling this option may prevent all known object leaks from being reported.  MPICH must have been configure with "--enable-g=handlealloc" or better in order for this functionality to work.
20211015 115039.978 INFO             PET5 index= 194                                           MPIR_CVAR_NOLOCAL : If true, force all processes to operate as though all processes are located on another node.  For example, this disables shared memory communication hierarchical collectives.
20211015 115039.978 INFO             PET5 index= 195                                  MPIR_CVAR_ODD_EVEN_CLIQUES : If true, odd procs on a node are seen as local to each other, and even procs on a node are seen as local to each other.  Used for debugging on a single machine. Deprecated in favor of MPIR_CVAR_NUM_CLIQUES.
20211015 115039.978 INFO             PET5 index= 196                                       MPIR_CVAR_NUM_CLIQUES : Specify the number of cliques that should be used to partition procs on a local node. Procs with the same clique number are seen as local to each other. Used for debugging on a single machine.
20211015 115039.978 INFO             PET5 index= 197                                  MPIR_CVAR_COLL_ALIAS_CHECK : Enable checking of aliasing in collective operations
20211015 115039.978 INFO             PET5 index= 198                                 MPIR_CVAR_REQUEST_POLL_FREQ : How frequent to poll during completion calls (wait/test) in terms of number of processed requests before polling.
20211015 115039.978 INFO             PET5 index= 199                                MPIR_CVAR_REQUEST_BATCH_SIZE : The number of requests to make completion as a batch in MPI_Waitall and MPI_Testall implementation. A large number is likely to cause more cache misses.
20211015 115039.978 INFO             PET5 index= 200                                MPIR_CVAR_POLLS_BEFORE_YIELD : When MPICH is in a busy waiting loop, it will periodically call a function to yield the processor.  This cvar sets the number of loops before the yield function is called.  A value of 0 disables yielding.
20211015 115039.978 INFO             PET5 index= 201                          MPIR_CVAR_NEMESIS_MXM_BULK_CONNECT : If true, force mxm to connect all processes at initialization time.
20211015 115039.978 INFO             PET5 index= 202                       MPIR_CVAR_NEMESIS_MXM_BULK_DISCONNECT : If true, force mxm to disconnect all processes at finalization time.
20211015 115039.978 INFO             PET5 index= 203                              MPIR_CVAR_NEMESIS_MXM_HUGEPAGE : If true, mxm tries detecting hugepage support.  On HPC-X 2.3 and earlier, this might cause problems on Ubuntu and other platforms even if the system provides hugepage support.
20211015 115039.978 INFO             PET5 index= 204                                  MPIR_CVAR_OFI_USE_PROVIDER : If non-null, choose an OFI provider by name. If using with the CH4 device and using a provider that supports an older version of the libfabric API then the default version of the installed library, specifying the OFI version via the appropriate CVARs is also recommended.
20211015 115039.978 INFO             PET5 index= 205                                MPIR_CVAR_OFI_DUMP_PROVIDERS : If true, dump provider information at init
20211015 115039.978 INFO             PET5 index= 206                            MPIR_CVAR_CH3_INTERFACE_HOSTNAME : If non-NULL, this cvar specifies the IP address that other processes should use when connecting to this process. This cvar is mutually exclusive with the MPIR_CVAR_CH3_NETWORK_IFACE cvar and it is an error to set them both.
20211015 115039.978 INFO             PET5 index= 207                                    MPIR_CVAR_CH3_PORT_RANGE : The MPIR_CVAR_CH3_PORT_RANGE environment variable allows you to specify the range of TCP ports to be used by the process manager and the MPICH library. The format of this variable is <low>:<high>.  To specify any available port, use 0:0.
20211015 115039.978 INFO             PET5 index= 208                         MPIR_CVAR_NEMESIS_TCP_NETWORK_IFACE : If non-NULL, this cvar specifies which pseudo-ethernet interface the tcp netmod should use (e.g., "eth1", "ib0"). Note, this is a Linux-specific cvar. This cvar is mutually exclusive with the MPIR_CVAR_CH3_INTERFACE_HOSTNAME cvar and it is an error to set them both.
20211015 115039.978 INFO             PET5 index= 209                   MPIR_CVAR_NEMESIS_TCP_HOST_LOOKUP_RETRIES : This cvar controls the number of times to retry the gethostbyname() function before giving up.
20211015 115039.978 INFO             PET5 index= 210                            MPIR_CVAR_NEMESIS_ENABLE_CKPOINT : If true, enables checkpointing support and returns an error if checkpointing library cannot be initialized.
20211015 115039.978 INFO             PET5 index= 211                          MPIR_CVAR_NEMESIS_SHM_EAGER_MAX_SZ : This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for shared memory. If this cvar is set to -1, then Nemesis will choose an appropriate value.
20211015 115039.978 INFO             PET5 index= 212                    MPIR_CVAR_NEMESIS_SHM_READY_EAGER_MAX_SZ : This cvar controls the message size at which Nemesis switches from eager to rendezvous mode for ready-send messages.  If this cvar is set to -1, then ready messages will always be sent eagerly.  If this cvar is set to -2, then Nemesis will choose an appropriate value.
20211015 115039.978 INFO             PET5 index= 213                                         MPIR_CVAR_ENABLE_FT : Enable fault tolerance functions
20211015 115039.978 INFO             PET5 index= 214                         MPIR_CVAR_NEMESIS_LMT_DMA_THRESHOLD : Messages larger than this size will use the "dma" (knem) intranode LMT implementation, if it is enabled and available.
20211015 115039.978 INFO             PET5 index= 215                                    MPIR_CVAR_NEMESIS_NETMOD : If non-empty, this cvar specifies which network module should be used for communication. This variable is case-insensitive.
20211015 115039.978 INFO             PET5 index= 216                                  MPIR_CVAR_CH3_ENABLE_HCOLL : If true, enable HCOLL collectives.
20211015 115039.978 INFO             PET5 index= 217                          MPIR_CVAR_CH3_COMM_CONNECT_TIMEOUT : The default time out period in seconds for a connection attempt to the server communicator where the named port exists but no pending accept. User can change the value for a specified connection through its info argument.
20211015 115039.978 INFO             PET5 index= 218               MPIR_CVAR_CH3_RMA_OP_PIGGYBACK_LOCK_DATA_SIZE : Specify the threshold of data size of a RMA operation which can be piggybacked with a LOCK message. It is always a positive value and should not be smaller than MPIDI_RMA_IMMED_BYTES. If user sets it as a small value, for middle and large data size, we will lose performance because of always waiting for round-trip of LOCK synchronization; if user sets it as a large value, we need to consume more memory on target side to buffer this lock request when lock is not satisfied.
20211015 115039.978 INFO             PET5 index= 219                      MPIR_CVAR_CH3_RMA_ACTIVE_REQ_THRESHOLD : Threshold of number of active requests to trigger blocking waiting in operation routines. When the value is negative, we never blockingly wait in operation routines. When the value is zero, we always trigger blocking waiting in operation routines to wait until no. of active requests becomes zero. When the value is positive, we do blocking waiting in operation routines to wait until no. of active requests being reduced to this value.
20211015 115039.978 INFO             PET5 index= 220               MPIR_CVAR_CH3_RMA_POKE_PROGRESS_REQ_THRESHOLD : Threshold at which the RMA implementation attempts to complete requests while completing RMA operations and while using the lazy synchonization approach.  Change this value if programs fail because they run out of requests or other internal resources
20211015 115039.978 INFO             PET5 index= 221                MPIR_CVAR_CH3_RMA_SCALABLE_FENCE_PROCESS_NUM : Specify the threshold of switching the algorithm used in FENCE from the basic algorithm to the scalable algorithm. The value can be nagative, zero or positive. When the number of processes is larger than or equal to this value, FENCE will use a scalable algorithm which do not use O(P) data structure; when the number of processes is smaller than the value, FENCE will use a basic but fast algorithm which requires an O(P) data structure.
20211015 115039.978 INFO             PET5 index= 222            MPIR_CVAR_CH3_RMA_DELAY_ISSUING_FOR_PIGGYBACKING : Specify if delay issuing of RMA operations for piggybacking LOCK/UNLOCK/FLUSH is enabled. It can be either 0 or 1. When it is set to 1, the issuing of LOCK message is delayed until origin process see the first RMA operation and piggyback LOCK with that operation, and the origin process always keeps the current last operation until the ending synchronization call in order to piggyback UNLOCK/FLUSH with that operation. When it is set to 0, in WIN_LOCK/UNLOCK case, the LOCK message is sent out as early as possible, in WIN_LOCK_ALL/UNLOCK_ALL case, the origin process still tries to piggyback LOCK message with the first operation; for UNLOCK/FLUSH message, the origin process no longer keeps the current last operation but only piggyback UNLOCK/FLUSH if there is an operation avaliable in the ending synchronization call.
20211015 115039.978 INFO             PET5 index= 223                                MPIR_CVAR_CH3_RMA_SLOTS_SIZE : Number of RMA slots during window creation. Each slot contains a linked list of target elements. The distribution of ranks among slots follows a round-robin pattern. Requires a positive value.
20211015 115039.978 INFO             PET5 index= 224                    MPIR_CVAR_CH3_RMA_TARGET_LOCK_DATA_BYTES : Size (in bytes) of available lock data this window can provided. If current buffered lock data is more than this value, the process will drop the upcoming operation data. Requires a positive calue.
20211015 115039.978 INFO             PET5 index= 225                            MPIR_CVAR_CH3_EAGER_MAX_MSG_SIZE : This cvar controls the message size at which CH3 switches from eager to rendezvous mode.
20211015 115039.978 INFO             PET5 index= 226                          MPIR_CVAR_CH3_RMA_OP_WIN_POOL_SIZE : Size of the window-private RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediately.  Requires a positive value.
20211015 115039.978 INFO             PET5 index= 227                       MPIR_CVAR_CH3_RMA_OP_GLOBAL_POOL_SIZE : Size of the Global RMA operations pool (in number of operations) that stores information about RMA operations that could not be issued immediatly.  Requires a positive value.
20211015 115039.979 INFO             PET5 index= 228                      MPIR_CVAR_CH3_RMA_TARGET_WIN_POOL_SIZE : Size of the window-private RMA target pool (in number of targets) that stores information about RMA targets that could not be issued immediately.  Requires a positive value.
20211015 115039.979 INFO             PET5 index= 229                   MPIR_CVAR_CH3_RMA_TARGET_GLOBAL_POOL_SIZE : Size of the Global RMA targets pool (in number of targets) that stores information about RMA targets that could not be issued immediatly.  Requires a positive value.
20211015 115039.979 INFO             PET5 index= 230           MPIR_CVAR_CH3_RMA_TARGET_LOCK_ENTRY_WIN_POOL_SIZE : Size of the window-private RMA lock entries pool (in number of lock entries) that stores information about RMA lock requests that could not be satisfied immediatly.  Requires a positive value.
20211015 115039.979 INFO             PET5 index= 231                     MPIR_CVAR_CH4_OFI_CAPABILITY_SETS_DEBUG : Prints out the configuration of each capability selected via the capability sets interface.
20211015 115039.979 INFO             PET5 index= 232                               MPIR_CVAR_CH4_OFI_ENABLE_DATA : Enable immediate data fields in OFI to transmit source rank outside of the match bits
20211015 115039.979 INFO             PET5 index= 233                           MPIR_CVAR_CH4_OFI_ENABLE_AV_TABLE : If true, the OFI addressing information will be stored with an FI_AV_TABLE. If false, an FI_AV_MAP will be used.
20211015 115039.979 INFO             PET5 index= 234                 MPIR_CVAR_CH4_OFI_ENABLE_SCALABLE_ENDPOINTS : If true, use OFI scalable endpoints.
20211015 115039.979 INFO             PET5 index= 235                    MPIR_CVAR_CH4_OFI_ENABLE_SHARED_CONTEXTS : If set to false (zero), MPICH does not use OFI shared contexts. If set to -1, it is determined by the OFI capability sets based on the provider. Otherwise, MPICH tries to use OFI shared contexts. If they are unavailable, it'll fall back to the mode without shared contexts.
20211015 115039.979 INFO             PET5 index= 236                        MPIR_CVAR_CH4_OFI_ENABLE_MR_SCALABLE : If true, MR_SCALABLE for OFI memory regions. If false, MR_BASIC for OFI memory regions.
20211015 115039.979 INFO             PET5 index= 237                             MPIR_CVAR_CH4_OFI_ENABLE_TAGGED : If true, use tagged message transmission functions in OFI.
20211015 115039.979 INFO             PET5 index= 238                                 MPIR_CVAR_CH4_OFI_ENABLE_AM : If true, enable OFI active message support.
20211015 115039.979 INFO             PET5 index= 239                                MPIR_CVAR_CH4_OFI_ENABLE_RMA : If true, enable OFI RMA support.
20211015 115039.979 INFO             PET5 index= 240                            MPIR_CVAR_CH4_OFI_ENABLE_ATOMICS : If true, enable OFI Atomics support.
20211015 115039.979 INFO             PET5 index= 241                       MPIR_CVAR_CH4_OFI_FETCH_ATOMIC_IOVECS : Specifies the maximum number of iovecs that can be used by the OFI provider for fetch_atomic operations. The default value is -1, indicating that no value is set.
20211015 115039.979 INFO             PET5 index= 242                 MPIR_CVAR_CH4_OFI_ENABLE_DATA_AUTO_PROGRESS : If true, enable MPI data auto progress.
20211015 115039.979 INFO             PET5 index= 243              MPIR_CVAR_CH4_OFI_ENABLE_CONTROL_AUTO_PROGRESS : If true, enable MPI control auto progress.
20211015 115039.979 INFO             PET5 index= 244                       MPIR_CVAR_CH4_OFI_ENABLE_PT2PT_NOPACK : If true, enable iovec for pt2pt.
20211015 115039.979 INFO             PET5 index= 245                           MPIR_CVAR_CH4_OFI_CONTEXT_ID_BITS : Specifies the number of bits that will be used for matching the context ID. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20211015 115039.979 INFO             PET5 index= 246                                 MPIR_CVAR_CH4_OFI_RANK_BITS : Specifies the number of bits that will be used for matching the MPI rank. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20211015 115039.979 INFO             PET5 index= 247                                  MPIR_CVAR_CH4_OFI_TAG_BITS : Specifies the number of bits that will be used for matching the user tag. The default value is -1, indicating that no value is set and that the default will be defined in the ofi_types.h file.
20211015 115039.979 INFO             PET5 index= 248                             MPIR_CVAR_CH4_OFI_MAJOR_VERSION : Specifies the major version of the OFI library. The default is the major version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
20211015 115039.979 INFO             PET5 index= 249                             MPIR_CVAR_CH4_OFI_MINOR_VERSION : Specifies the major version of the OFI library. The default is the minor version of the OFI library used with MPICH. If using this CVAR, it is recommended that the user also specifies a specific OFI provider.
20211015 115039.979 INFO             PET5 index= 250                                  MPIR_CVAR_CH4_OFI_MAX_VNIS : If set to positive, this CVAR specifies the maximum number of CH4 VNIs that OFI netmod exposes.
20211015 115039.979 INFO             PET5 index= 251                           MPIR_CVAR_CH4_OFI_MAX_RMA_SEP_CTX : If set to positive, this CVAR specifies the maximum number of transmit contexts RMA can utilize in a scalable endpoint. This value is effective only when scalable endpoint is available, otherwise it will be ignored.
20211015 115039.979 INFO             PET5 index= 252                          MPIR_CVAR_CH4_OFI_MAX_EAGAIN_RETRY : If set to positive, this CVAR specifies the maximum number of retries of an ofi operations before returning MPIX_ERR_EAGAIN. This value is effective only when the communicator has the MPI_OFI_set_eagain info hint set to true.
20211015 115039.979 INFO             PET5 index= 253                            MPIR_CVAR_CH4_OFI_NUM_AM_BUFFERS : Specifies the number of buffers for receiving active messages.
20211015 115039.979 INFO             PET5 index= 254                                        MPIR_CVAR_CH4_NETMOD : If non-empty, this cvar specifies which network module to use
20211015 115039.979 INFO             PET5 index= 255                                           MPIR_CVAR_CH4_SHM : If non-empty, this cvar specifies which shm module to use
20211015 115039.979 INFO             PET5 index= 256                                MPIR_CVAR_CH4_ROOTS_ONLY_PMI : Enables an optimized business card exchange over PMI for node root processes only.
20211015 115039.979 INFO             PET5 index= 257                            MPIR_CVAR_CH4_RUNTIME_CONF_DEBUG : If enabled, CH4-level runtime configurations are printed out
20211015 115039.979 INFO             PET5 index= 258                                      MPIR_CVAR_CH4_MT_MODEL : Specifies the CH4 multi-threading model. Possible values are: direct (default) handoff trylock
20211015 115039.979 INFO             PET5 index= 259                          MPIR_CVAR_CH4_COMM_CONNECT_TIMEOUT : The default time out period in seconds for a connection attempt to the server communicator where the named port exists but no pending accept. User can change the value for a specified connection through its info argument.
20211015 115039.979 INFO             PET5 index= 260                             MPIR_CVAR_CH4_RANDOM_ADDR_RETRY : The default number of retries for generating a random address. A retrying involves only local operations.
20211015 115039.979 INFO             PET5 index= 261                             MPIR_CVAR_CH4_SHM_SYMHEAP_RETRY : The default number of retries for allocating a symmetric heap in shared memory. A retrying involves collective communication over the group in the shared memory.
20211015 115039.979 INFO             PET5 index= 262                             MPIR_CVAR_CH4_RMA_MEM_EFFICIENT : If true, memory-saving mode is on, per-target object is released at the epoch end call. If false, performance-efficient mode is on, all allocated target objects are cached and freed at win_finalize.
20211015 115039.979 INFO             PET5 index= 263                                      MPIR_CVAR_ENABLE_HCOLL : Enable hcoll collective support.
20211015 115039.979 INFO             PET5 index= 264                                   MPIR_CVAR_COLL_SCHED_DUMP : Print schedule data for nonblocking collective operations.
20211015 115039.979 INFO             PET5 --- VMK::logSystem() end ---------------------------------
20211015 115039.979 INFO             PET5 main: --- VMK::log() start -------------------------------------
20211015 115039.979 INFO             PET5 main: vm located at: 0x7fda1740cf90
20211015 115039.979 INFO             PET5 main: petCount=6 localPet=5 mypthid=0 currentSsiPe=-1
20211015 115039.979 INFO             PET5 main: Current system level OMP_NUM_THREADS setting for local PET: 12
20211015 115039.979 INFO             PET5 main: ssiCount=1 localSsi=0
20211015 115039.979 INFO             PET5 main: mpionly=1 threadsflag=0
20211015 115039.979 INFO             PET5 main: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20211015 115039.979 INFO             PET5 main: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20211015 115039.979 INFO             PET5 main:  PE=0 SSI=0 SSIPE=0
20211015 115039.979 INFO             PET5 main: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20211015 115039.979 INFO             PET5 main:  PE=1 SSI=0 SSIPE=1
20211015 115039.979 INFO             PET5 main: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20211015 115039.979 INFO             PET5 main:  PE=2 SSI=0 SSIPE=2
20211015 115039.979 INFO             PET5 main: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20211015 115039.979 INFO             PET5 main:  PE=3 SSI=0 SSIPE=3
20211015 115039.979 INFO             PET5 main: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20211015 115039.979 INFO             PET5 main:  PE=4 SSI=0 SSIPE=4
20211015 115039.979 INFO             PET5 main: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20211015 115039.979 INFO             PET5 main:  PE=5 SSI=0 SSIPE=5
20211015 115039.979 INFO             PET5 main: --- VMK::log() end ---------------------------------------
20211015 115039.980 INFO             PET5 Executing 'userm1_setvm'
20211015 115039.980 INFO             PET5 Executing 'userm1_register'
20211015 115039.980 INFO             PET5 Executing 'userm2_setvm'
20211015 115039.980 INFO             PET5 Executing 'userm2_register'
20211015 115039.980 INFO             PET5 model1: --- VMK::log() start -------------------------------------
20211015 115039.980 INFO             PET5 model1: vm located at: 0x7fda20101b70
20211015 115039.980 INFO             PET5 model1: petCount=6 localPet=5 mypthid=0 currentSsiPe=-1
20211015 115039.980 INFO             PET5 model1: Current system level OMP_NUM_THREADS setting for local PET: 12
20211015 115039.980 INFO             PET5 model1: ssiCount=1 localSsi=0
20211015 115039.980 INFO             PET5 model1: mpionly=1 threadsflag=0
20211015 115039.980 INFO             PET5 model1: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20211015 115039.980 INFO             PET5 model1: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20211015 115039.980 INFO             PET5 model1:  PE=0 SSI=0 SSIPE=0
20211015 115039.980 INFO             PET5 model1: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20211015 115039.980 INFO             PET5 model1:  PE=1 SSI=0 SSIPE=1
20211015 115039.980 INFO             PET5 model1: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20211015 115039.980 INFO             PET5 model1:  PE=2 SSI=0 SSIPE=2
20211015 115039.980 INFO             PET5 model1: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20211015 115039.980 INFO             PET5 model1:  PE=3 SSI=0 SSIPE=3
20211015 115039.980 INFO             PET5 model1: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20211015 115039.980 INFO             PET5 model1:  PE=4 SSI=0 SSIPE=4
20211015 115039.980 INFO             PET5 model1: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20211015 115039.980 INFO             PET5 model1:  PE=5 SSI=0 SSIPE=5
20211015 115039.980 INFO             PET5 model1: --- VMK::log() end ---------------------------------------
20211015 115039.985 INFO             PET5 Entering 'user1_run'
20211015 115039.985 INFO             PET5  user1_run: on SSIPE:           -1  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20211015 115040.387 INFO             PET5  user1_run: on SSIPE:           -1  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20211015 115040.636 INFO             PET5  user1_run: on SSIPE:           -1  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20211015 115040.899 INFO             PET5  user1_run: on SSIPE:           -1  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20211015 115041.148 INFO             PET5  user1_run: on SSIPE:           -1  Filling data lbound:        8335           1           1  ubound:       10000        1200          10
20211015 115041.409 INFO             PET5 Exiting 'user1_run'
20211015 115041.412 INFO             PET5 Entering 'user2_run'
20211015 115041.412 INFO             PET5 model2: --- VMK::log() start -------------------------------------
20211015 115041.412 INFO             PET5 model2: vm located at: 0x7fda201022f0
20211015 115041.412 INFO             PET5 model2: petCount=6 localPet=5 mypthid=0 currentSsiPe=-1
20211015 115041.412 INFO             PET5 model2: Current system level OMP_NUM_THREADS setting for local PET: 12
20211015 115041.412 INFO             PET5 model2: ssiCount=1 localSsi=0
20211015 115041.412 INFO             PET5 model2: mpionly=1 threadsflag=0
20211015 115041.412 INFO             PET5 model2: ESMF level logical PET->PE association (but consider system level affinity pinning!):
20211015 115041.412 INFO             PET5 model2: PET=0 lpid=0 tid=0 pid=0 peCount=1 accCount=0
20211015 115041.412 INFO             PET5 model2:  PE=0 SSI=0 SSIPE=0
20211015 115041.412 INFO             PET5 model2: PET=1 lpid=1 tid=0 pid=1 peCount=1 accCount=0
20211015 115041.412 INFO             PET5 model2:  PE=1 SSI=0 SSIPE=1
20211015 115041.412 INFO             PET5 model2: PET=2 lpid=2 tid=0 pid=2 peCount=1 accCount=0
20211015 115041.412 INFO             PET5 model2:  PE=2 SSI=0 SSIPE=2
20211015 115041.412 INFO             PET5 model2: PET=3 lpid=3 tid=0 pid=3 peCount=1 accCount=0
20211015 115041.412 INFO             PET5 model2:  PE=3 SSI=0 SSIPE=3
20211015 115041.412 INFO             PET5 model2: PET=4 lpid=4 tid=0 pid=4 peCount=1 accCount=0
20211015 115041.412 INFO             PET5 model2:  PE=4 SSI=0 SSIPE=4
20211015 115041.412 INFO             PET5 model2: PET=5 lpid=5 tid=0 pid=5 peCount=1 accCount=0
20211015 115041.412 INFO             PET5 model2:  PE=5 SSI=0 SSIPE=5
20211015 115041.412 INFO             PET5 model2: --- VMK::log() end ---------------------------------------
20211015 115041.412 INFO             PET5  user2_run: ssiLocalDeCount=           6
20211015 115041.413 INFO             PET5  user2_run: OpenMP thread:           4  on SSIPE:           -1  Testing data for localDe =           4  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20211015 115041.413 INFO             PET5  user2_run: OpenMP thread:           3  on SSIPE:           -1  Testing data for localDe =           3  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20211015 115041.423 INFO             PET5  user2_run: OpenMP thread:           0  on SSIPE:           -1  Testing data for localDe =           0  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20211015 115041.433 INFO             PET5  user2_run: OpenMP thread:           1  on SSIPE:           -1  Testing data for localDe =           1  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20211015 115041.443 INFO             PET5  user2_run: OpenMP thread:           2  on SSIPE:           -1  Testing data for localDe =           2  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20211015 115041.463 INFO             PET5  user2_run: OpenMP thread:           5  on SSIPE:           -1  Testing data for localDe =           5  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20211015 115042.904 INFO             PET5  user2_run: OpenMP thread:           4  on SSIPE:           -1  Testing data for localDe =           4  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20211015 115042.904 INFO             PET5  user2_run: OpenMP thread:           0  on SSIPE:           -1  Testing data for localDe =           0  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20211015 115042.911 INFO             PET5  user2_run: OpenMP thread:           3  on SSIPE:           -1  Testing data for localDe =           3  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20211015 115042.914 INFO             PET5  user2_run: OpenMP thread:           1  on SSIPE:           -1  Testing data for localDe =           1  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20211015 115042.914 INFO             PET5  user2_run: OpenMP thread:           2  on SSIPE:           -1  Testing data for localDe =           2  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20211015 115042.918 INFO             PET5  user2_run: OpenMP thread:           5  on SSIPE:           -1  Testing data for localDe =           5  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20211015 115044.242 INFO             PET5  user2_run: OpenMP thread:           0  on SSIPE:           -1  Testing data for localDe =           0  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20211015 115044.242 INFO             PET5  user2_run: OpenMP thread:           4  on SSIPE:           -1  Testing data for localDe =           4  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20211015 115044.243 INFO             PET5  user2_run: OpenMP thread:           3  on SSIPE:           -1  Testing data for localDe =           3  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20211015 115044.244 INFO             PET5  user2_run: OpenMP thread:           1  on SSIPE:           -1  Testing data for localDe =           1  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20211015 115044.247 INFO             PET5  user2_run: OpenMP thread:           2  on SSIPE:           -1  Testing data for localDe =           2  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20211015 115044.248 INFO             PET5  user2_run: OpenMP thread:           5  on SSIPE:           -1  Testing data for localDe =           5  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20211015 115045.585 INFO             PET5  user2_run: OpenMP thread:           0  on SSIPE:           -1  Testing data for localDe =           0  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20211015 115045.588 INFO             PET5  user2_run: OpenMP thread:           4  on SSIPE:           -1  Testing data for localDe =           4  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20211015 115045.588 INFO             PET5  user2_run: OpenMP thread:           1  on SSIPE:           -1  Testing data for localDe =           1  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20211015 115045.592 INFO             PET5  user2_run: OpenMP thread:           3  on SSIPE:           -1  Testing data for localDe =           3  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20211015 115045.593 INFO             PET5  user2_run: OpenMP thread:           5  on SSIPE:           -1  Testing data for localDe =           5  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20211015 115045.593 INFO             PET5  user2_run: OpenMP thread:           2  on SSIPE:           -1  Testing data for localDe =           2  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20211015 115046.924 INFO             PET5  user2_run: OpenMP thread:           0  on SSIPE:           -1  Testing data for localDe =           0  DE=           5  lbound:        8335           1           1  ubound:       10000        1200          10
20211015 115046.925 INFO             PET5  user2_run: OpenMP thread:           4  on SSIPE:           -1  Testing data for localDe =           4  DE=           3  lbound:        5002           1           1  ubound:        6668        1200          10
20211015 115046.926 INFO             PET5  user2_run: OpenMP thread:           1  on SSIPE:           -1  Testing data for localDe =           1  DE=           0  lbound:           1           1           1  ubound:        1667        1200          10
20211015 115046.935 INFO             PET5  user2_run: OpenMP thread:           5  on SSIPE:           -1  Testing data for localDe =           5  DE=           4  lbound:        6669           1           1  ubound:        8334        1200          10
20211015 115046.936 INFO             PET5  user2_run: OpenMP thread:           3  on SSIPE:           -1  Testing data for localDe =           3  DE=           2  lbound:        3335           1           1  ubound:        5001        1200          10
20211015 115046.941 INFO             PET5  user2_run: OpenMP thread:           2  on SSIPE:           -1  Testing data for localDe =           2  DE=           1  lbound:        1668           1           1  ubound:        3334        1200          10
20211015 115048.259 INFO             PET5  user2_run: All data correct.
20211015 115048.259 INFO             PET5 Exiting 'user2_run'
20211015 115048.313 INFO             PET5  NUMBER_OF_PROCESSORS           6
20211015 115048.314 INFO             PET5  PASS  System Test ESMF_FieldSharedDeSSISTest, ESMF_FieldSharedDeSSISTest.F90, line 276
20211015 115048.314 INFO             PET5 Finalizing ESMF
